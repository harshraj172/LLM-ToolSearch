{"completion": "sentence-transformers/gtr-t5-base", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search."}
{"completion": "sentence-transformers/gtr-t5-base", "text": "query: We are building a search engine to help our customers find similar texts. The goal is to process the search results and return ranked recommendations."}
{"completion": "facebook/wav2vec2-base-960h", "text": "document: Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files."}
{"completion": "facebook/wav2vec2-base-960h", "text": "query: A lawyer needs to convert some audio files from a court session into text for analysis. Automate the process for transcription."}
{"completion": "uclanlp/visualbert-vqa", "text": "document: A VisualBERT model for Visual Question Answering."}
{"completion": "uclanlp/visualbert-vqa", "text": "query: I am an owner of a startup and I want to build an intelligent assistant that can answer questions about images, for example, what color is the object in the image?"}
{"completion": "blip2-flan-t5-xl", "text": "document: BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xl", "text": "query: Create a program that takes an image URL and the question \"What's the subject of the image?\" as input and returns the answer from the model."}
{"completion": "nitrosocke/nitro-diffusion", "text": "document: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use."}
{"completion": "nitrosocke/nitro-diffusion", "text": "query: I have a bunch of descriptive text on different characters, I want my machine to generate images with different clothes and different suggestive content based on the text provided."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "document: This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "query: I need a personal assistant which can understand 10 different languages to answer my questions based on provided context."}
{"completion": "duncan93/video", "text": "document: A text-to-video model trained on OpenAssistant/oasst1 dataset."}
{"completion": "duncan93/video", "text": "query: Our company wants to develop an app that can create video gags from text inputs. We need a model to generate a video from text."}
{"completion": "opus-mt-ca-es", "text": "document: A Hugging Face model for translation between Catalan (ca) and Spanish (es) languages, based on the OPUS dataset and using the transformer-align architecture. The model has been pre-processed with normalization and SentencePiece."}
{"completion": "opus-mt-ca-es", "text": "query: Our company is expanding our user base and we need to localize our web application. Specifically, to have the ability to translate Catalan text to Spanish."}
{"completion": "wavymulder/Analog-Diffusion", "text": "document: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}
{"completion": "wavymulder/Analog-Diffusion", "text": "query: A film studio wishes to create a storyboard for a historical movie, but they want it in analog style. Create an image of ancient city."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: Our company is developing an application that recommends restaurants to users. We need to analyze the sentiment of Yelp review texts to provide the best suggestions."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "document: Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI)."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "query: Write a prescription for a patient with the side effect of allergies using the knowledge stored in this pretrained model."}
{"completion": "typeform/squeezebert-mnli", "text": "document: SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks."}
{"completion": "typeform/squeezebert-mnli", "text": "query: We have a large set of text sentences that we want to classify into categories but we do not have any labeled data. We wish to use Zero-Shot Classification for this task."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "query: Develop a video classification system that is capable of identifying human actions in videos."}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: We are a news website and we want to provide voiceover for our articles in Spanish."}
{"completion": "lang-id-voxlingua107-ecapa", "text": "document: This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. It covers 107 different languages."}
{"completion": "lang-id-voxlingua107-ecapa", "text": "query: Our company wants to provide support for multiple languages in our voice assistant. We need to be able to recognize the language spoken by the user."}
{"completion": "Eklavya/ZFF_VAD", "text": "document: A Voice Activity Detection model by Eklavya, using the Hugging Face framework."}
{"completion": "Eklavya/ZFF_VAD", "text": "query: Our new voice assistant can make calls to users. An important feature developers requested is detecting silence during a call. We need to understand when the user has stopped talking in order to make important decisions."}
{"completion": "xlnet-base-cased", "text": "document: XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context."}
{"completion": "xlnet-base-cased", "text": "query: We are a content agency, in oder to quickly generate drafts, we need to use the AI model. Use a zero shot classifier to classify the article into 5 different topics(base on the content provided)."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "query: Come up with an approach to find the relation between consecutive description of image depictions."}
{"completion": "google/flan-t5-xxl", "text": "document: FLAN-T5 XXL is a fine-tuned version of the T5 language model, achieving state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. It has been fine-tuned on more than 1000 additional tasks covering multiple languages, including English, German, and French. It can be used for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning and question answering."}
{"completion": "google/flan-t5-xxl", "text": "query: In our language learning app, we need a function to rewrite a sentence in French with the negation."}
{"completion": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804", "text": "document: A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech."}
{"completion": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804", "text": "query: We are building an audio-guide app for Japanese tourists. We need a text-to-speech function for reading Japanese text in the app."}
{"completion": "padmalcom/tts-tacotron2-german", "text": "document: Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently."}
{"completion": "padmalcom/tts-tacotron2-german", "text": "query: I need a voice assistant to tell me the weather forecast for Germany tomorrow in German."}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "document: Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "query: There is a document provided, can you find the total cost listed within that document?"}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "query: Our company focuses on autonomous robots in warehouses. We need to estimate depth using a pretrained model."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "document: Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "query: A hospital wants an AI system to remove any sensitive information from anonymized patient cases before sharing them for research purposes."}
{"completion": "rajistics/MAPIE-TS-Electricity", "text": "document: A RandomForestRegressor model for electricity consumption prediction."}
{"completion": "rajistics/MAPIE-TS-Electricity", "text": "query: A local electricity company is looking to build a machine learning model to predict electricity consumption in their city for the next quarter. They need assistance in selecting and building the right model to predict the consumption accurately."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761512", "text": "document: A model trained using AutoTrain for predicting US housing prices with single column regression. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a CO2 Emissions of 50.5369 grams."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761512", "text": "query: As a real estate agency, we want to predict the potential value of a house based on its features. Use a pre-trained model to predict housing prices."}
{"completion": "ruDialoGpt3-medium-finetuned-telegram", "text": "document: DialoGPT trained on Russian language and fine tuned on my telegram chat. This model was created by sberbank-ai and trained on Russian forums. It has been fine-tuned on a 30mb json file of exported telegram chat data."}
{"completion": "ruDialoGpt3-medium-finetuned-telegram", "text": "query: I want to develop a chatbot in Russian that can talk about a wide range of topics like politics, economy, and technology."}
{"completion": "bert-base-multilingual-uncased-sentiment", "text": "document: This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5)."}
{"completion": "bert-base-multilingual-uncased-sentiment", "text": "query: I am an online store owner. I want to understand the sentiment of my customers based on the reviews that they have provided. Analyze their sentiment using a model that's capable of working with multilingual reviews."}
{"completion": "nateraw/vit-age-classifier", "text": "document: A vision transformer finetuned to classify the age of a given person's face."}
{"completion": "nateraw/vit-age-classifier", "text": "query: We want to build an AI system that monitors the visitors' age in our amusement park."}
{"completion": "donut-base-finetuned-cord-v2", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset."}
{"completion": "donut-base-finetuned-cord-v2", "text": "query: I have travelled a lot and taken pictures of many places, but I can't remember what those places were. Extract texts from the photograph of a historic monument."}
{"completion": "DialogLED-base-16384", "text": "document: DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase."}
{"completion": "DialogLED-base-16384", "text": "query: We want to summarize the main points discussed in a long conversation between people. Please help us with a model and example code."}
{"completion": "bigscience/bloom-7b1", "text": "document: BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text."}
{"completion": "bigscience/bloom-7b1", "text": "query: My website requires a tool for generating short stories when given a prompt."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "query: We are a smart city company looking for solutions to repair roads on time. Please help us identify potholes in images."}
{"completion": "openai/clip-vit-large-patch14", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-large-patch14", "text": "query: I'm an educator creating a study app for students. I want to provide an image description option so students can understand the image better. Separate options for animal or sport explanations are provided depending on the context of the image."}
{"completion": "deepset/deberta-v3-large-squad2", "text": "document: This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-large-squad2", "text": "query: \"What is the role of neurons in the human brain?\""}
{"completion": "pygmalion-6b", "text": "document: Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."}
{"completion": "pygmalion-6b", "text": "query: We need an AI character that can provide information about climate change to explain the problem to young children."}
{"completion": "lllyasviel/control_v11p_sd15_scribble", "text": "document: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_scribble", "text": "query: Our company focuses on creating personalized posters. We require an application that would generate images based on user given text inputs."}
{"completion": "mazkooleg/0-9up-hubert-base-ls960-ft", "text": "document: This model is a fine-tuned version of facebook/hubert-base-ls960 on the None dataset. It achieves an accuracy of 0.9973 on the evaluation set."}
{"completion": "mazkooleg/0-9up-hubert-base-ls960-ft", "text": "query: I need a tool that transcribes my voice notes from numbers to text."}
{"completion": "optimum/t5-small", "text": "document: T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization."}
{"completion": "optimum/t5-small", "text": "query: I want to translate English text to French using T5."}
{"completion": "google/pegasus-xsum", "text": "document: PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences."}
{"completion": "google/pegasus-xsum", "text": "query: Create a short summary of the company's annual report for the press release."}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "document: CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER"}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "query: For a project, an application needs to extract features from a medical text to perform some analysis."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "query: technology, sports, and politics."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "query: Develop a reinforcement learning model to guide virtual agents walking along a 2D terrain."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: I need to extract information from a document and answer a specific question. How can I do that?"}
{"completion": "fxmarty/resnet-tiny-beans", "text": "document: A model trained on the beans dataset, just for testing and having a really tiny model."}
{"completion": "fxmarty/resnet-tiny-beans", "text": "query: A mobile app for plant farmers needs to identify whether a plant leaf is healthy or diseased. Determine a solution to classify the images of plant leaves."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "document: A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "query: I want to translate the content of my website into Italian for my Italian customers."}
{"completion": "blip2-opt-6.7b", "text": "document: BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-opt-6.7b", "text": "query: Create a virtual assistant that can describe an image of my favorite vacation spot and give me some travel tips."}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "document: Baseline Model trained on tips5wx_sbh5 to apply regression on tip"}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "query: The restaurant's manager would like to predict tip amount. How can I train a model to predict the tip based on the available features?"}
{"completion": "al02783013/autotrain-faseiii_diciembre-2311773112", "text": "document: A tabular regression model trained using AutoTrain to predict carbon emissions based on input features."}
{"completion": "al02783013/autotrain-faseiii_diciembre-2311773112", "text": "query: I am an environmental consultant. I need to predict carbon emissions for a new factory based on some specific data points of the factory."}
{"completion": "patrickjohncyh/fashion-clip", "text": "document: FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks."}
{"completion": "patrickjohncyh/fashion-clip", "text": "query: A fashion magazine company has approached us to design a text-driven fashion categorization system for various fashion apparel."}
{"completion": "pyannote/brouhaha", "text": "document: Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library."}
{"completion": "pyannote/brouhaha", "text": "query: We are developing a meeting summarization solution. We want to estimate voice activity in the audio and provide a clean transcript of the meeting."}
{"completion": "903429548", "text": "document: A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text."}
{"completion": "903429548", "text": "query: An investor is researching a niche market and would like to find companies within the text."}
{"completion": "ConvTasNet_Libri2Mix_sepclean_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset."}
{"completion": "ConvTasNet_Libri2Mix_sepclean_16k", "text": "query: I record a lot of podcasts and have trouble removing background noise. I need an audio denoising tool."}
{"completion": "sentence-transformers/LaBSE", "text": "document: This is a port of the LaBSE model to PyTorch. It can be used to map 109 languages to a shared vector space."}
{"completion": "sentence-transformers/LaBSE", "text": "query: We are working on improving our customer support services. Find a way to measure how similar customer questions are to our knowledge base articles."}
{"completion": "fastspeech2-en-male1", "text": "document: FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4."}
{"completion": "fastspeech2-en-male1", "text": "query: A company is developing an AI customer service chatbot. Based on the customers' typed messages, the chatbot is expected to generate spoken answers that can be played to the customers in real-time."}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "document: A binary classification model for predicting carbon emissions"}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "query: An organization is committed to reducing its carbon footprint. Can they assess each company and predict if they are above or below the limit of emissions?"}
{"completion": "openai/clip-vit-base-patch16", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-base-patch16", "text": "query: A bot for an Instagram account wants to detect the content of the images (such as food, animals, cars, etc.) posted by users."}
{"completion": "google/byt5-small", "text": "document: ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5. ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is usable on a downstream task. ByT5 works especially well on noisy text data, e.g., google/byt5-small significantly outperforms mt5-small on TweetQA."}
{"completion": "google/byt5-small", "text": "query: We need to paraphrase a document containing various sentences for better readability."}
{"completion": "plguillou/t5-base-fr-sum-cnndm", "text": "document: This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization."}
{"completion": "plguillou/t5-base-fr-sum-cnndm", "text": "query: Summarize an article in French about economics."}
{"completion": "pygmalion-6b", "text": "document: Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue."}
{"completion": "pygmalion-6b", "text": "query: I'm currently building a chatbot that can impersonate a popular historical figure. I need this chatbot to return natural language responses based on user inputs."}
{"completion": "tiny-random-LayoutLMv3ForQuestionAnswering", "text": "document: A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API."}
{"completion": "tiny-random-LayoutLMv3ForQuestionAnswering", "text": "query: Our client needs to extract data from a document related to a question. Please provide a solution."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: A bank customer has recieved a scanned paper document as a PDF, which contains information about his loan details. He wants to know the total loan amount without going through the entire document."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "document: A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "query: We are working on a project where we have to predict the carbon emissions of different vehicles. We need a solution to predict vehicle emissions."}
{"completion": "d4data/Indian-voice-cloning", "text": "document: A model for detecting voice activity in Indian languages."}
{"completion": "d4data/Indian-voice-cloning", "text": "query: The user wants to automatically trim a podcast and remove the silence parts. Use an API to detect the spoken parts in the podcast."}
{"completion": "pyannote/speaker-diarization", "text": "document: This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers."}
{"completion": "pyannote/speaker-diarization", "text": "query: We have a recorded meeting and we need to separate the speakers in the given meeting."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "query: I want to build a pothole detection system for a city. Please guide me on how to use this API."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "document: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "query: Create an advertisement image for a new luxury watch brand using a text prompt."}
{"completion": "pyannote/voice-activity-detection", "text": "document: A pretrained voice activity detection pipeline that detects active speech in audio files."}
{"completion": "pyannote/voice-activity-detection", "text": "query: In our business meeting recording, we need to identify the segments where our company leader speaks."}
{"completion": "Salesforce/codegen-350M-multi", "text": "document: CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well."}
{"completion": "Salesforce/codegen-350M-multi", "text": "query: I am building a mobile app for time management. I would like to create a function that, given a task, returns how long in minutes it should take."}
{"completion": "Rakib/roberta-base-on-cuad", "text": "document: This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents."}
{"completion": "Rakib/roberta-base-on-cuad", "text": "query: Our client is a lawyer and needs help extracting essential information from a legal document."}
{"completion": "setu4993/LaBSE", "text": "document: Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval."}
{"completion": "setu4993/LaBSE", "text": "query: Our organization is hosting an international conference. We need to match speakers with similar research interests, regardless of language."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "document: This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "query: Your task is to build a language detector tool that analyses the given text and determines in which language it is written."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "document: TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "query: We are compiling a report. Help us extract data from a large dataset in table form."}
{"completion": "facebook/vc1-large", "text": "document: The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation."}
{"completion": "facebook/vc1-large", "text": "query: We are building an AI-powered robot that can autonomously map and navigate an environment. We need to process images captured by robot's camera feed to understand the environment."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "query: Detect depth information in images using depth estimation model for a computer vision application."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "document: A Korean Table Question Answering model finetuned on the korwikitq dataset."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "query: We are running a Korean board game and one of the players is asking a question \"What is the price of the item X?\". The answers to all the questions can be found in the table from the manual. Can you help us to answer that?"}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "query: A homeowner needs a device to identify whether an animal in their backyard is a cat or a dog. Design a solution for them."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: I own a restaurant, and I want to find the positive reviews and the negative reviews automatically."}
{"completion": "mio/Artoria", "text": "document: This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output."}
{"completion": "mio/Artoria", "text": "query: Imagine you are creating an audiobook. Use an AI model to convert a text paragraph into speech."}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "document: Barthez model finetuned on orangeSum for abstract generation in French language"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "query: I have a lengthy article in French and need a brief summary."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "document: This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "query: Our company is receiving customer feedback in various languages. We want to sort these feedbacks into categories like 'customer service', 'product quality', 'delivery', and 'user experience'."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "document: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "query: I work for an airline service provider. We need to detect planes in images captured from the airport tarmac."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "document: This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "query: My robotics club needs to build an AI system to simulate a two-link robot, then create an agent that learns how to swing the robot to reach the goal."}
{"completion": "glpn-nyu-finetuned-diode-221122-082237", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}
{"completion": "glpn-nyu-finetuned-diode-221122-082237", "text": "query: Our customers are interested in depth estimation. Guide me with an instruction using the provided API to serve my customers."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "document: A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "query: We recently launched a language learning app for children. We need to create a feature that answers questions about images that our young users may have."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "document: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead']."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "query: Global Offensive (CS:GO) players in a given image."}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. French, single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "query: I want a system that reads french texts that I send it, and play the audio for me."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa", "text": "query: I have a technical manual about a product we are selling and we want to create an online FAQ to answer customers' questions."}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "document: A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data."}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "query: I have a dataset containing features related to carbon emissions, and I want to predict the emission levels based on the provided features."}
{"completion": "sd-class-pandas-32", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "sd-class-pandas-32", "text": "query: A web media company needs to design banners for their upcoming event. How should they use this model to generate creative images?"}
{"completion": "microsoft/tapex-large-sql-execution", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large-sql-execution", "text": "query: I'm analyzing historical data of Olympic Games and I want to know which year the Olympic Games took place in Beijing."}
{"completion": "opus-mt-ROMANCE-en", "text": "document: A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing."}
{"completion": "opus-mt-ROMANCE-en", "text": "query: Our website has just launched in several countries that speak Romance languages. We need to translate user comments from those languages to English."}
{"completion": "Realistic_Vision_V1.4", "text": "document: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}
{"completion": "Realistic_Vision_V1.4", "text": "query: We are a fashion blog and our latest post is about futuristic fashion. Help us to generate an image of a woman wearing a dress made of some futuristic and sustainable material."}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "document: Barthez model finetuned on orangeSum for abstract generation in French language"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "query: Our company needs a summary of a French language article to present to our English speaking investors."}
{"completion": "TalTechNLP/voxlingua107-epaca-tdnn", "text": "document: This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages."}
{"completion": "TalTechNLP/voxlingua107-epaca-tdnn", "text": "query: I'm developing an app that can identify languages in real-time. The app should be able to process audio files and recognize the spoken language."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "document: Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "query: In our robotics project, a robot needs to hop on one leg. We need to supply the robot with the next action based on the current state."}
{"completion": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "text": "document: SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output."}
{"completion": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "text": "query: Our client is a medical research center. They requested a tool that can analyze the relationships between different biomedical entities."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "document: A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "query: We are building a robot to understand the contents of an image and answer questions based on that image. The robot needs to support English, Chinese, Japanese, and German languages as input."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: A language learning platform wants to build a speech-to-phoneme model to assist users in checking their pronunciation."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "document: A model trained for binary classification of carbon emissions using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "query: We have data about the carbon emissions of different companies. We are trying to determine if a company is responsible for high or low carbon emissions based on this data."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "document: Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It's smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "query: I am building a product to analyze the emotions of the users. Can you help me detect the emotion of a given text?"}
{"completion": "google/pegasus-newsroom", "text": "document: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks."}
{"completion": "google/pegasus-newsroom", "text": "query: We have to summarize a news article to be shared on our social media."}
{"completion": "philschmid/pyannote-speaker-diarization-endpoint", "text": "document: A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters."}
{"completion": "philschmid/pyannote-speaker-diarization-endpoint", "text": "query: Our company has a collection of audio recordings with multiple speakers, and we need to determine the speakers' turns."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "document: Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI)."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "query: Design a health-related application that uses AI to assist physicians in understanding medical reports by filling in masked words."}
{"completion": "layoutlm-vqa", "text": "document: A model for document question answering using the LayoutLM architecture."}
{"completion": "layoutlm-vqa", "text": "query: We have an HR department where we receive CVs, resumes, and job applications. Frequently, we need to find specific details like the person's experience or education within the document."}
{"completion": "glpn-nyu-finetuned-diode-221122-030603", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221122-030603", "text": "query: Assess the elevation profile of the captured image using a depth estimation model."}
{"completion": "glpn-nyu-finetuned-diode-221215-095508", "text": "document: A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture."}
{"completion": "glpn-nyu-finetuned-diode-221215-095508", "text": "query: Implement an AI-based application to estimate the depth of objects in a photograph to assist photographers in understanding the depth of field."}
{"completion": "pygmalion-350m", "text": "document: This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors."}
{"completion": "pygmalion-350m", "text": "query: Design a chatbot that generates responses in a conversation based on the user's input history."}
{"completion": "uclanlp/visualbert-vqa", "text": "document: A VisualBERT model for Visual Question Answering."}
{"completion": "uclanlp/visualbert-vqa", "text": "query: We are building an application to help tourists identify famous landmarks and find answers to their questions based on the image of the landmark. Using a multimodal model, we need to provide information about the provided image and the text question."}
{"completion": "dslim/bert-base-NER", "text": "document: bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset."}
{"completion": "dslim/bert-base-NER", "text": "query: Provide a way to extract certain information like name, location, and organizations from an article."}
{"completion": "superb/hubert-base-superb-ks", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-base-superb-ks", "text": "query: We have developed a smart speaker system and want to detect keyword commands from audio clips."}
{"completion": "facebook/dragon-plus-context-encoder", "text": "document: DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders."}
{"completion": "facebook/dragon-plus-context-encoder", "text": "query: We need to create an AI-driven search engine, and we want to use the best performing model for searching and ranking the text from large databases."}
{"completion": "microsoft/git-base-coco", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "microsoft/git-base-coco", "text": "query: Design a system that describes different scenes in images to provide more interactive experiences for visually impaired people."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: I am trying to remember the correct punctuation in this long sentence but I am not sure, give me the correct punctuation."}
{"completion": "google/ncsnpp-church-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024."}
{"completion": "google/ncsnpp-church-256", "text": "query: A film producer wants to get artwork for his upcoming movie. Generate a church-themed image"}
{"completion": "google/flan-t5-xl", "text": "document: FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models."}
{"completion": "google/flan-t5-xl", "text": "query: Develop a platform for a company that can perform text translations from English to other languages, like French, Spanish, and German."}
{"completion": "bert-base-multilingual-cased", "text": "document: BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-multilingual-cased", "text": "query: I am an AI developer who needs to unmask some missing words from a bilingual text."}
{"completion": "tejas23/autotrain-amx2-1702259725", "text": "document: Multi-class Classification Model for Carbon Emissions"}
{"completion": "tejas23/autotrain-amx2-1702259725", "text": "query: Assess the environmental impact of your company's products by estimating their carbon emissions with a classification model."}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "document: This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder."}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "query: Imagine you are working with a publishing agency. Using the text provided, create a captivating image for the book cover."}
{"completion": "padmalcom/wav2vec2-large-emotion-detection-german", "text": "document: This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral."}
{"completion": "padmalcom/wav2vec2-large-emotion-detection-german", "text": "query: We are building a chatbot for a German company that handles customer support. It would be great if we could detect the emotions of customers to cater to their needs better through audio calls."}
{"completion": "CompVis/ldm-celebahq-256", "text": "document: Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs."}
{"completion": "CompVis/ldm-celebahq-256", "text": "query: We are building an application requiring high-resolution human face images. Let's generate a high-quality face image."}
{"completion": "shibing624/text2vec-base-chinese", "text": "document: This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search."}
{"completion": "shibing624/text2vec-base-chinese", "text": "query: I am creating a chatbot that can suggest similar questions to the user based on their initial question. It needs to function properly for Chinese language questions."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: The company needs a voiceover for their advertisement video. Generate the audio from the script provided."}
{"completion": "naver-clova-ix/donut-base", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base", "text": "query: I have a painting and need to write a description or a story about the painting. The description should be creative and interesting."}
{"completion": "fxmarty/resnet-tiny-beans", "text": "document: A model trained on the beans dataset, just for testing and having a really tiny model."}
{"completion": "fxmarty/resnet-tiny-beans", "text": "query: Our team is working on automated quality control in our coffee bean production line. We need a machine learning model to recognize and classify different beans."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "query: We are a team of researchers studying public opinion on social media. Help us find the similarity between different tweet texts."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "query: I'm trying to build an AI chatbot that can fill in the blanks in a sentence."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "document: A model that classifies images as hotdog or not hotdog."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "query: Develop an application for people at a food festival to differentiate images of hotdogs from other types of foods instantly."}
{"completion": "d4data/biomedical-ner-all", "text": "document: An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased."}
{"completion": "d4data/biomedical-ner-all", "text": "query: Our company works in the biomedical field. We need to analyze patient reports and extract biomedical entities for further analysis."}
{"completion": "opus-mt-en-ROMANCE", "text": "document: A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID)."}
{"completion": "opus-mt-en-ROMANCE", "text": "query: There is a product we are developing where we need to understand the user's input language and translate it into Spanish."}
{"completion": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan", "text": "document: A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech."}
{"completion": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan", "text": "query: During a presentation, our computer-based virtual assistant will read aloud the text through automatically generated speech. The text is \"Welcome to the annual technology conference.\""}
{"completion": "blip-image-captioning-large", "text": "document: BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}
{"completion": "blip-image-captioning-large", "text": "query: Our customer is an e-commerce company that wants to add automatic caption generation for their product images."}
{"completion": "julien-c/voice-activity-detection", "text": "document: Example pyannote-audio Voice Activity Detection model using PyanNet. Imported from https://github.com/pyannote/pyannote-audio-hub and trained by @hbredin."}
{"completion": "julien-c/voice-activity-detection", "text": "query: We are developing a meeting transcription app. We need to detect sections of the audio recording where people are talking."}
{"completion": "fcakyon/yolov5s-v7.0", "text": "document: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories."}
{"completion": "fcakyon/yolov5s-v7.0", "text": "query: Implement object detection in a parking lot to count the number of parked cars."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: Detect similar questions in an FAQ dataset to avoid repetitive inquiries."}
{"completion": "glpn-nyu-finetuned-diode-221116-104421", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-104421", "text": "query: A real estate company wants to create 3D interior plans for their clients based on 2D images. We need to estimate the depth of the rooms in the images."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: We are running an event in a stadium, and we need a system to transcribe the announcement in real-time."}
{"completion": "dpt-large-redesign", "text": "document: A depth estimation model based on the DPT architecture."}
{"completion": "dpt-large-redesign", "text": "query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture."}
{"completion": "runwayml/stable-diffusion-inpainting", "text": "document: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask."}
{"completion": "runwayml/stable-diffusion-inpainting", "text": "query: Let's show a user-generated text prompt as a photo-realistic image at a park."}
{"completion": "lidiya/bart-large-xsum-samsum", "text": "document: This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset."}
{"completion": "lidiya/bart-large-xsum-samsum", "text": "query: I am an assistant on a support team for a popular streaming service. I need to quickly understand the conversation between my colleague and a client."}
{"completion": "pygmalion-1.3b", "text": "document: Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-1.3b", "text": "query: As a game designer, we want to create a chatbot to interact with the players and provide them with hints when they are stuck in the game."}
{"completion": "ast-finetuned-speech-commands-v2", "text": "document: Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."}
{"completion": "ast-finetuned-speech-commands-v2", "text": "query: We need to identify different types of speech commands in audio files. Develop a tool to achieve this goal."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "document: TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "query: Our startup plans to create a platform where users can find new video content related to their interests. We need to classify user preferences automatically."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: I'm trying to create a restaurant review analyzer that can help me determine if the review is positive or negative."}
{"completion": "indobenchmark/indobert-base-p1", "text": "document: IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective."}
{"completion": "indobenchmark/indobert-base-p1", "text": "query: I am creating an Indonesian language analysis application. I need to generate contextual embeddings for text written in the Indonesian language."}
{"completion": "shi-labs/oneformer_ade20k_swin_large", "text": "document: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model."}
{"completion": "shi-labs/oneformer_ade20k_swin_large", "text": "query: I'm working on a project that performs image segmentation. I need a pre-trained model that can handle semantic, instance, and panoptic segmentation tasks."}
{"completion": "valhalla/distilbart-mnli-12-3", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop."}
{"completion": "valhalla/distilbart-mnli-12-3", "text": "query: I am a filmmaker creating a series of short films. I need to sort them into genres like thriller, sci-fi, romance based on their plot description."}
{"completion": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean", "text": "document: This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset."}
{"completion": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean", "text": "query: A friend sent you a mixed audio-recorded file where two people were speaking at the same time, and you need to separate the speakers for clearer understanding."}
{"completion": "lllyasviel/control_v11p_sd15_canny", "text": "document: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_canny", "text": "query: I want to create a concept image featuring a futuristic city with tall buildings and flying cars."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "query: I am creating an app for a fitness coach that records videos of athletes performing different exercises. The app should identify which exercise the athlete is doing."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "query: We are working on a project that matches customer product reviews. We need our model to create embeddings of sentences to allow for semantic clustering."}
{"completion": "microsoft/deberta-v3-base", "text": "document: DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2."}
{"completion": "microsoft/deberta-v3-base", "text": "query: I am a teacher, and I want to autofill some words in a sentence that my students are struggling with."}
{"completion": "glpn-nyu-finetuned-diode-230131-041708", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}
{"completion": "glpn-nyu-finetuned-diode-230131-041708", "text": "query: We need a fine-tuned model for depth estimation in our self-driving car project."}
{"completion": "keremberke/yolov8s-csgo-player-detection", "text": "document: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead']."}
{"completion": "keremberke/yolov8s-csgo-player-detection", "text": "query: Global Offensive (CS:GO). We need to identify the players and draw bounding boxes around them."}
{"completion": "Helsinki-NLP/opus-mt-zh-en", "text": "document: A Chinese to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Marian NMT framework and trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-zh-en", "text": "query: I have an e-commerce business in China, and I need to translate customer messages from Chinese to English. Please create a model to do that."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "query: I need an image classifier to detect the presence of a cat or dog from a supplied image."}
{"completion": "google/pix2struct-chartqa-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}
{"completion": "google/pix2struct-chartqa-base", "text": "query: I am tasked with creating a machine-learning workflow for a platform that makes use of computer-generated charts. The users will be able to submit queries and receive answers from the graphs utilizing the query data. "}
{"completion": "ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "ddpm-cifar10-32", "text": "query: Develop a new wildlife-themed party banner image to be used for promotions and marketing of a board game."}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "document: Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack"}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "query: We need to know if a query has a question associated or it is just a regular sentence."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "query: Our city is planning to repair potholes. We need to analyze a series of images to find all pothole locations."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: We need a robot with voice synthesized for the hearing-impaired patients in the hospital."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "document: TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "query: We are developing a surveillance system that identifies activities occurring in public spaces. Detect the activity shown in a video."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "document: The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "query: To better understand the product, create a program that classifies an image from a URL."}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. French, single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "query: We have an AI translator that translates texts to different languages, and we would like to implement a text-to-speech feature for the translated text."}
{"completion": "microsoft/trocr-base-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-base-printed", "text": "query: I am trying to convert an image into readable text to automatically update my customer information in a banking system."}
{"completion": "flexudy/t5-base-multi-sentence-doctor", "text": "document: Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text."}
{"completion": "flexudy/t5-base-multi-sentence-doctor", "text": "query: Develop a technology that checks typos and corrects sentences from resumes and cover letters during a job interview."}
{"completion": "distil-ast-audioset", "text": "document: Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset."}
{"completion": "distil-ast-audioset", "text": "query: Write a code to classify a given audio clip into a category based on its content, such as music, speech, or environmental sounds."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "document: Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "query: Create a model instance using the provided api_call and set the model to evaluate mode."}
{"completion": "openai/whisper-large", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large", "text": "query: One of our customers uses an app to transcribe voice messages. We are integrating into the back end to transcribe the audio using the OpenAI Whisper API."}
{"completion": "MCG-NJU/videomae-base", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches."}
{"completion": "MCG-NJU/videomae-base", "text": "query: I need a software that will classify videos in the park if it's a person, animal or equipment."}
{"completion": "CodeBERTa-small-v1", "text": "document: CodeBERTa is a RoBERTa-like model trained on the CodeSearchNet dataset from GitHub. It supports languages like Go, Java, JavaScript, PHP, Python, and Ruby. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face tokenizers. The small model is a 6-layer, 84M parameters, RoBERTa-like Transformer model."}
{"completion": "CodeBERTa-small-v1", "text": "query: Write a function to complete a partially complete computer program (code)."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "document: MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "query: Our client needs help in classifying images of different objects."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "document: This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "query: Summarize the German text provided below."}
{"completion": "codet5-large-ntp-py", "text": "document: CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi."}
{"completion": "codet5-large-ntp-py", "text": "query: We are a startup involved in building code generation systems for basic Python programming. Help us use a pretrained model to convert English text instruction to a Python function."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: The management requires a sales report based on the quarterly sales table. Find out the top-selling product and the corresponding sales for the 3rd quarter."}
{"completion": "keremberke/yolov8m-building-segmentation", "text": "document: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images."}
{"completion": "keremberke/yolov8m-building-segmentation", "text": "query: Implement building segmentation on satellite images to be used for city planning."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese", "text": "query: Create a transcribe function to perform speech recognition on various Japanese audio files and produce a text summary of the spoken content."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "document: A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "query: We need to find a way to filter out nonsensical comments from the feedback section of our website."}
{"completion": "prithivida/parrot_paraphraser_on_T5", "text": "document: Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models."}
{"completion": "prithivida/parrot_paraphraser_on_T5", "text": "query: An advertising company wants to rephrase their slogans without losing their original meaning. Help them achieve this."}
{"completion": "glpn-nyu-finetuned-diode-221228-072509", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221228-072509", "text": "query: We are working with an autonomous vehicle company. They need a depth estimation model for their cars' camera systems."}
{"completion": "openai/clip-vit-large-patch14", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-large-patch14", "text": "query: We are developing an app for identifying animal species by images. The app should recognize cats and dogs, among others."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment-latest", "text": "document: This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment-latest", "text": "query: Create a tool to analyze the sentiment of tweets about the launch of our new product."}
{"completion": "google/pegasus-pubmed", "text": "document: The PEGASUS model is designed for abstractive summarization. It is pretrained on a mixture of C4 and HugeNews datasets and stochastically samples important sentences. The model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that encodes newline characters."}
{"completion": "google/pegasus-pubmed", "text": "query: I am looking for a natural language API solution to shorten a PubMed article into 1-2 sentences long summary."}
{"completion": "ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "ddpm-cifar10-32", "text": "query: We are designing a wallpaper for a mobile app and need a procedurally generated background image for the wallpaper."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "query: The urban planning department needs a tool to automatically generate rough drafts of buildings based on drawn line segments."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "query: We are a city council engineering office creating a program to detect and locate potholes in street images."}
{"completion": "farouk97/autotrain-test7-2644pc-linearregr-38619101723", "text": "document: A tabular regression model trained using AutoTrain to predict CO2 emissions (in grams)."}
{"completion": "farouk97/autotrain-test7-2644pc-linearregr-38619101723", "text": "query: We are building an environmental consulting company and want to predict CO2 emissions using the model."}
{"completion": "lllyasviel/sd-controlnet-hed", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-hed", "text": "query: We are working on a brochure for our company. We want to apply a creative edge effect to one of the images to make it stand out."}
{"completion": "MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary", "text": "document: This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective."}
{"completion": "MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary", "text": "query: \"The movie was good.\""}
{"completion": "sd-class-pandas-32", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "sd-class-pandas-32", "text": "query: Generate a cute animal image that can be used as an avatar on a pet-lovers website."}
{"completion": "microsoft/codebert-base", "text": "document: Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective."}
{"completion": "microsoft/codebert-base", "text": "query: As a software development company, we want our virtual assistant to help employees by generating code templates that can be used as a starting point based on their code query."}
{"completion": "google/pegasus-xsum", "text": "document: PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences."}
{"completion": "google/pegasus-xsum", "text": "query: Our team is working on a smart city project and we have collected a large article explaining the benefits of smart cities. Help us to summarize it."}
{"completion": "google/tapas-large-finetuned-wikisql-supervised", "text": "document: TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table."}
{"completion": "google/tapas-large-finetuned-wikisql-supervised", "text": "query: A world-wide construction company is looking for an AI solution to extract key data from construction project tables. Can you provide them assistance?"}
{"completion": "facebook/textless_sm_ro_en", "text": "document: A speech-to-speech translation model for Romanian to English developed by Facebook AI"}
{"completion": "facebook/textless_sm_ro_en", "text": "query: I want to create a real-time translation application that can be used to translate spoken language from Romanian to English during meetings."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "query: We are developing an application to identify and categorize different types of animals in images. Using an existing image classification model, help us recognize the animal present in an image."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "document: A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "query: Create a system that will analyze the sentiment of users movie reviews and decide if they liked or disliked the movie."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur", "text": "document: Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur", "text": "query: I need to translate an audio file in Spanish to English and keep it in the same audio format."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "query: Our team needs to design an intelligent security camera system. We want a program that categorizes video clips from surveillance cameras as safety-related or normal activities."}
{"completion": "Realistic_Vision_V1.4", "text": "document: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}
{"completion": "Realistic_Vision_V1.4", "text": "query: Our organization wants to develop a project to provide users with images based on the text description they provide."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: I am a financial analyst, I want to estimate the income of customers from their available data in a csv file, named 'customer_income.csv'."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "document: Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "query: A post-apocalyptic adventure, set in 2300, featuring a group of survivors including a scientist, a soldier, and a young hero. The scene should have an eerie aura, with decaying buildings and nature taking over. Include the title \"Reclaim\" on the image."}
{"completion": "DCCRNet_Libri1Mix_enhsingle_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}
{"completion": "DCCRNet_Libri1Mix_enhsingle_16k", "text": "query: We are looking for a solution to improve the quality of an audio recording by removing background noise."}
{"completion": "philschmid/pyannote-speaker-diarization-endpoint", "text": "document: A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters."}
{"completion": "philschmid/pyannote-speaker-diarization-endpoint", "text": "query: Design an application that detects different speakers in an audio conversation and provides the start and end time of each speaker's segment."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "document: TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "query: I have data about the latest office supplies in a table form. Help me find the most expensive office supply by querying the table."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "query: As a corporate business who focus on business insights for other companies, we often receive long articles about economics, policies, and technologies. We need to retrieve the most relevant segment of the article to a specific question."}
{"completion": "xm_transformer_s2ut_en-hk", "text": "document: Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain."}
{"completion": "xm_transformer_s2ut_en-hk", "text": "query: The company needs to demonstrate a speech-to-speech translation system for English to Hokkien in a TED Talks presentation."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "document: High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "query: We have a website that requires unique profile pictures for each user. We need to generate profile pictures for the users."}
{"completion": "blip-image-captioning-large", "text": "document: BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}
{"completion": "blip-image-captioning-large", "text": "query: I want to create an AI assistant that generates image captions for pictures from the internet."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "document: BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "query: A tourist company wants to create a guide application to answer questions related to a given image. Help them create the application."}
{"completion": "google/maxim-s3-deblurring-gopro", "text": "document: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository."}
{"completion": "google/maxim-s3-deblurring-gopro", "text": "query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show."}
{"completion": "facebook/wav2vec2-base-960h", "text": "document: Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files."}
{"completion": "facebook/wav2vec2-base-960h", "text": "query: We need to transcribe the audio of a podcast for people with hearing difficulties."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: \"Who was the first person to walk on the moon?\""}
{"completion": "microsoft/beit-base-patch16-224", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224."}
{"completion": "microsoft/beit-base-patch16-224", "text": "query: I want to classify the objects in an image to determine whether the image content is related to a customer complaint."}
{"completion": "microsoft/xclip-base-patch16-zero-shot", "text": "document: X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."}
{"completion": "microsoft/xclip-base-patch16-zero-shot", "text": "query: We are a company working on security cameras. It is important for us to detect violence and suspicious events in the videos."}
{"completion": "guillaumekln/faster-whisper-large-v2", "text": "document: Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper."}
{"completion": "guillaumekln/faster-whisper-large-v2", "text": "query: We have some audio from a recent company podcast. Please help us transcribe the audio into text."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "document: This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "query: We have developed a GTA5 game AI model and now want to apply it for traffic prediction of GTA5."}
{"completion": "dmis-lab/biobert-base-cased-v1.2", "text": "document: BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering."}
{"completion": "dmis-lab/biobert-base-cased-v1.2", "text": "query: As a part of a new medical research study, we have to analyze biomedical texts. Help us understand the relevant terminology."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: We are building a flower delivery service website, and we want to show suggestions to users about what kind of iris flower they might like based on some given features."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "query: Identify fake news and real news by checking how closely related the context of an article's summary and its title are."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "document: Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "query: I am creating a software to analyze business deals. Adapt an API to help us understand if two given sentences contradict, entail, or are neutral to each other."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "document: A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "query: Our customer, a manufacturer of sporting goods, wants us to determine what type of sports equipment is in the images they provided."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "document: A Korean Table Question Answering model finetuned on the korwikitq dataset."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "query: We need a solution to help customers find answers to their questions from a table containing information in Korean. Please create a pipeline using a model that can achieve this."}
{"completion": "deepset/roberta-base-squad2-distilled", "text": "document: This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature."}
{"completion": "deepset/roberta-base-squad2-distilled", "text": "query: I am working on a project that needs to answer questions using the given context. Please set up a solution for me."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "document: Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "query: I need to write an article about the impact of Artificial Intelligence on the environment. Please help me draft the introduction."}
{"completion": "ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "ddpm-cifar10-32", "text": "query: We are a gaming company that wants to create random images for our game based on a pre-trained model."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "query: I'm interested in transcribing some Russian audio files I have. Can you help me with that?"}
{"completion": "microsoft/trocr-large-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-handwritten", "text": "query: I am researching historical documents and need to transcribe handwritten texts. Can you help me?"}
{"completion": "frizwankhan/entity-linking-model-final", "text": "document: A Document Question Answering model based on layoutlmv2"}
{"completion": "frizwankhan/entity-linking-model-final", "text": "query: Please provide a solution for extracting information from a scanned document based on user\u2019s questions."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "document: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "query: Design an image for a company website with a catchphrase \"Green energy for a sustainable future.\""}
{"completion": "monologg/koelectra-small-v2-distilled-korquad-384", "text": "document: A Korean Question Answering model based on Electra and trained on the KorQuAD dataset."}
{"completion": "monologg/koelectra-small-v2-distilled-korquad-384", "text": "query: We need to provide automated customer support for Korean users. They often ask questions about product usage or troubleshooting. Help us find their answers."}
{"completion": "facebook/bart-large", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-large", "text": "query: We are a research group working on a text summarization project. We need to extract features from the text."}
{"completion": "facebook/wav2vec2-large-960h-lv60-self", "text": "document: Facebook's Wav2Vec2 model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio. The model was trained with Self-Training objective. The model is used for Automatic Speech Recognition and can be used as a standalone acoustic model."}
{"completion": "facebook/wav2vec2-large-960h-lv60-self", "text": "query: I want to transcribe the speeches in a video recording into text using a model."}
{"completion": "distil-ast-audioset", "text": "document: Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset."}
{"completion": "distil-ast-audioset", "text": "query: Our company is producing cars and we need to know if the engine sound is normal or if it indicates a problem."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa", "text": "query: The Human Resources department wants to extract the relevant information like name, email, and job title from a r\u00e9sum\u00e9 to improve the hiring process."}
{"completion": "popcornell/pyannote-segmentation-chime6-mixer6", "text": "document: Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline."}
{"completion": "popcornell/pyannote-segmentation-chime6-mixer6", "text": "query: Our team needs a solution to detect voice activity in audio recordings from our conference meetings."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli", "text": "document: This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying model was pre-trained by Microsoft on the CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli", "text": "query: We need a model that can predict if a given sentence implies either politics, economy, entertainment, or environment in multiple languages."}
{"completion": "distilbart-cnn-12-6-samsum", "text": "document: This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text."}
{"completion": "distilbart-cnn-12-6-samsum", "text": "query: The marketing team needs to write a pitch for an ad campaign, but the original text is too long. Summarize the campaign idea."}
{"completion": "imodels/figs-compas-recidivism", "text": "document: A tabular classification model for predicting recidivism using the COMPAS dataset. The model is an imodels.FIGSClassifier trained with Scikit-learn and can be used with the Hugging Face Inference API."}
{"completion": "imodels/figs-compas-recidivism", "text": "query: Our company needs a predictive model to analyze the risk of recidivism in our rehabilitation program."}
{"completion": "google/pegasus-xsum", "text": "document: PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences."}
{"completion": "google/pegasus-xsum", "text": "query: How would you give me a brief summary of a long article?"}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "query: Our company provides a social media management tool. In order to make our tool more understable, we need to detect the tone in the provided news headlines."}
{"completion": "dmis-lab/biobert-base-cased-v1.2", "text": "document: BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering."}
{"completion": "dmis-lab/biobert-base-cased-v1.2", "text": "query: Our client needs background details of a medicine called \"Ibuprofen\" to optimize search queries for their pharmaceutical website."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "document: Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "query: I would like to create an audio clip of a given text in a Taiwanese Hokkien accent."}
{"completion": "MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary", "text": "document: This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective."}
{"completion": "MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary", "text": "query: Create an AI-based system that, given a sentence, can predict whether or not it contradicts another piece of information."}
{"completion": "903929564", "text": "document: A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score."}
{"completion": "903929564", "text": "query: I want to develop a program that can identify the entities in a given text string."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "document: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "query: I need to create a machine learning model to generate an image of a beautiful sunset on the beach with a couple holding hands when given a piece of text describing the scene."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "document: This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "query: Identify the emotion in the sentence \"I can't believe I finally managed to score the interview with my dream company!\""}
{"completion": "facebook/wav2vec2-base-960h", "text": "document: Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files."}
{"completion": "facebook/wav2vec2-base-960h", "text": "query: We are developing an AI system that could transcribe spoken English from a series of podcasts."}
{"completion": "bigscience/bloom-7b1", "text": "document: BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text."}
{"completion": "bigscience/bloom-7b1", "text": "query: Generate a text to answer the question, \"How can I make my garden more eco-friendly?\""}
{"completion": "microsoft/trocr-large-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-handwritten", "text": "query: We need to convert a handwritten image into text. The image can be found at https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-dutch", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Dutch. Fine-tuned on Dutch using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-dutch", "text": "query: Add an automatic transcription service to our conference call application."}
{"completion": "pyannote/voice-activity-detection", "text": "document: A pretrained voice activity detection pipeline that detects active speech in audio files."}
{"completion": "pyannote/voice-activity-detection", "text": "query: We are having a meeting in office. Implement a pipeline that detects when someone speaks during the meeting."}
{"completion": "keras-io/tab_transformer", "text": "document: This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model."}
{"completion": "keras-io/tab_transformer", "text": "query: We need to predict the annual income of individuals. We have the United States Census Income Dataset with various demographic features."}
{"completion": "mazkooleg/0-9up-unispeech-sat-base-ft", "text": "document: This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979."}
{"completion": "mazkooleg/0-9up-unispeech-sat-base-ft", "text": "query: Develop a voice assistant that recognizes digits from zero to nine in spoken language and sorts them in ascending order."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "document: Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "query: To create educational content for children, we want to generate sentences that use specific sets of words, like \"apple\", \"tree\", and \"pick\"."}
{"completion": "tts-hifigan-german", "text": "document: A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram."}
{"completion": "tts-hifigan-german", "text": "query: An e-learning company needs a digital assistant that can convert written text into audio for language learning purposes."}
{"completion": "bert-base-cased", "text": "document: BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task."}
{"completion": "bert-base-cased", "text": "query: Build a lesson plan generator to fill missing gaps in a sentence provided by the teacher. The generator should recognize the correct answer _blank_ from the context."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "document: This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "query: I need a code that trains a neural network on the MNIST dataset with a specific data cap, that can be further used for game applications like GTA5."}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al."}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "query: I want to help movie producers to understand what genre their movie fits in by applying AI to their trailers."}
{"completion": "ast-finetuned-audioset-10-10-0.4593", "text": "document: Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."}
{"completion": "ast-finetuned-audioset-10-10-0.4593", "text": "query: Our company wants to categorize real-world sounds within the environment to improve the functionality of our devices."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "query: Can you please help an Image editing company how to use AI to detect whether a photo contains a cat?"}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023", "text": "document: A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023", "text": "query: We have a scanned image of a document and need to extract particular information from it."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "document: This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "query: In our healthcare program, we are researching Plantar Fasciitis, and we want to know what the best treatment options are for this condition."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "query: I need to create a system to generate realistic human faces. We need to create a model that can generate 1024x1024 images."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "document: This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details)."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "query: Build a tool to analyze the general sentiment of my audience on social media platforms."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "document: A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "query: Our customer support receives several inquiries. We want to be able to categorize them as technical issue, billing issue, or general inquiry."}
{"completion": "farouk97/autotrain-test7-2644pc-linearregr-38619101723", "text": "document: A tabular regression model trained using AutoTrain to predict CO2 emissions (in grams)."}
{"completion": "farouk97/autotrain-test7-2644pc-linearregr-38619101723", "text": "query: We have just bought a car and we are trying to predict the CO2 emissions of it based on the car specification. "}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "document: This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language."}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "query: A student is working on a research project and needs help with a question-answering model to quickly find answers in their research materials. Can you guide them?"}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761513", "text": "document: A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761513", "text": "query: We have a real estate company, and would like the model to predict the housing prices based on the property features."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "document: Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "query: A tutoring platform wants to create question generation system so that tutor can check student's understanding."}
{"completion": "textless_sm_cs_en", "text": "document: A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation."}
{"completion": "textless_sm_cs_en", "text": "query: I want to build a speech-to-speech translation application. Translate spoken Czech to English audio in real-time."}
{"completion": "convnext_base.fb_in1k", "text": "document: A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings."}
{"completion": "convnext_base.fb_in1k", "text": "query: Our company is now working on a pedestrian detection project. We need to detect the pedestrian from the video in real time."}
{"completion": "data2vec-audio-base-960h", "text": "document: Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision."}
{"completion": "data2vec-audio-base-960h", "text": "query: The conference organizers would like a transcription of their recorded presentations."}
{"completion": "prithivida/parrot_fluency_model", "text": "document: Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model."}
{"completion": "prithivida/parrot_fluency_model", "text": "query: We need a chatbot that can interact with users and generate variety in its responses. Can you help provide different ways to express the same thing?"}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: We are working on a multilingual news app, and we want to offer summaries of the articles in various languages."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "query: We have a collection of sport videos and we want to automatically tag them with the sport being played in the video."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "document: This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details)."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "query: I'm developing an app to analyze tweets from multiple languages. It needs to provide sentiment analysis without translating the tweets first."}
{"completion": "google/ddpm-ema-cat-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17."}
{"completion": "google/ddpm-ema-cat-256", "text": "query: I want to generate random images of cats to showcase them to my friends, is it possible using a computer-generated model?"}
{"completion": "JosephusCheung/GuanacoVQA", "text": "document: A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "query: Create a program that can answer questions about an image given in different languages like English, Chinese, Japanese, and German."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: I want to generate voice from text files so I can listen to them while relaxing."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "document: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "query: I am an artist working on an anime theme who wants to generate a realistic image based on a character description provided as text input."}
{"completion": "microsoft/tapex-large", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large", "text": "query: We need to extract specific data from a structured table containing customer information."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "document: Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "query: Could you help me generate the continuation of the story \"Once upon a time there lived an old man and a young boy in a small village near the mountains...\"?"}
{"completion": "flax-community/clip-rsicd-v2", "text": "document: This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images."}
{"completion": "flax-community/clip-rsicd-v2", "text": "query: Develop a solution to identify the category of the location given an image URL."}
{"completion": "allenai/cosmo-xl", "text": "document: COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation."}
{"completion": "allenai/cosmo-xl", "text": "query: I want to create a conversational agent that briefly describes their experience at a conference and answer my questions about it."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "document: A speech-to-speech translation model that can be loaded on the Inference API on-demand."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "query: I am a journalist, how do I get the text translated from English to German?"}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "document: BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "query: We're designing a chatbot for our website. We need to summarize long articles for our users before they read them."}
{"completion": "julien-c/voice-activity-detection", "text": "document: Example pyannote-audio Voice Activity Detection model using PyanNet. Imported from https://github.com/pyannote/pyannote-audio-hub and trained by @hbredin."}
{"completion": "julien-c/voice-activity-detection", "text": "query: In order to automatically transcribe a conversation, we first need to detect when someone is speaking."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "query: \"A cozy living room with a blue sofa, a wooden coffee table, and a large window overlooking a garden.\""}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Russian single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "query: We need an assistant that will notify us of the price changes in cryptocurrencies by reading the prices to us."}
{"completion": "codet5-large-ntp-py", "text": "document: CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi."}
{"completion": "codet5-large-ntp-py", "text": "query: We are building a platform to enable non-programmers to create basic Python programs through natural language instructions. Kindly implement a solution to generate Python code."}
{"completion": "AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')", "text": "document: This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly."}
{"completion": "AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')", "text": "query: crime, tragedy, and theft."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on Portuguese using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "query: Implement a text-based virtual assistant to identify Portuguese speech from audio recordings."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: We are building a tool for the government to estimate individual CO2 emission based on the Adult dataset. They want to classify whether a person is above or below a certain threshold of emissions."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "document: A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "query: A government environmental monitoring unit uses the provided dataset to predict the carbon emissions; please design a solution for them."}
{"completion": "speechbrain/sepformer-wham", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise."}
{"completion": "speechbrain/sepformer-wham", "text": "query: An audio recording with two people speaking was mistakenly recorded over background noise. We need a solution that separates the two speakers."}
{"completion": "roberta-large", "text": "document: RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "roberta-large", "text": "query: Create a question answering form to prompt users to complete a sentence by filling in a mask."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: Implement a speech recognition system that can transcribe an audio in any language."}
{"completion": "Realistic_Vision_V1.4", "text": "document: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}
{"completion": "Realistic_Vision_V1.4", "text": "query: As a digital artist, I am searching for a model that can take a text-based description of a scene and generate a photo-realistic image based on the given description. I want this to be done extremely efficiently."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: We want to create an app to convert written text into audio content to help visually impaired individuals."}
{"completion": "pyannote/speaker-diarization", "text": "document: This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers."}
{"completion": "pyannote/speaker-diarization", "text": "query: We would like a system to distinguish audio files and recognize which person is speaking in a conference call."}
{"completion": "tejas23/autotrain-amx2-1702259729", "text": "document: A multi-class classification model for predicting carbon emissions."}
{"completion": "tejas23/autotrain-amx2-1702259729", "text": "query: Our client is looking for a solution to predict carbon emissions from different industrial sectors. Let's help them categorize it."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "document: roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "query: The company is working on a project to identify key entities in business articles. We need to extract names, organizations and locations from the text."}
{"completion": "araffin/ppo-LunarLander-v2", "text": "document: This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library."}
{"completion": "araffin/ppo-LunarLander-v2", "text": "query: A new video game company wants to design an AI model for a lunar lander game. Guide them on using an already trained model."}
{"completion": "laion/CLIP-ViT-g-14-laion2B-s34B-b88K", "text": "document: A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories."}
{"completion": "laion/CLIP-ViT-g-14-laion2B-s34B-b88K", "text": "query: The owner of a pet store needs a software to categorize uploaded pet images to their corresponding animal type."}
{"completion": "xm_transformer_unity_en-hk", "text": "document: Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain."}
{"completion": "xm_transformer_unity_en-hk", "text": "query: Help me translate English speech to Hokkien speech for a documentary video on Taiwanese culture."}
{"completion": "sultan/BioM-ELECTRA-Large-SQuAD2", "text": "document: BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks."}
{"completion": "sultan/BioM-ELECTRA-Large-SQuAD2", "text": "query: A doctor needs to get answers from medical articles to help their patients. Suggest a pre-trained model for question-answering that is specifically made for the medical domain."}
{"completion": "lysandre/tapas-temporary-repo", "text": "document: TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up."}
{"completion": "lysandre/tapas-temporary-repo", "text": "query: Our startup is working on a project that allows users to easily find information from tables. Please help us extract relevant information based on a user's question."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "query: We are an environment consulting agency and need to predict carbon emissions given the appropriate input features."}
{"completion": "swin2SR-lightweight-x2-64", "text": "document: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution."}
{"completion": "swin2SR-lightweight-x2-64", "text": "query: To help the computer vision module to design a better drone, it is required to design a lightweight image super-resolution algorithm and integrate it on the computer."}
{"completion": "dslim/bert-base-NER-uncased", "text": "document: A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text."}
{"completion": "dslim/bert-base-NER-uncased", "text": "query: To track the activities of competitors, we want a tool that extracts company names, locations, and important people. We need to automatically identify these entities in the text data before it's sent to the user."}
{"completion": "mgp-str", "text": "document: MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images."}
{"completion": "mgp-str", "text": "query: We are analyzing social media posts, and we need to extract text from images to understand the messages being shared."}
{"completion": "Eklavya/ZFF_VAD", "text": "document: A Voice Activity Detection model by Eklavya, using the Hugging Face framework."}
{"completion": "Eklavya/ZFF_VAD", "text": "query: Our client has recorded a call with their customers and wants to know which parts of the audio file contain human speech."}
{"completion": "0xid/poca-SoccerTwos", "text": "document: A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "0xid/poca-SoccerTwos", "text": "query: We would like to create a virtual soccer game for two teams using reinforcement learning. How can we implement this trained model?"}
{"completion": "vblagoje/bert-english-uncased-finetuned-pos", "text": "document: A BERT model fine-tuned for Part-of-Speech (POS) tagging in English text."}
{"completion": "vblagoje/bert-english-uncased-finetuned-pos", "text": "query: The company is developing a language learning application that needs to identify the part of speech for each word in a sentence. Determine the parts of speech for a given sentence."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "document: Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "query: Design a tool that creates poster images for a butterfly-themed event."}
{"completion": "facebook/wav2vec2-large-960h-lv60-self", "text": "document: Facebook's Wav2Vec2 model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio. The model was trained with Self-Training objective. The model is used for Automatic Speech Recognition and can be used as a standalone acoustic model."}
{"completion": "facebook/wav2vec2-large-960h-lv60-self", "text": "query: We are a transcription company that wants to transcribe the speaker's voice from a meeting into text for our clients using a pre-trained model."}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "document: TAPAS mini model fine-tuned on Sequential Question Answering (SQA)"}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "query: In my business, I'll use the TAPAS model in a case where I have to correlate spreadsheet data with user input."}
{"completion": "philschmid/distilbert-onnx", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1."}
{"completion": "philschmid/distilbert-onnx", "text": "query: I want my phone to talk with me, everytime I want to know who is the leader of a country, I want my system to know it and answer me."}
{"completion": "yiyanghkust/finbert-tone", "text": "document: FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task."}
{"completion": "yiyanghkust/finbert-tone", "text": "query: We are running a local news agency, and we need to analyze the sentiment of the financial news we collected today."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "query: We are working on an application that needs to automatically label workout videos to add them to the correct category."}
{"completion": "chinese-clip-vit-large-patch14", "text": "document: Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks."}
{"completion": "chinese-clip-vit-large-patch14", "text": "query: Our new project is an AI-Powered content filtering tool that checks users' input data on various platforms. We need to check the appropriateness of an image based on given categories."}
{"completion": "CompVis/ldm-celebahq-256", "text": "document: Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs."}
{"completion": "CompVis/ldm-celebahq-256", "text": "query: I want to develop an app to generate high-quality images of people for fashion advertisement purposes."}
{"completion": "d4data/Indian-voice-cloning", "text": "document: A model for detecting voice activity in Indian languages."}
{"completion": "d4data/Indian-voice-cloning", "text": "query: We need to analyze a podcast on Indian heritage and culture for a client who is doing research. Identify the segments where the speaker is talking."}
{"completion": "superb/hubert-large-superb-sid", "text": "document: Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification."}
{"completion": "superb/hubert-large-superb-sid", "text": "query: Create a tool that can filter podcast episodes based on the speaker's identity and find the top 5 matching speakers."}
{"completion": "lllyasviel/control_v11p_sd15s2_lineart_anime", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images."}
{"completion": "lllyasviel/control_v11p_sd15s2_lineart_anime", "text": "query: Our company wants to generate artwork for a manga-style comic series. We need to create lineart sketches of characters and scenes based on text descriptions."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "query: As a podcast editor, I need a tool to separate the voices in the audio recordings of my podcast episodes automatically."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "document: TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "query: We need to analyze the population of different countries but the data is spread in a table format. How can we use AI to extract specific information from the table?"}
{"completion": "Lykon/DreamShaper", "text": "document: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper"}
{"completion": "Lykon/DreamShaper", "text": "query: An art gallery wants to display paintings inspired by the theme \"A serene forest at dawn\". Use DreamShaper to generate an image for them."}
{"completion": "tiny-random-LayoutLMv3ForQuestionAnswering", "text": "document: A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API."}
{"completion": "tiny-random-LayoutLMv3ForQuestionAnswering", "text": "query: Build a program that extracts information from an image of a scanned document by asking questions about it."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "document: This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "query: A German user needs a summary of a long article they just read."}
{"completion": "distilbert-base-uncased", "text": "document: DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. It was pretrained with three objectives: Distillation loss, Masked language modeling (MLM), and Cosine embedding loss. This model is uncased and can be used for masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task."}
{"completion": "distilbert-base-uncased", "text": "query: We are building a content generation tool and need a model to predict the missing words in the sentences to improve the overall quality of the writing."}
{"completion": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024", "text": "document: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024", "text": "query: Our city planning department would like to analyze satellite images for land use classification. We need a tool to segment and classify different areas like roads, buildings, and vegetation."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "document: This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "query: There are some emails circulating in our company which are misleading. We need to filter them based on their content against our company policies."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "document: DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "query: In order to help our customers find information more quickly, we require a model that can answer their questions based on the given context."}
{"completion": "sentence-transformers/distilbert-base-nli-mean-tokens", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/distilbert-base-nli-mean-tokens", "text": "query: We are building a search engine for finding relevant articles with similar content. We need to use sentence similarity information."}
{"completion": "Xinhhd/autotrain-zhongxin-contest-49402119333", "text": "document: A multi-class classification model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "Xinhhd/autotrain-zhongxin-contest-49402119333", "text": "query: A car manufacturer needs a solution that can estimate carbon emissions level for the cars they produced based on some given features. Propose a solution using the provided API."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "query: I need a digital illustration for my next children's book. The illustration should feature a kid playing with a cat."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: With the help of the provided model, I want to create a restaurant sentiment analysis tool that will categorize the reviews as positive or negative."}
{"completion": "google/pegasus-large", "text": "document: google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks."}
{"completion": "google/pegasus-large", "text": "query: I need to condense a lengthy news article into a brief summary."}
{"completion": "xlm-roberta-large", "text": "document: XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "xlm-roberta-large", "text": "query: \"Ich habe heute keine Zeit, ich muss zur <mask> gehen.\""}
{"completion": "facebook/timesformer-base-finetuned-k400", "text": "document: TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels."}
{"completion": "facebook/timesformer-base-finetuned-k400", "text": "query: Our marketing team is working on a commercial project. We need to classify video clips for action categories to create a sports promo video."}
{"completion": "opus-mt-fr-es", "text": "document: A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing."}
{"completion": "opus-mt-fr-es", "text": "query: Implement a machine-translate function that gets as input a french sentence and outputs its spanish equivalent."}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al."}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "query: A culinary media firm wants to automatcially categorize cooking tutorial videos by classifying the content of the videos into one of the 400 possible categories."}
{"completion": "laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg", "text": "document: A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on LAION-2B (english), a subset of LAION-5B, using OpenCLIP. The models are trained at 256x256 image resolution and achieve a 75.9 top-1 zero-shot accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg", "text": "query: We're a social media website and have an image sharing service. Please suggest a way to categorize user-uploaded images into animals, vehicles, and landmarks categories."}
{"completion": "Minecraft-Skin-Diffusion", "text": "document: Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods."}
{"completion": "Minecraft-Skin-Diffusion", "text": "query: We are developing a new update for our Minecraft site. Generate a unique, high-quality Minecraft skin for us."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "query: We are building an advertising campaign and want to create videos based on text descriptions. Generate a video of a dog playing with a ball."}
{"completion": "google/pegasus-cnn_dailymail", "text": "document: PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset."}
{"completion": "google/pegasus-cnn_dailymail", "text": "query: We have a news platform, we need to have short summaries for our longer articles for people who are in a hurry."}
{"completion": "distilgpt2", "text": "document: DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more."}
{"completion": "distilgpt2", "text": "query: Please help in generating some creative sentences for marketing purpose."}
{"completion": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "text": "document: A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k."}
{"completion": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "text": "query: We are working on a new mobile application which suggests fashionable clothing. We need to categorize different types of clothing."}
{"completion": "gpt2", "text": "document: GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences."}
{"completion": "gpt2", "text": "query: A robotic science fiction novel is being written. Generate a few sentences to understand the behavior of the robots in the novel."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "document: Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "query: \"Voy a la tienda para comprar algunos comestibles\"."}
{"completion": "Dizex/InstaFoodRoBERTa-NER", "text": "document: InstaFoodRoBERTa-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition of Food entities on informal text (social media like). It has been trained to recognize a single entity: food (FOOD). Specifically, this model is a roberta-base model that was fine-tuned on a dataset consisting of 400 English Instagram posts related to food."}
{"completion": "Dizex/InstaFoodRoBERTa-NER", "text": "query: Our company is developing a food detection AI that analyzes social media posts. We want to identify food items mentioned in the text."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "query: Implement a system to predict the carbon emissions of several cars based on variables like production year, kilometers driven, etc."}
{"completion": "facebook/timesformer-hr-finetuned-k600", "text": "document: TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels."}
{"completion": "facebook/timesformer-hr-finetuned-k600", "text": "query: Our bot should be able to tell if a person is demonstrating a certain athletic skill, such as sprinting, swimming or weightlifting, from short video clips."}
{"completion": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", "text": "query: We need to create an automated system to group similar customer reviews together."}
{"completion": "keremberke/yolov8m-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes."}
{"completion": "keremberke/yolov8m-pothole-segmentation", "text": "query: The local department of transportation needs to detect potholes in images from road inspection drones."}
{"completion": "shi-labs/oneformer_coco_swin_large", "text": "document: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model."}
{"completion": "shi-labs/oneformer_coco_swin_large", "text": "query: semantic, instance, and panoptic segmentations."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "document: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "query: As a geospatial analyst, I need to segment an aerial view of an urban environment into buildings, roads, parks, and other features."}
{"completion": "shi-labs/oneformer_ade20k_swin_large", "text": "document: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model."}
{"completion": "shi-labs/oneformer_ade20k_swin_large", "text": "query: Our company provides image analysis services for real estate images. We need to segment parts of images into categories such as walls, windows, and doors."}
{"completion": "Helsinki-NLP/opus-mt-zh-en", "text": "document: A Chinese to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Marian NMT framework and trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-zh-en", "text": "query: I need to translate Mandarin phrases to English so that I can understand what my manager is saying."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "document: This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization."}
{"completion": "Einmalumdiewelt/T5-Base_GNAD", "text": "query: I'm a German reporter and I need to summarize a long article quickly. Can you help me to build a tool to complete this task?"}
{"completion": "speechbrain/sepformer-whamr", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation."}
{"completion": "speechbrain/sepformer-whamr", "text": "query: We want to create an app to separate voices from a noisy environment for people suffering from hearing problems like tinnitus."}
{"completion": "distilbert-base-multilingual-cased", "text": "document: This model is a distilled version of the BERT base multilingual model. It is trained on the concatenation of Wikipedia in 104 different languages. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters. On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base."}
{"completion": "distilbert-base-multilingual-cased", "text": "query: It is said \"Reading between the lines\", but I need a model that can fill in the blanks in my text automatically when I'm typing."}
{"completion": "Zixtrauce/BaekBot", "text": "document: BaekBot is a conversational model based on the GPT-2 architecture for text generation. It can be used for generating human-like responses in a chat-like environment."}
{"completion": "Zixtrauce/BaekBot", "text": "query: I would like to have a conversation with an AI about an upcoming birthday party."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: A customer asked, \"My son is feeling sick with a fever and sore throat. What should I do?\" Recommend a passage with the helpful information from a list of passages."}
{"completion": "sshleifer/tiny-gpt2", "text": "document: A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt."}
{"completion": "sshleifer/tiny-gpt2", "text": "query: Design advertisements for our shaving cream product and wants creative and catchy taglines."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "query: Our company is in need of an image recognition tool that can determine whether given images contain animals or not."}
{"completion": "laion/CLIP-ViT-g-14-laion2B-s34B-b88K", "text": "document: A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories."}
{"completion": "laion/CLIP-ViT-g-14-laion2B-s34B-b88K", "text": "query: \"dog\", \"cat\", \"squirrel\", \"mongoose\", \"lion\". Help us with that!"}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Arabic. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Arabic using the train and validation splits of Common Voice 6.1 and Arabic Speech Corpus."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "query: Can you please detect an ongoing conversation between two friends in Arabic and give us a transcription?"}
{"completion": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}
{"completion": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "text": "query: I have an audio file that I need to separate the speaker's voice from the background noise."}
{"completion": "Linaqruf/anything-v3.0", "text": "document: A text-to-image model that generates images from text descriptions."}
{"completion": "Linaqruf/anything-v3.0", "text": "query: A digital content creator is looking for a way to create a visual representation of the text \"A cat sitting on a green grass by a tree\"."}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "document: A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images."}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "query: Develop a feature for a mobile app that helps users get information about images by asking questions."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: I am building a Q&A system for my website. I would like to retrieve relevant answers based on user queries using neural search."}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "document: MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!"}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "query: \"\u041e\u043d\u0438 \u043f\u0440\u043e\u0432\u0435\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0443\u044e \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u044e \u0440\u0435\u043a\u043b\u0430\u043c\u044b \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 \u0441 \u0444\u043e\u043a\u0443\u0441\u043e\u043c \u043d\u0430 \u043c\u043e\u043b\u043e\u0434\u0443\u044e \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u044e. \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u043a\u0446\u0438\u0439, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0438\u0432\u043b\u0435\u0447\u044c \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u044e \u0438 \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u0442\u044c \u0438\u0445 \u043f\u0440\u043e\u0434\u0443\u043a\u0446\u0438\u0435\u0439. \u041a\u043e\u043d\u0442\u0435\u043d\u0442 \u0440\u0430\u0437\u043c\u0435\u0449\u0430\u043b\u0441\u044f \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0435\u0442\u044f\u0445, \u043e\u0441\u043d\u043e\u0432\u043d\u0443\u044e \u043a\u043e\u043d\u0446\u0435\u043d\u0442\u0440\u0430\u0446\u0438\u044e \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438 \u0432\u0438\u0434\u0435\u043e\u0440\u043e\u043b\u0438\u043a\u0438 \u0438 \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0438 \u043c\u043d\u043e\u0433\u0438\u0445 \u0437\u0440\u0438\u0442\u0435\u043b\u0435\u0439.\""}
{"completion": "google/pix2struct-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}
{"completion": "google/pix2struct-base", "text": "query: We are developing an app that can generate descriptions for user interface elements on mobile applications. We need to utilize Pix2Struct to achieve this function."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "document: A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "query: Help me to design a language translation system for a hospital emergency ward where the doctors need to translate English to Chinese."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "document: Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "query: A virtual game festival wants to include an AI to help the players in the Gym Hopper environment. Implement an AI-based solution to play the game."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "query: We need to know whether two sentences, one being a news headline, and the other being an advertisement description, are similar."}
{"completion": "tiny-random-VideoMAEForVideoClassification", "text": "document: A tiny random VideoMAE model for video classification."}
{"completion": "tiny-random-VideoMAEForVideoClassification", "text": "query: The marketing department wants to automatically sort promotional videos into categories, like food, technology, and fashion. Help with a solution."}
{"completion": "oliverguhr/german-sentiment-bert", "text": "document: This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews."}
{"completion": "oliverguhr/german-sentiment-bert", "text": "query: A German learning platform wants to analyze the sentiment of user reviews for their courses. Help them classify the sentiment of these reviews."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "document: BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "query: I am the founder of a tutoring company, I want a question and answer system to help my students."}
{"completion": "fcakyon/timesformer-large-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al."}
{"completion": "fcakyon/timesformer-large-finetuned-k400", "text": "query: We are developing an AI-based application to summarize the top sporting events of the day. The model needs to be able to recognize the sport events from videos."}
{"completion": "keras-io/tab_transformer", "text": "document: This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model."}
{"completion": "keras-io/tab_transformer", "text": "query: We are building a smart application to help people with disabilities in understanding their job possibilities. We need to analyze their data. We will start with a tabular Transformer model for Structured Data Learning"}
{"completion": "facebook/timesformer-hr-finetuned-k600", "text": "document: TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels."}
{"completion": "facebook/timesformer-hr-finetuned-k600", "text": "query: Our company is developing security systems and we want to analyze the video footage to recognize any suspicious activities."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "query: A customer wants us to create an image describing \"A colorful sunset over a mountain range\" using text-to-image generation."}
{"completion": "microsoft/tapex-large", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large", "text": "query: In our database of customers, we need to know the number of customers who are between the age of 20 and 35 and living in New York."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: Develop a way to transcribe interviews for reporters."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "document: Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "query: We are developing a smart speaker product and need to implement a keyword detection function. Can you help us do that?"}
{"completion": "ppo-Pendulum-v1", "text": "document: This is a trained model of a PPO agent playing Pendulum-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-Pendulum-v1", "text": "query: We need to train a new robot arm for gripping. The environment is like a pendulum and the robot must learn how to balance the pendulum."}
{"completion": "google/maxim-s3-deblurring-gopro", "text": "document: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository."}
{"completion": "google/maxim-s3-deblurring-gopro", "text": "query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "document: Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "query: I received an email in Spanish from a client, but I only speak English. I need a translator to understand the content."}
{"completion": "t5-small", "text": "document: T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks."}
{"completion": "t5-small", "text": "query: We have written an article about the health benefits of owning a dog, but people don't have time to read it. We need to summarize it."}
{"completion": "financial-summarization-pegasus", "text": "document: This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization."}
{"completion": "financial-summarization-pegasus", "text": "query: We are establishing a publication firm for financial news. We need to condense long articles from file 'news_long.txt' into summaries."}
{"completion": "facebook/detr-resnet-101-dc5", "text": "document: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set."}
{"completion": "facebook/detr-resnet-101-dc5", "text": "query: We have to detect objects present in a given image URL and classify them using an appropriate pre-trained model."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: Your client uploaded a list of similar sentences and wants you to cluster them to find the most similar groups."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "query: Estimate the carbon emissions generated by different home appliances based on their energy usage data."}
{"completion": "git-large-textvqa", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-textvqa", "text": "query:  Our team is building a Visual Question Answering system to improve our customer service response time."}
{"completion": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}
{"completion": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "text": "query: Design a system for enhancing the quality of audio files, especially when multiple people are speaking at once."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "query: I am a designer, I need to generate a picture of an interior for presentation purposes by making use of the controlnet API."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "document: A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "query: I am building an interactive tool to identify fruits based on images. I need a model to classify the images of various fruits."}
{"completion": "google/flan-t5-xl", "text": "document: FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models."}
{"completion": "google/flan-t5-xl", "text": "query: Create a powerful translation tool that translates an English sentence/question into German."}
{"completion": "google/t5-v1_1-base", "text": "document: Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks."}
{"completion": "google/t5-v1_1-base", "text": "query: Social Butterfly Club is planning an event and needs to send out an invitation but lost its words. Generate a short description that'd sell this in a click."}
{"completion": "ivelin/donut-refexp-combined-v1", "text": "document: A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question."}
{"completion": "ivelin/donut-refexp-combined-v1", "text": "query: We are designing an exhibit in which visitors can ask questions about images. We want to create a system that answers their questions based on the images."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "document: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "query: We are building a safety compliance checking system for construction sites. We need to find if workers are wearing hard hats in the given image."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "document: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead']."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "query: Global Offensive. We want an AI-based object detection for tracking from just a screenshot."}
{"completion": "MCG-NJU/videomae-base-finetuned-ssv2", "text": "document: VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-finetuned-ssv2", "text": "query: Create a sports highlights analyzer to detect key moments or events from a match video."}
{"completion": "flair/ner-english-ontonotes", "text": "document: This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-ontonotes", "text": "query: I'm creating news chatbots for flight center and I want to extract useful information like names, dates, and locations from the text."}
{"completion": "ckiplab/bert-base-chinese-ws", "text": "document: This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition)."}
{"completion": "ckiplab/bert-base-chinese-ws", "text": "query: Jimmy is looking for a program that can segment long Chinese sentences into words and phrases. Can you help him build a model?"}
{"completion": "ahotrod/electra_large_discriminator_squad2_512", "text": "document: ELECTRA_large_discriminator language model fine-tuned on SQuAD2.0 for question answering tasks."}
{"completion": "ahotrod/electra_large_discriminator_squad2_512", "text": "query: As a university, we need to compile a list of frequently asked questions (FAQs) for incoming students. To do this, we need to analyze various documents and extract answers to these questions."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "document: Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "query: We are developing an autonomous robot with language understanding capabilities, and we need to determine whether the given output sentence logically follows the input sentence."}
{"completion": "pszemraj/long-t5-tglobal-base-16384-book-summary", "text": "document: A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text."}
{"completion": "pszemraj/long-t5-tglobal-base-16384-book-summary", "text": "query: We are a publishing house and we need to create summaries for the books we are releasing."}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: Design a system for a Spanish language learning app to convert the text explanations of language concepts into speech."}
{"completion": "ToddGoldfarb/Cadet-Tiny", "text": "document: Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model."}
{"completion": "ToddGoldfarb/Cadet-Tiny", "text": "query: Our startup aims at developing a conversational AI for customer support and we want to run it on our web server. We can't use a large model because of our server limitations."}
{"completion": "microsoft/wavlm-large", "text": "document: WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}
{"completion": "microsoft/wavlm-large", "text": "query: Develop a voice assistant that takes speech input from users and transcribes it into text, so it can perform various tasks based on the content of the speech."}
{"completion": "microsoft/DialoGPT-medium", "text": "document: DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread."}
{"completion": "microsoft/DialoGPT-medium", "text": "query: Our chatbot system needs to generate human-like responses to user input. We require an implementation to provide the chatbot with the capability to generate those responses."}
{"completion": "opus-mt-de-en", "text": "document: A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library."}
{"completion": "opus-mt-de-en", "text": "query: A German article needs to be translated into English for our international clients."}
{"completion": "naver-clova-ix/donut-base", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base", "text": "query: We are a news organization and we need to automatically caption the images we use in our articles."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: Our company is building an application to help customers make decisions about restaurants. We need to identify whether the given restaurant review is positive or negative."}
{"completion": "microsoft/git-base-vqav2", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}
{"completion": "microsoft/git-base-vqav2", "text": "query: We are a smartphone company and we need to build an app to respond the question about an image taken from the smartphone."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "document: This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "query: I am thinking about founding a green NGO. I'd like to predict carbon emissions in a certain area so I can determine how effective our efforts are."}
{"completion": "dslim/bert-base-NER-uncased", "text": "document: A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text."}
{"completion": "dslim/bert-base-NER-uncased", "text": "query: Provide a report about the number of people, organizations, and locations mentioned in a news article."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: I work at a law firm and need to analyze scanned documents for specific information."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "document: This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "query: Our company receives global customer inquiries, and we have a list of questions. I need your help in multilingual question answering."}
{"completion": "google/flan-t5-large", "text": "document: FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research."}
{"completion": "google/flan-t5-large", "text": "query: One of my friends just sent me a message in English, and I would like to have it translated into German."}
{"completion": "microsoft/tapex-large", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large", "text": "query: I need a table question-answering system that can extract information from a given table and answer my questions."}
{"completion": "julien-c/wine-quality", "text": "document: A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya."}
{"completion": "julien-c/wine-quality", "text": "query: Our company needs to perform quality control on the wine we produce. Can you provide a model that can help us classify the quality of our product based on its chemical properties?"}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "query: We own a news website and we want to give user suggestions based on what they have searched."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "document: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "query: We need to create a custom trading card game. Can you generate an image of a mystical creature based on a description?"}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "query: The company is building a product to recommend videos of cooking tutorials to users based on their preferences. We need to figure out the categories of videos based on their content."}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "document: GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs."}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "query: Design a chatbot that can respond empathically to a user's questions by incorporating context from a given dialog."}
{"completion": "lllyasviel/sd-controlnet-canny", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-canny", "text": "query: Create a visual effect on art focused on the edges in the image to generate a new art piece."}
{"completion": "google/mobilenet_v2_1.0_224", "text": "document: MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v2_1.0_224", "text": "query: As a software engineer at an AI-powered photo sharing app, we need to classify user uploaded images into various categories."}
{"completion": "microsoft/tapex-base", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base", "text": "query: I am a data scientist who works at a tech company. I have a dataset containing information about Olympic Games, and I want to retrieve the year when the games were held in Beijing."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "document: A Korean Table Question Answering model finetuned on the korwikitq dataset."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "query: We have a table of population data of different countries. Let's figure out which country has the highest population."}
{"completion": "microsoft/deberta-base", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data."}
{"completion": "microsoft/deberta-base", "text": "query: \"The capital of Italy is [MASK].\""}
{"completion": "padmalcom/tts-tacotron2-german", "text": "document: Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently."}
{"completion": "padmalcom/tts-tacotron2-german", "text": "query: We need a German text-to-speech application that converts the given text to an audio file using Tacotron2 and HIFIGAN."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "query: The company is working on autonomous cars. I would like to estimate depth from images to understand the surrounding environment."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Arabic. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Arabic using the train and validation splits of Common Voice 6.1 and Arabic Speech Corpus."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "query: Our marketing team wants to transcribe Arabic audio files into text. Please provide a model and code to perform this task."}
{"completion": "vicgalle/xlm-roberta-large-xnli-anli", "text": "document: XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification."}
{"completion": "vicgalle/xlm-roberta-large-xnli-anli", "text": "query: Apprehend a robot athlete who needs to be able to categorize a sports activity during practice sessions."}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "document: This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China."}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "query: I am building a home automation system. Whenever my guests or family ask questions, the system should answer by analyzing the images in the room."}
{"completion": "xlnet-base-cased", "text": "document: XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context."}
{"completion": "xlnet-base-cased", "text": "query: 'The Importance of Physical Exercise for Mental Health'."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "document: This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "query: The local government wants to calculate the CO2 emissions for several different scenarios. Help them get the carbon emissions based on the given input data."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "document: A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "query: Our customer is an e-commerce company in the Middle-East. They want to translate their product descriptions from English to Arabic."}
{"completion": "lllyasviel/control_v11p_sd15_inpaint", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images."}
{"completion": "lllyasviel/control_v11p_sd15_inpaint", "text": "query: In a mobile game, we want to introduce a feature to automatically remove unwanted parts from images uploaded by users. Please propose a solution for this."}
{"completion": "tiny-random-CLIPSegModel", "text": "document: A tiny random CLIPSegModel for zero-shot image classification."}
{"completion": "tiny-random-CLIPSegModel", "text": "query: We want to analyze an image and produce a list of relevant keywords/tags to describe this image."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "document: BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "query: A user wants to know the capital of France from a given text passage. Provide a solution using BERT large model."}
{"completion": "blip2-flan-t5-xxl", "text": "document: BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xxl", "text": "query: We are a company producing smart TV's and we need a model to generate textual information about images and answer questions."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "document: Multilingual XLM-RoBERTa large model for extractive question answering on various languages. Trained on SQuAD 2.0 dataset and evaluated on SQuAD dev set, German MLQA, and German XQuAD."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "query: A user is asking about the importance of model conversion in AI. Use the given context to provide an appropriate answer."}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "document: Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0. The model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning Wav2Vec2-Large-Robust on MSP-Podcast (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An ONNX export of the model is available from doi:10.5281/zenodo.6221127. Further details are given in the associated paper and tutorial."}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "query: I want to analyze an audio file and determine its emotion in terms of arousal, dominance, and valence."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "query: We want to create an application for the visually impaired that estimates the depth of objects in front of them. Build a solution for this task."}
{"completion": "edbeeching/decision-transformer-gym-hopper-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment"}
{"completion": "edbeeching/decision-transformer-gym-hopper-expert", "text": "query: Our next big project is to build an AI-driven auto trading bot for financial markets. Find the best React library to create realistic gesture-based navigation for mobile apps."}
{"completion": "Raiden-1001/poca-Soccerv7.1", "text": "document: A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "Raiden-1001/poca-Soccerv7.1", "text": "query: My company is having a farewell party for a retiring employee, and we want to entertain the guests using an AI-controlled soccer game."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "query: I want a tool for my blog dashboard that shows a list of articles and sorts them based on how relevant they are to a given topic."}
{"completion": "facebook/blenderbot_small-90M", "text": "document: Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation."}
{"completion": "facebook/blenderbot_small-90M", "text": "query: Implement a conversational chatbot that can help answer questions in different subjects."}
{"completion": "flair/ner-english-fast", "text": "document: This is the fast 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-fast", "text": "query: We have an article about the presidents of the United States. Extract the names of people and their locations in the article."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "query: Analyze similarities between the given documents and the search query."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: Our company needs to analyze a series of documents to answer questions about them. Please provide guidance on how to create a working model to solve this problem."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: We are a transcription service company and we need to restore punctuation in transcribed spoken language."}
{"completion": "microsoft/table-transformer-structure-recognition", "text": "document: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables."}
{"completion": "microsoft/table-transformer-structure-recognition", "text": "query: We are working on a project that needs to extract table structures from images of pages containing tables. Make use of the table transformer model to detect the rows and columns."}
{"completion": "git-large-r-textcaps", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-r-textcaps", "text": "query: We are an e-commerce website. Help us to automatically label the products we received daily with brief descriptions."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "document: A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "query: Design a computer vision solution to identify rare bird species. Prepare the main code segment to be used for classifying the bird species in images."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "document: Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "query: An AI tool is being developed by the company to help with creative writing. The software needs to create a sentence involving three specific words."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "document: GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "query: A nonprofit organization needs a statement describing their mission to eradicate poverty through education."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: Our company wants a short headline of the news on the current situation of COVID-19 vaccine misinformation."}
{"completion": "glpn-kitti-finetuned-diode", "text": "document: This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset."}
{"completion": "glpn-kitti-finetuned-diode", "text": "query: I need a model to estimate depth from a given single image."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "document: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "query: Our firm designs merchandise for pet lovers. We need to create a new design of a cat wearing a crown."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "document: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "query: Create image variations using stable diffusion with guidance scale 3 and save them as a result."}
{"completion": "microsoft/git-base", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "microsoft/git-base", "text": "query: A supermarket wants to advertise their products online. They need a short description of an image showing fruit and vegetable stands. Can you generate a suitable caption for the image?"}
{"completion": "EleutherAI/gpt-j-6B", "text": "document: GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI."}
{"completion": "EleutherAI/gpt-j-6B", "text": "query: I am the CEO of a technology company. I need a brief introduction about my company for my LinkedIn profile."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset."}
{"completion": "tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa", "text": "query: I am a researcher studying Vietnamese data extraction. I want to extract information from a Vietnamese document to answer questions."}
{"completion": "facebook/opt-6.7b", "text": "document: OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example."}
{"completion": "facebook/opt-6.7b", "text": "query: Create a paragraph of text that could be used as the introductory paragraph of a newspaper article describing the recent discovery of a new element."}
{"completion": "shi-labs/oneformer_ade20k_swin_tiny", "text": "document: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model."}
{"completion": "shi-labs/oneformer_ade20k_swin_tiny", "text": "query: Develop a system to automatically identify plants and animals in a given image. We need to differentiate and label different types of plants, animals, and their surroundings."}
{"completion": "deepset/tinyroberta-squad2", "text": "document: This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model."}
{"completion": "deepset/tinyroberta-squad2", "text": "query: We need to retrieve some information from a support document. Help us in getting details about a topic from given text."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "document: This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "query: We're building a virtual assistant to support people's practice of spoken numbers (0-9) by verifying if they pronounced the digits correctly. Show me how to use this model to accomplish this task."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: I manage a bank, and I need a model to predict whether a person will default on a loan application using their income history and other data."}
{"completion": "tinkoff-ai/ruDialoGPT-medium", "text": "document: This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents. The model was trained with context size 3."}
{"completion": "tinkoff-ai/ruDialoGPT-medium", "text": "query: We are developing an AI-powered concierge service for a hotel chain. We need the system to be able to understand and generate human-like responses to guest inquiries."}
{"completion": "lllyasviel/control_v11p_sd15_canny", "text": "document: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_canny", "text": "query: A kids' channel is creating images for its book series, they require an image based on \"a group of colorful animals playing together.\""}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10", "text": "document: A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10", "text": "query: An educational website wants to provide an option to have the content on the website, available in multiple languages, spoken aloud at the user's request."}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "query: We are building a mobile app that helps users park their car safely. In order to provide a great user experience, we need to estimate the distance between the car and various objects."}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "query: We have an architectural project and we are looking for a way to estimate the depth of an image."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "document: A model trained for binary classification of carbon emissions using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "query: Find a way to build a model which will help in the classification of carbon emissions."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "document: This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "query: We are the staff of a customer support center. We have collected different voice records to analyze customer's emotions."}
{"completion": "xlnet-base-cased", "text": "document: XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context."}
{"completion": "xlnet-base-cased", "text": "query: Our HR department is reviewing candidate cover letters, and they need assistance in creating a summary for each letter. "}
{"completion": "opus-mt-de-en", "text": "document: A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library."}
{"completion": "opus-mt-de-en", "text": "query: Translate German sales emails to English so that the sales team can address the clients' needs."}
{"completion": "sepformer-wham-enhancement", "text": "document: This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k."}
{"completion": "sepformer-wham-enhancement", "text": "query: Our office is in a loud environment, and we record meetings for minutes. We need to remove the noise from the recordings."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "query: I am trying to measure the similarity between product descriptions and customer reviews to find the most relevant reviews for each product."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "query: We are working on a travel agency to advise visitors on information about the former host cities of the Olympic Games given the years they hosted them, so we want to extract the city hosting the games in 2012."}
{"completion": "microsoft/trocr-small-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository."}
{"completion": "microsoft/trocr-small-handwritten", "text": "query: Our company deals with handwritten documents. Isolate the text from a handwritten document of our user and store it for future processing."}
{"completion": "prompthero/openjourney", "text": "document: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts."}
{"completion": "prompthero/openjourney", "text": "query: I need an AI artwork as cover art for my upcoming techno album. The album is called \"Cyber Flux\". I want the generated image to feature a futuristic city landscape with neon lights."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "document: A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "query: We are an international organization who prepares to make an announcement to the Arabic speaking-world, the announcement is in an .txt format. We need a translator."}
{"completion": "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP."}
{"completion": "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg", "text": "query: I have a picture of an animal that I do not know what it is. I need to identify the animal in this photo."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "query: Help an interior design team to estimate the depth of objects in a photo to decide appropriate decoration placements."}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "query: We are a software company that wants to integrate depth estimation in our product for visually-impaired users. We need to estimate the depth of different objects in the environment."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "document: Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "query: Our new marketing campaign for the global market is launched today. Please translate the following Spanish text to English for our website."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "query: Our school has been receiving reviews on social media platforms, and we need to categorize them into positive, negative, or neutral sentiment."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "document: High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "query: Our dating app wants to generate high-quality images of people for testing purposes."}
{"completion": "t5-large", "text": "document: T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks."}
{"completion": "t5-large", "text": "query: I'm going to present a business update to a group of stakeholders, and I want to use artificial intelligence to summarize the key points of a lengthy document for my presentation. Prepare me for the presentation."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "document: A text-to-speech model trained on mtedx, covost2, europarl_st, and voxpopuli datasets for English, French, Spanish, and Italian languages. Licensed under cc-by-nc-4.0."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "query:  I'm creating a language learning app, and tasked with providing translation and pronunciation for a given text in English, French, Spanish or Italian."}
{"completion": "facebook/mask2former-swin-large-cityscapes-semantic", "text": "document: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency."}
{"completion": "facebook/mask2former-swin-large-cityscapes-semantic", "text": "query: Our traffic management department needs assistance in identifying traffic signs, vehicles, and pedestrians from surveillance camera images."}
{"completion": "facebook/tts_transformer-zh-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Simplified Chinese, Single-speaker female voice, Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-zh-cv7_css10", "text": "query: We have a podcast about Chinese culture, and we need to create an introduction in Simplified Chinese."}
{"completion": "google/vit-base-patch16-384", "text": "document: Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."}
{"completion": "google/vit-base-patch16-384", "text": "query: http://images.cocodataset.org/val2017/000000039769.jpg"}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "document: A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "query: We have a customer support team to address customer complaints. Help categorize the complaints according to their subject and urgency."}
{"completion": "microsoft/git-base-coco", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "microsoft/git-base-coco", "text": "query: We are an ecommerce company and want to generate descriptions for the products in our inventory"}
{"completion": "facebook/convnext-base-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification."}
{"completion": "facebook/convnext-base-224", "text": "query: Develop a recommendation engine for an online store that offers personalized discounts based on user's favorite categories of mobile phone images."}
{"completion": "Lykon/DreamShaper", "text": "document: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper"}
{"completion": "Lykon/DreamShaper", "text": "query: Our client is an artist in California, his request is to see a visual representation of the beach with palm trees and a sunset."}
{"completion": "lysandre/tapas-temporary-repo", "text": "document: TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up."}
{"completion": "lysandre/tapas-temporary-repo", "text": "query: I want to create a bot able to answer queries based on tabular data."}
{"completion": "flair/upos-english", "text": "document: This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/upos-english", "text": "query: Discover part of speech tags for given sentence."}
{"completion": "runwayml/stable-diffusion-v1-5", "text": "document: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input."}
{"completion": "runwayml/stable-diffusion-v1-5", "text": "query: A marketing company wants to create a high-resolution image with inputted text related to \"a futuristic city with flying cars\"."}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "document: This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets."}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "query: I want to generate a unique abstract artwork for my home based on the prompt \"Magical Winter Landscape\"."}
{"completion": "vintedois-diffusion-v0-1", "text": "document: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps."}
{"completion": "vintedois-diffusion-v0-1", "text": "query: Create an image of a sunny beach with a single palm tree and a surfer riding a wave to display on our travel website."}
{"completion": "microsoft/resnet-50", "text": "document: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback."}
{"completion": "microsoft/resnet-50", "text": "query: Our company has recently developed a new toy robot for children. We need to build a classifier for the robot to recognize objects in pictures."}
{"completion": "microsoft/codebert-base", "text": "document: Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective."}
{"completion": "microsoft/codebert-base", "text": "query: We are building a code-search tool for our internal programming project. We need to extract features from the code and the accompanying text."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "query: I work for a talent agency, and I need a tool that answers queries about the actors' details from a database in natural language format."}
{"completion": "lllyasviel/control_v11p_sd15_softedge", "text": "document: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_softedge", "text": "query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges."}
{"completion": "pyannote/overlapped-speech-detection", "text": "document: Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file."}
{"completion": "pyannote/overlapped-speech-detection", "text": "query: Give me an example to build a virtual assistant that helps in recording meeting minutes by detecting overlapping speeches."}
{"completion": "blip-image-captioning-base", "text": "document: BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone)."}
{"completion": "blip-image-captioning-base", "text": "query: Develop an application that can generate captions for images. The app should be capable of describing an image using natural language."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "document: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "query: Generate a photo that would be representative of a landscape scene that includes a beautiful sunset by the ocean, a lighthouse, and a small fishing boat."}
{"completion": "openai/clip-vit-large-patch14-336", "text": "document: This model was trained from scratch on an unknown dataset."}
{"completion": "openai/clip-vit-large-patch14-336", "text": "query: We are working on a web application that allows users to analyze Instagram images and provide context."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "document: A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "query: I'd like to make a prediction with the model on a new set of data containing features about houses and get the estimated prices."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "document: TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-k600", "text": "query: Our company received a video for evaluation, and we want to classify the action in it."}
{"completion": "xm_transformer_s2ut_en-hk", "text": "document: Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain."}
{"completion": "xm_transformer_s2ut_en-hk", "text": "query: We have a project that aims to translate spoken English into spoken Hokkien. We need the English speech translated and converted into Hokkien speech."}
{"completion": "swin-tiny-patch4-window7-224-bottom_cleaned_data", "text": "document: This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset."}
{"completion": "swin-tiny-patch4-window7-224-bottom_cleaned_data", "text": "query: A startup is trying to detect the type of object present in an image for their application. They are seeking an image classification model for their project."}
{"completion": "srg/outhimar_64-Close-regression", "text": "document: Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt."}
{"completion": "srg/outhimar_64-Close-regression", "text": "query: We are building an automated stock trading platform. The goal is to predict the closing price of specific stocks."}
{"completion": "ppo-seals-CartPole-v0", "text": "document: This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-seals-CartPole-v0", "text": "query: We are creating an AI with the ability to learn how to keep the pole in CartPole upright using deep reinforcement learning."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "query: We are developing a parking assistance system that needs to estimate the distance to objects in a 3D scene. Can you create a model using depth estimation?"}
{"completion": "microsoft/xclip-base-patch32", "text": "document: X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."}
{"completion": "microsoft/xclip-base-patch32", "text": "query: We are designing a video sharing platform. In order to organize our videos into categories for easy searching, we want to automatically assign tags based on their content."}
{"completion": "kredor/punctuate-all", "text": "document: A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian."}
{"completion": "kredor/punctuate-all", "text": "query: I'm working on a transcription service and I need a way to add punctuation to transcribed text."}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "document: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels."}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "query: Global Offensive (CS:GO) players in a given image URL and visualize the result."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "query: We are building a video game for the children's museum. We need to create an AI player for the Breakout game."}
{"completion": "microsoft/swin-tiny-patch4-window7-224", "text": "document: Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks."}
{"completion": "microsoft/swin-tiny-patch4-window7-224", "text": "query: A website is focusing on categorizing photographs of animals from around the world. They need a model to help them predict the category of new images they receive."}
{"completion": "cross-encoder/nli-deberta-v3-base", "text": "document: This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-base", "text": "query: A magazine recently approached us with the task of analyzing relationships between headlines and the actual story content. We need to automate this process."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "document: Multilingual XLM-RoBERTa large model for extractive question answering on various languages. Trained on SQuAD 2.0 dataset and evaluated on SQuAD dev set, German MLQA, and German XQuAD."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "query: Can you design a Multi-Lingual Question Answering model and clarify the steps to implement?  "}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "document: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "query: We are working on generating images for a fantastical video game. Generate a fantasy scene based on a given text description."}
{"completion": "Salesforce/codegen-2B-multi", "text": "document: CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters."}
{"completion": "Salesforce/codegen-2B-multi", "text": "query: There's an upcoming competition where we need to create programs from a functional description. Can you devise a solution for this?"}
{"completion": "google/tapas-large-finetuned-wikisql-supervised", "text": "document: TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table."}
{"completion": "google/tapas-large-finetuned-wikisql-supervised", "text": "query: We need to categorize the sales data of different products into categories of highest sales and lowest sales from the given table. Provide a code snippet to answer this using a table-question-answering model."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "document: This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data."}
{"completion": "pcoloc/autotrain-600-dragino-1839063122", "text": "query: Please predict the carbon emissions of a vehicle given its features."}
{"completion": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224", "text": "document: BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering."}
{"completion": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224", "text": "query: \"pneumonia\", \"fracture\", or \"normal\"."}
{"completion": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech."}
{"completion": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD", "text": "query: Our customer support team needs to review customer sentiment based on recorded audio calls. Can your solution analyze the sentiment of these audio recordings?"}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: We are planning to use Machine Learning to build a flower classification application. The application needs to be able to predict the label of flowers from their attributes."}
{"completion": "SYSPIN/Telugu_Male_TTS", "text": "document: A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face."}
{"completion": "SYSPIN/Telugu_Male_TTS", "text": "query: Create a text-to-speech conversion system in Telugu, using a male voice."}
{"completion": "swin2SR-classical-sr-x4-64", "text": "document: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution."}
{"completion": "swin2SR-classical-sr-x4-64", "text": "query: I am sending a low-resolution image of a historical document and would like it to be enhanced."}
{"completion": "BaptisteDoyen/camembert-base-xnli", "text": "document: Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French."}
{"completion": "BaptisteDoyen/camembert-base-xnli", "text": "query: A french newspaper is looking for a way to detect the main theme of an article."}
{"completion": "sepformer-wsj02mix", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset."}
{"completion": "sepformer-wsj02mix", "text": "query: I am a musician. I want to isolate the vocals and instruments from a recording of a live performance."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "query: We want to implement a depth-sensing app that uses a smartphone's camera. Please help to estimate the depth."}
{"completion": "mgp-str", "text": "document: MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images."}
{"completion": "mgp-str", "text": "query: A sign translation app needs to be built and we want to extract text strings from images of signboards."}
{"completion": "fxmarty/resnet-tiny-beans", "text": "document: A model trained on the beans dataset, just for testing and having a really tiny model."}
{"completion": "fxmarty/resnet-tiny-beans", "text": "query: I am looking for a computer vision model to identify the type of beans in a given image."}
{"completion": "kykim/bertshared-kor-base", "text": "document: Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks."}
{"completion": "kykim/bertshared-kor-base", "text": "query: Let's devise a method to convert Korean questions to answers in sentences."}
{"completion": "bert-large-cased", "text": "document: BERT large model (cased) pretrained on English language using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters."}
{"completion": "bert-large-cased", "text": "query: \"I recently bought a new [MASK] for my kitchen\""}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "document: LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "query: I want my text to be passed through the Legal-BERT model to make it applicable as a legal document."}
{"completion": "clip-vit-base-patch32-ko", "text": "document: Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data."}
{"completion": "clip-vit-base-patch32-ko", "text": "query: We are developing a Korean news website, and we need a tool to categorize images based on their content without any specific training data. Obtain the suitable API and prepare an example code to test it."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "document: roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "query: Help me create a resume screening tool, which can extract the name, organization, and location from the given text."}
{"completion": "indobenchmark/indobert-base-p1", "text": "document: IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective."}
{"completion": "indobenchmark/indobert-base-p1", "text": "query: I have a dataset in the Indonesian language, and I want to get the hidden representation of the data using a model specialized in the Indonesian language."}
{"completion": "al02783013/autotrain-faseiii_diciembre-2311773112", "text": "document: A tabular regression model trained using AutoTrain to predict carbon emissions based on input features."}
{"completion": "al02783013/autotrain-faseiii_diciembre-2311773112", "text": "query: We are an environmental organization, and we want to estimate the carbon emissions produced by a set of processes."}
{"completion": "speechbrain/mtl-mimic-voicebank", "text": "document: This repository provides all the necessary tools to perform enhancement and\nrobust ASR training (EN) within\nSpeechBrain. For a better experience we encourage you to learn more about\nSpeechBrain. The model performance is:\nRelease\nTest PESQ\nTest COVL\nValid WER\nTest WER\n22-06-21\n3.05\n3.74\n2.89\n2.80\nWorks with SpeechBrain v0.5.12"}
{"completion": "speechbrain/mtl-mimic-voicebank", "text": "query: An audio engineering team is working on speech enhancement for noisy recorded audio files. Assist them with a solution to improve audio clarity."}
{"completion": "keremberke/yolov8n-blood-cell-detection", "text": "document: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset."}
{"completion": "keremberke/yolov8n-blood-cell-detection", "text": "query: I run a medical lab for blood sample analysis, I want to identify blood cells in the samples."}
{"completion": "optimum/t5-small", "text": "document: T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization."}
{"completion": "optimum/t5-small", "text": "query: I want to create a tool that automatically translates English sentences into French sentences for my language learning app."}
{"completion": "facebook/m2m100_418M", "text": "document: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token."}
{"completion": "facebook/m2m100_418M", "text": "query: A company is setting up a website that will be able to translate text to multiple languages."}
{"completion": "deepset/roberta-base-squad2", "text": "document: This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions."}
{"completion": "deepset/roberta-base-squad2", "text": "query: A geography teacher would like to find the correct answer to certain student's questions, using a system that can find the answers online."}
{"completion": "speechbrain/emotion-recognition-wav2vec2-IEMOCAP", "text": "document: This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data."}
{"completion": "speechbrain/emotion-recognition-wav2vec2-IEMOCAP", "text": "query: I am building a software to detect emotions in audio for customer service improvement. Can you give me an example code on how to detect emotions in a single audio file?"}
{"completion": "pyannote/segmentation", "text": "document: A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework."}
{"completion": "pyannote/segmentation", "text": "query: We are building a virtual assistant that can analyze audio files and detect voice activities, overlaying speech, and perform speaker segmentation. Suggest an appropriate method."}
{"completion": "google/flan-t5-base", "text": "document: FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering."}
{"completion": "google/flan-t5-base", "text": "query: We are building a translation application that will be able to translate text from English to German. What transformers model should we be using?"}
{"completion": "facebook/wav2vec2-base-960h", "text": "document: Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files."}
{"completion": "facebook/wav2vec2-base-960h", "text": "query: I want to transcribe a recorded conversation between three people that are discussing a project."}
{"completion": "dpt-large-redesign", "text": "document: A depth estimation model based on the DPT architecture."}
{"completion": "dpt-large-redesign", "text": "query: You have a 3D application where you want to understand the depth of objects. Compute the depth based on rgb input image conversion."}
{"completion": "facebook/detr-resnet-50", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50", "text": "query: A delivery company wants to make package identification more efficient. They want to identify different types of shipping boxes within a warehouse using an object detection model."}
{"completion": "facebook/blenderbot-400M-distill", "text": "document: BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX."}
{"completion": "facebook/blenderbot-400M-distill", "text": "query: I want to create a conversational AI chatbot that can help me book a hotel room."}
{"completion": "impira/layoutlm-document-qa", "text": "document: A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}
{"completion": "impira/layoutlm-document-qa", "text": "query: I want to extract the necessary information from an invoice for finance tracking software. Can you find who the customer is on the invoice?"}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "document: A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images."}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "query: Build a chatbot that recommends places to visit based on pictures of the places."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "document: Helsinki-NLP/opus-mt-fi-en is a machine translation model for translating Finnish text to English text. It is trained on the OPUS dataset and can be used with the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "query: I have a paragraph in Finnish and I want it translated to English."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "query: I have some noisy audio coming from my home assistant and I want to clean the audio using ConvTasNet for clear command detection."}
{"completion": "it5-base-news-summarization", "text": "document: IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation."}
{"completion": "it5-base-news-summarization", "text": "query: There is an Italian article in my company's dashboard, and I want you to provide me with a summary of the content."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "document: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture."}
{"completion": "darkstorm2150/Protogen_v2.2_Official_Release", "text": "query: We are trying to create a high-quality image from text describing a scene with a dragon overlooking a village."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "document: This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques."}
{"completion": "mrm8488/bert-multi-cased-finetuned-xquadv1", "text": "query: Let's analyze a case where a user wants to ask questions in different languages from the context they provide."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: We are hosting an international conference with many speakers that use multiple languages. We want to add punctuation to the automatic transcriptions of the speakers."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "document: This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details)."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "query: Our company collects customer feedback on its products. The goal is to analyze the customer feedback and understand the sentiment behind them."}
{"completion": "wavymulder/Analog-Diffusion", "text": "document: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}
{"completion": "wavymulder/Analog-Diffusion", "text": "query: A client is interested in an advertising campaign incorporating analog-style images, and they would like a car in the beach setting."}
{"completion": "microsoft/xclip-base-patch16-zero-shot", "text": "document: X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."}
{"completion": "microsoft/xclip-base-patch16-zero-shot", "text": "query: Computer Vision Video Classification"}
{"completion": "timm/mobilenetv3_large_100.ra_in1k", "text": "document: A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup."}
{"completion": "timm/mobilenetv3_large_100.ra_in1k", "text": "query: I need to create an image classifier for a mobile app. The first step is to find a model with a compact architecture."}
{"completion": "pygmalion-2.7b", "text": "document: Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-2.7b", "text": "query: I am trying to build an AI chatbot with a fictional character personality. I want the AI to provide responses based on the character's persona."}
{"completion": "facebook/mbart-large-50", "text": "document: mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper."}
{"completion": "facebook/mbart-large-50", "text": "query: Create a translation from English article about climate change to Russian language."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "query: We are working on a project to enhance the architecture of a building. We need to process images of the building and identify straight lines."}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: We are an online streaming service company that wants to leverage audio analysis to provide personalized recommendations. We require an audio spectrogram transformer for this purpose."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "document: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "query: We are an insurance company that processes forms, but we need to speed up the process by extracting tables from forms. "}
{"completion": "microsoft/deberta-v2-xxlarge", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data."}
{"completion": "microsoft/deberta-v2-xxlarge", "text": "query: \"During summer, the weather is usually very...\""}
{"completion": "facebook/bart-large-mnli", "text": "document: This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities."}
{"completion": "facebook/bart-large-mnli", "text": "query: programming, design, or writing. Analyze their statement for potential insights."}
{"completion": "videomae-small-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "videomae-small-finetuned-kinetics", "text": "query: A client from an educational institution is building a smart video library and needs to classify their educational videos based on their content."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "document: BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "query: Write a script that uses the BigBird API to summarize scientific articles."}
{"completion": "git-large-r-textcaps", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-r-textcaps", "text": "query: My product needs to help tourists navigate art galleries. I want them to snap a picture and get contextual explanations of the artwork."}
{"completion": "glpn-nyu-finetuned-diode-221116-110652", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}
{"completion": "glpn-nyu-finetuned-diode-221116-110652", "text": "query: We have an image of a room and we want to calculate the depth of objects in it."}
{"completion": "bert-large-cased-whole-word-masking-finetuned-squad", "text": "document: BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset."}
{"completion": "bert-large-cased-whole-word-masking-finetuned-squad", "text": "query: I want to develop an AI that can answer questions from a given context."}
{"completion": "microsoft/trocr-large-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-printed", "text": "query: In a warehouse management system, you need to extract the text from product images and store it in the database."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "document: This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "query: When I am exchanging text with my colleague, I want to automatically detect my sentiment/emotion without having to analyze it consciously."}
{"completion": "microsoft/wavlm-large", "text": "document: WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}
{"completion": "microsoft/wavlm-large", "text": "query: Our company is developing a voice-controlled fitness app. We need to incorporate a model capable of understanding and differentiating spoken commands from users."}
{"completion": "lvwerra/distilbert-imdb", "text": "document: This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set."}
{"completion": "lvwerra/distilbert-imdb", "text": "query: I am a music teacher who is working on an automatic grading platform, and I want to gauge the sentiment from a student's feedback."}
{"completion": "speechbrain/sepformer-whamr", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation."}
{"completion": "speechbrain/sepformer-whamr", "text": "query: We want a solution to separate the voices of two speakers from a single audio file."}
{"completion": "glpn-nyu-finetuned-diode-230131-041708", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}
{"completion": "glpn-nyu-finetuned-diode-230131-041708", "text": "query: We are researching on self-driving cars and would like to estimate the distance of objects from the environments."}
{"completion": "distil-ast-audioset", "text": "document: Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset."}
{"completion": "distil-ast-audioset", "text": "query: I want to add sound effects to the video game I am developing, based on the type of audio events happening in the gameplay. Whip up a system of audio classification that can detect distinct types of audio events."}
{"completion": "ConvTasNet_Libri2Mix_sepclean_8k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset."}
{"completion": "ConvTasNet_Libri2Mix_sepclean_8k", "text": "query: I have a party at my place this evening, and we need to separate sources of music in our audio system. Can you suggest a way to do this?"}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "query: Our self-driving car project requires identifying the distance between objects in the scene. How can we estimate the depth of a scene using a pre-trained model?"}
{"completion": "facebook/textless_sm_en_fr", "text": "document: This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech."}
{"completion": "facebook/textless_sm_en_fr", "text": "query: A translator software is needed for people who travel between England and France to communicate fluently in both languages using spoken speech rather than text."}
{"completion": "lllyasviel/sd-controlnet-seg", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-seg", "text": "query: I am an environmental scientist. I need to extract information about different plant types from an image of a lush green forest."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "document: RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1]."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "query: We are a linguistic research institute, and we need to perform feature extraction on a corpus of Russian text for further analysis."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "query: Create a 5-second video clip of Spiderman surfing."}
{"completion": "MCG-NJU/videomae-base-short", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "MCG-NJU/videomae-base-short", "text": "query: To be used for video-based search, the company developed a service that needs to be automized for categorizing videos into classes."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "document: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "query: I have a photo, and I want to create variations of it for my art project."}
{"completion": "facebook/blenderbot-400M-distill", "text": "document: BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX."}
{"completion": "facebook/blenderbot-400M-distill", "text": "query: Our team needs to create an automated assistant that can handle general inquiries from clients."}
{"completion": "microsoft/codebert-base", "text": "document: Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective."}
{"completion": "microsoft/codebert-base", "text": "query: Our company is building a chatbot for developers that understands and answers coding questions. The chatbot should understand both natural language queries and code samples."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "query: Our company is building a robotic vacuum cleaner, and we need to estimate the depth of objects in the room in real-time."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "document: Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "query: I have the answer \"42\" and its context is from a passage where it is mentioned as the ultimate answer to life, the universe, and everything. Generate a suitable question for this answer."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: Our customer is a data journalist. Let's find a quick fact from the statistic data for an upcoming news report."}
{"completion": "videomae-small-finetuned-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-small-finetuned-ssv2", "text": "query: Create an automatic video content recognition system to detect and classify actions taking place in a video."}
{"completion": "facebook/bart-large-mnli", "text": "document: This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities."}
{"completion": "facebook/bart-large-mnli", "text": "query: A website needs an NLP powered solution for sorting out a list of available movies according to the mostly related genre."}
{"completion": "microsoft/deberta-v3-base", "text": "document: DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2."}
{"completion": "microsoft/deberta-v3-base", "text": "query: We have a new AI blog, and we want to use AI to generate text based on chosen keywords."}
{"completion": "gpt2", "text": "document: GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences."}
{"completion": "gpt2", "text": "query: \"In the dark and stormy night, the castle stood as a shadow on the hill.\" Make sure each continuation is limited to 30 words."}
{"completion": "sup-simcse-roberta-large", "text": "document: A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks."}
{"completion": "sup-simcse-roberta-large", "text": "query: As an e-commerce website owner, I would like to identify similar product descriptions to recommend them to the customers."}
{"completion": "distilbert-base-uncased-distilled-squad", "text": "document: DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark."}
{"completion": "distilbert-base-uncased-distilled-squad", "text": "query: Create a system that can answer questions based on a given context."}
{"completion": "facebook/dragon-plus-context-encoder", "text": "document: DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders."}
{"completion": "facebook/dragon-plus-context-encoder", "text": "query: We're curating an archive of historical events. Help us retrieve relevant information about Marie Curie."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: Our photography company needs an automated way to generate captions for a set of images that describe the content in the image."}
{"completion": "pachi107/autotrain-in-class-test-1780161764", "text": "document: A binary classification model for predicting CO2 emissions based on tabular data. Trained using AutoTrain with a model ID of 1780161764."}
{"completion": "pachi107/autotrain-in-class-test-1780161764", "text": "query: Our team is attempting to predict carbon emissions levels based on tabular data."}
{"completion": "blip-image-captioning-large", "text": "document: BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}
{"completion": "blip-image-captioning-large", "text": "query: Our marketing department requires a technology that automatically generates captions for user-uploaded images."}
{"completion": "zhayunduo/roberta-base-stocktwits-finetuned", "text": "document: This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'."}
{"completion": "zhayunduo/roberta-base-stocktwits-finetuned", "text": "query: The finance team is analyzing stock-related comments. Help us predict the sentiment of these comments."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "document: MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "query: I want to create a mobile application that allows users to identify different fruit species using a photo. Please help me classify the images."}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: Develop an AI that can engage in a multi-round conversation."}
{"completion": "Babelscape/wikineural-multilingual-ner", "text": "document: A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks."}
{"completion": "Babelscape/wikineural-multilingual-ner", "text": "query: In a multilingual customer support system, we want to extract information including name and location details from the customer query."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "document: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "query: Develop a tool that helps city planners to segment buildings in satellite images."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "document: This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "query: We have an educational startup. We want a conversational agent that can answer questions to the text provided."}
{"completion": "glpn-nyu", "text": "document: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-nyu", "text": "query: Identify the depth of objects in a given image for autonomous driving system that has been installed in public transport buses."}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "document: This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language."}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "query: A history teacher is having trouble finding an answer for when the Mongol Empire was founded. Please assist the teacher."}
{"completion": "glpn-nyu", "text": "document: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-nyu", "text": "query: We are a robotics company working with drones, and we need to estimate the depth of objects within the drone's field of view."}
{"completion": "mazkooleg/0-9up-hubert-base-ls960-ft", "text": "document: This model is a fine-tuned version of facebook/hubert-base-ls960 on the None dataset. It achieves an accuracy of 0.9973 on the evaluation set."}
{"completion": "mazkooleg/0-9up-hubert-base-ls960-ft", "text": "query: I'm developing a smartwatch application that can determine the spoken numbers from the user's voice input."}
{"completion": "flair/ner-german", "text": "document: This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-german", "text": "query: Can you tell me the organizations in the German sentence \"Die Volkswagen AG hat ihren Sitz in Wolfsburg.\""}
{"completion": "sshleifer/distilbart-cnn-12-6", "text": "document: DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library."}
{"completion": "sshleifer/distilbart-cnn-12-6", "text": "query: I am a researcher studying on an article about Neural Networks. The article is too long, so I need to generate a summary for a better understanding."}
{"completion": "andite/anything-v4.0", "text": "document: Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model."}
{"completion": "andite/anything-v4.0", "text": "query: Build an image generation tool that can create anime-style images based on text prompts."}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Russian single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "query: The customer is interested in creating a text-to-speech system that speaks the Russian language. We need a transformer that can convert text to speech in Russian."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: Need a musician, who can convert a text to speech sample for me."}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: Our client is looking to build a conversational bot system. Implement the conversational model in their system."}
{"completion": "Zixtrauce/BaekBot", "text": "document: BaekBot is a conversational model based on the GPT-2 architecture for text generation. It can be used for generating human-like responses in a chat-like environment."}
{"completion": "Zixtrauce/BaekBot", "text": "query: Can you help me create a conversational bot that could interact with people, and also provide them with general information?"}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "document: Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "query: Build a system to improve customer service by analyzing user sentiments from their tweets."}
{"completion": "impira/layoutlm-document-qa", "text": "document: A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}
{"completion": "impira/layoutlm-document-qa", "text": "query: I'm an accountant who needs to extract information from invoices. Create a system that can identify the invoice total, date, and vendor from invoice images."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: I work for a car dealer and our inventory is in a spreadsheet. I want to find out the total number of SUVs in the inventory."}
{"completion": "stabilityai/stable-diffusion-2-inpainting", "text": "document: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts."}
{"completion": "stabilityai/stable-diffusion-2-inpainting", "text": "query: Kindly provide a sample code to get a picture of a panda wearing a red hat using this text-to-image model."}
{"completion": "microsoft/trocr-large-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-printed", "text": "query: We have a collection of old newspapers that we need to digitize, and we want to find a solution for extracting their textual content."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "query: The company is building a recommendation system for users based on their preferences in the movies. Analyze the video to find the movie genre."}
{"completion": "blip2-opt-6.7b", "text": "document: BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-opt-6.7b", "text": "query: Assist me in searching images related to \"wonders of the world\" and describe them."}
{"completion": "ocariz/universe_1400", "text": "document: This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs."}
{"completion": "ocariz/universe_1400", "text": "query: Our VR company is working on creating a space-themed game. We need to generate images of the universe as backgrounds for different scenes."}
{"completion": "lysandre/tiny-vit-random", "text": "document: A tiny-vit-random model for image classification using Hugging Face Transformers."}
{"completion": "lysandre/tiny-vit-random", "text": "query: I am building a mobile app for my startup, and I need a model that can perform image classification using low-resource architecture."}
{"completion": "facebook/convnext-base-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification."}
{"completion": "facebook/convnext-base-224", "text": "query: We need to classify images of cats of various breeds to create a search option based on breed."}
{"completion": "bert-base-cased", "text": "document: BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task."}
{"completion": "bert-base-cased", "text": "query: Our team is building a content creation platform. We want to fix sentences with missing words."}
{"completion": "dreamlike-art/dreamlike-diffusion-1.0", "text": "document: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art."}
{"completion": "dreamlike-art/dreamlike-diffusion-1.0", "text": "query: The company needs to create images to illustrate a blog post about an alien invasion. Please develop an image illustrating an alien spaceship attacking a city at night."}
{"completion": "Awais/Audio_Source_Separation", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset."}
{"completion": "Awais/Audio_Source_Separation", "text": "query: The musician in our team wants to separate vocals from instrumental sounds. Can you use audio source separation to help him?"}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "query: As a movie review website, the team is working on classifying movie reviews into positive or negative categories automatically."}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "document: A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model."}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "query: As an ornithologist, I want to classify bird species based on images I have taken with my camera. I need a model that can provide a prediction even if the species has never been seen before."}
{"completion": "andite/anything-v4.0", "text": "document: Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model."}
{"completion": "andite/anything-v4.0", "text": "query: We require a high-quality anime-style image for our upcoming marketing campaign. The image should depict \"a magical girl with white hair and a blue dress\"."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "query: We are an urban planning company, and we need to identify different regions in satellite images for better planning purposes."}
{"completion": "luhua/chinese_pretrain_mrc_roberta_wwm_ext_large", "text": "document: A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition."}
{"completion": "luhua/chinese_pretrain_mrc_roberta_wwm_ext_large", "text": "query: I am a car salesman in China. I need to find how many km per liter the hybrid model of the car does."}
{"completion": "pygmalion-1.3b", "text": "document: Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-1.3b", "text": "query: I am a writer who wants to create a creative conversation. Now, I am preparing a conversation between me and a character named Alex, a software developer."}
{"completion": "45473113800", "text": "document: A tabular regression model trained with AutoTrain for predicting carbon emissions."}
{"completion": "45473113800", "text": "query: As a car manufacturing company, we want to reduce the carbon emissions of our vehicles. Analyze the given dataset to predict carbon emissions and identify the factors affecting them."}
{"completion": "dpt-large-redesign", "text": "document: A depth estimation model based on the DPT architecture."}
{"completion": "dpt-large-redesign", "text": "query: We are creating an obstacle avoidance system for autonomous vehicles. We need to estimate the depth of objects in an image to accomplish this."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "document: This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "query: We have developed a podcast platform, and we want to verify the speaker's identity in the uploaded audio files."}
{"completion": "microsoft/DialoGPT-small", "text": "document: DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread."}
{"completion": "microsoft/DialoGPT-small", "text": "query: An AI assistant is needed for taking care of customers' concerns and questions. What can be done to make this happen?"}
{"completion": "bert-base-cased", "text": "document: BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task."}
{"completion": "bert-base-cased", "text": "query: We want to build a smart text editor, and the model will suggest the appropriate word when there is a masked word to solve."}
{"completion": "nvidia/segformer-b0-finetuned-cityscapes-1024-1024", "text": "document: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b0-finetuned-cityscapes-1024-1024", "text": "query: Describe the process of building a system to perform real-time semantic segmentation of images using the SegFormer model."}
{"completion": "bigscience/bloom-560m", "text": "document: BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads."}
{"completion": "bigscience/bloom-560m", "text": "query: I am an art museum curator, collect a summary of the imagery and meaning behind one of the most famous 18th-century paintings to give a brief introduction to the audience."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "document: This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "query: politics, health, entertainment, sports, or fake news."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "document: A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "query: I am having trouble handling customer support emails in German. I need help in identifying the topic of the email, whether it's related to computers, phones, tablets, urgent, or not urgent."}
{"completion": "edbeeching/decision-transformer-gym-hopper-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment"}
{"completion": "edbeeching/decision-transformer-gym-hopper-expert", "text": "query: We are running a tournament for the popular game Hopper. Our goal is to create an AI player with the records of the top players."}
{"completion": "facebook/textless_sm_en_fr", "text": "document: This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech."}
{"completion": "facebook/textless_sm_en_fr", "text": "query: Convert the English speech of this recorded meeting to French so that our French speakers can follow it."}
{"completion": "Linaqruf/anything-v3.0", "text": "document: A text-to-image model that generates images from text descriptions."}
{"completion": "Linaqruf/anything-v3.0", "text": "query: Can you generate an image of a green car based on its description?"}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "document: A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model."}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "query: Create a function that processes an image and outputs the category of the object in the image."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "query: We want to classify images of animals with zero-shot learning."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "document: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "query: Develop an AI chatbot that can generate a relevant anime-style image based on a given textual description."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: Create a Python function to provide a list of descriptions for images in a folder."}
{"completion": "clipseg-rd64-refined", "text": "document: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation."}
{"completion": "clipseg-rd64-refined", "text": "query: I run a painting shop, and I want to automatically segment the foreground and background of different images before I start painting on them. What do you suggest?"}
{"completion": "opus-mt-tc-big-en-pt", "text": "document: Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world."}
{"completion": "opus-mt-tc-big-en-pt", "text": "query: Create a system that translates English text into Portuguese for a language learning app."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "document: This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "query: I am a content creator for websites, and I need to identify the language of the given texts to create subtitles for multilingual content."}
{"completion": "OFA-Sys/chinese-clip-vit-large-patch14-336px", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-large-patch14-336px", "text": "query: As a media company targeting the eastern markets, we need help identifying the content of Chinese texts and images."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "document: A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "query: The local newspaper is looking for a way to automatically create short summaries of lengthy news articles."}
{"completion": "dreamlike-art/dreamlike-photoreal-2.0", "text": "document: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts."}
{"completion": "dreamlike-art/dreamlike-photoreal-2.0", "text": "query: Asynchronously generate a realistic image of a tropical beach with palm trees."}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: Implement a chatbot to answer customers' questions in the customer service section of an eCommerce store."}
{"completion": "glpn-kitti", "text": "document: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-kitti", "text": "query: Our landscaping firm needs a monocular depth estimation tool to analyze pictures of gardens from clients."}
{"completion": "shibing624/text2vec-base-chinese", "text": "document: This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search."}
{"completion": "shibing624/text2vec-base-chinese", "text": "query: Collect abstracts of research papers in Chinese language and find the pairwise similarity scores between all the collected abstracts."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "document: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "query: We found an article about our new home security product. Our team would like to know which objects are present in that article photo and need detailed segmentation."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "document: Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It's smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "query: I would like to analyze the emotions contained in a piece of text."}
{"completion": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best", "text": "document: A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese."}
{"completion": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best", "text": "query: We are creating an AI assistant in Chinese language that helps the user navigate in the city which needs to communicate via text-to-speech."}
{"completion": "sentence-transformers/nli-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/nli-mpnet-base-v2", "text": "query: Our client is a Russian professional publisher and they want to analyze sentences for similarity with the articles they publish."}
{"completion": "julien-c/skops-digits", "text": "document: A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}
{"completion": "julien-c/skops-digits", "text": "query: A healthcare company wants to classify its patients. They have a dataset of patient features, and they want to train a model to predict which patients are likely to have certain conditions."}
{"completion": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K", "text": "document: A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others."}
{"completion": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K", "text": "query: We developed an image uploading feature for our app. The app allows users to pick a category for their image, but it's prone to incorrect categorization. Implement a model to detect and classify the uploaded images into correct categories."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "query: In order to build a better navigation system, the company needs to estimate the depth of objects in the environment."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "document: This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "query: I want to generate a series of ice cream designs with colorful patterns. What kind of image generation model should I use?"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: I want to create a search engine for a document repository, ranking the most relevant passages based on a user's query."}
{"completion": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "text": "document: A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k."}
{"completion": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "text": "query: A mobile app company is working on improving the search functionality of their app. The search should be able to recognize the category of a particular image provided by the user."}
{"completion": "hustvl/yolos-small", "text": "document: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN)."}
{"completion": "hustvl/yolos-small", "text": "query: We have a large collection of images that need to be sorted by the objects present in them. We want to identify objects in them easily."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "document: MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "query: We are working on an e-commerce website project which needs to identify products by analyzing their images."}
{"completion": "imodels/figs-compas-recidivism", "text": "document: A tabular classification model for predicting recidivism using the COMPAS dataset. The model is an imodels.FIGSClassifier trained with Scikit-learn and can be used with the Hugging Face Inference API."}
{"completion": "imodels/figs-compas-recidivism", "text": "query: I am a criminal attorney and I want to use an AI system to predict whether my client has a high or low risk of recidivism."}
{"completion": "abhishek/autotrain-iris-xgboost", "text": "document: A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%."}
{"completion": "abhishek/autotrain-iris-xgboost", "text": "query: I want to predict which species of iris a flower belongs to, based on values measured on the petals and sepals."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "query: A team of researchers is developing a robotic navigation system. Obtain depth information of indoor environments using a computer vision model."}
{"completion": "cl-tohoku/bert-base-japanese-whole-word-masking", "text": "document: This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective."}
{"completion": "cl-tohoku/bert-base-japanese-whole-word-masking", "text": "query: We are constructing a language learning application for users learning Japanese. Generate filled Japanese sentences for input phrases containing masked tokens."}
{"completion": "DialoGPT-large", "text": "document: DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread."}
{"completion": "DialoGPT-large", "text": "query: I need my chat assistant to suggest healthy dinner recipes when asked by users."}
{"completion": "opus-mt-de-es", "text": "document: A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization."}
{"completion": "opus-mt-de-es", "text": "query: I want to translate a short story I wrote in German to Spanish so that Spanish-speaking friends can read it."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "query: I'd like to create an image of a toy robot with a friendly appearance based on a rough scribble of the toy design idea. "}
{"completion": "flair/ner-english-large", "text": "document: This is the large 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on document-level XLM-R embeddings and FLERT."}
{"completion": "flair/ner-english-large", "text": "query: We want to analyze user-generated text for our dating app to find any important nouns like people, places, and organizations."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "document: Helsinki-NLP/opus-mt-fi-en is a machine translation model for translating Finnish text to English text. It is trained on the OPUS dataset and can be used with the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "query: I am writing a blog related to Finnish culture and need to translate it to English. What are the steps?"}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "document: This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions."}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "query: A relevant answer to the question below must be extracted from the given passage."}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "document: MetricGAN-trained model for Enhancement"}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "query: For some podcasts, there is a lot of background noise. The company is now strenghtening the speech audio."}
{"completion": "google/flan-t5-small", "text": "document: FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application."}
{"completion": "google/flan-t5-small", "text": "query: We are building an AI chatbot for our website. We need to integrate a language model to generate responses."}
{"completion": "opus-mt-en-ru", "text": "document: Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers."}
{"completion": "opus-mt-en-ru", "text": "query: I need to translate an employee manual written in English to Russian for our company's branch in Russia."}
{"completion": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli", "text": "document: This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation."}
{"completion": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli", "text": "query: Extract transcription from an audio file with proper punctuation."}
{"completion": "danupurnomo/dummy-titanic", "text": "document: This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more."}
{"completion": "danupurnomo/dummy-titanic", "text": "query: I want to predict if a person will survive on a cruise ship depending on features such as passenger class, age, sex, fare, etc."}
{"completion": "glpn-nyu-finetuned-diode", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode", "text": "query: We are creating a safe route for a visually impaired person. Help us to estimate the depth of an indoor scene from an RGB image."}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819"}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "query: We need a question answering system for multimodal documents."}
{"completion": "45473113800", "text": "document: A tabular regression model trained with AutoTrain for predicting carbon emissions."}
{"completion": "45473113800", "text": "query: Develop a CO2 emissions prediction system for our new eco-friendly cars using our dataset."}
{"completion": "facebook/detr-resnet-101", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-101", "text": "query: The surveillance team is eager to monitor any suspicious activity in public places, detect objects in the surveillance feed and notify the team."}
{"completion": "videomae-large", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "videomae-large", "text": "query: We develop robots and smart cameras for security at shopping malls. Can you classify video samples for actions like sitting, walking, running, and fighting?"}
{"completion": "roberta-base-openai-detector", "text": "document: RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model."}
{"completion": "roberta-base-openai-detector", "text": "query: After creating AI-generated content using GPT-2, a tool is required to detect the content generated by GPT-2 for quality assurance purposes."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "document: A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "query: I am a smartphone app developer focusing on accessibility. We need to caption images that visually impaired people can easily understand."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: I am developing a travel app, and I want to build a feature to answer user's questions about countries' population and GDP."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "query: A university researcher is trying to measure the depth of an object from an image."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "document: Helsinki-NLP/opus-mt-fi-en is a machine translation model for translating Finnish text to English text. It is trained on the OPUS dataset and can be used with the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-fi-en", "text": "query: Translate the Finnish sentence \"Tervetuloa Helsinkiin!\" into English."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "document: TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "query: I have a table with the revenue details of a company. The table contains information about different divisions, and I need to know the total revenue made by the company."}
{"completion": "microsoft/resnet-18", "text": "document: ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision."}
{"completion": "microsoft/resnet-18", "text": "query: We want to build a plant species identification app, so we need to classify plant images."}
{"completion": "cointegrated/rut5-base-absum", "text": "document: This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets."}
{"completion": "cointegrated/rut5-base-absum", "text": "query: Our customer is an online news website, and we need to provide summaries for their Russian articles."}
{"completion": "facebook/dpr-question_encoder-single-nq-base", "text": "document: Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019)."}
{"completion": "facebook/dpr-question_encoder-single-nq-base", "text": "query: Develop a model that can classify whether a given question is about science, technology, arts, or sports."}
{"completion": "cointegrated/rut5-base-absum", "text": "document: This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets."}
{"completion": "cointegrated/rut5-base-absum", "text": "query: Help us translate and shorten the russian text provided by the CEO of our company to use in briefing their employees."}
{"completion": "philschmid/bart-large-cnn-samsum", "text": "document: philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations."}
{"completion": "philschmid/bart-large-cnn-samsum", "text": "query: Our company is working on an app that summarizes conversations between users. Please provide a solution on how to do this."}
{"completion": "flair/upos-english", "text": "document: This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/upos-english", "text": "query: My 5-year-old daughter is learning grammar; create a tool that enlightens her about several grammar aspects."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "document: A speech-to-speech translation model that can be loaded on the Inference API on-demand."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "query: Translate this English podcast's audio file into a German text transcript."}
{"completion": "keremberke/yolov5m-license-plate", "text": "document: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy."}
{"completion": "keremberke/yolov5m-license-plate", "text": "query: I'm developing a parking system application which needs to automatically detect the license plate numbers of vehicles entering the parking lot. Find a suitable model for this task."}
{"completion": "keremberke/yolov8m-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes."}
{"completion": "keremberke/yolov8m-pothole-segmentation", "text": "query: We are a civil engineering company specialized in road repairs. We need to detect and segment potholes in road images."}
{"completion": "harithapliyal/autotrain-tatanic-survival-51030121311", "text": "document: A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class."}
{"completion": "harithapliyal/autotrain-tatanic-survival-51030121311", "text": "query: I am a researcher, I would like to tell if a certain passenger survived the Titanic ship after providing some features like age, gender and passenger class."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "document: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-base-coco-panoptic", "text": "query: Our client needs a tool for processing satellite images of rivers and roads and generating a segmented map to understand the geography of a given area."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "document: Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "query: We're building an app that matches synonyms. Find out how well our sentences match each other."}
{"completion": "glpn-nyu-finetuned-diode-221215-095508", "text": "document: A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture."}
{"completion": "glpn-nyu-finetuned-diode-221215-095508", "text": "query: Advise me how to build a program to predict the depth map of an image for indoor robotics."}
{"completion": "pyannote/segmentation", "text": "document: A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework."}
{"completion": "pyannote/segmentation", "text": "query: We are hosting a podcast application, and we need to detect the voice activity, overlaps, and resegmentation in the submitted audio file."}
{"completion": "mrm8488/bert2bert_shared-spanish-finetuned-summarization", "text": "document: Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization"}
{"completion": "mrm8488/bert2bert_shared-spanish-finetuned-summarization", "text": "query: Summarize a Spanish news article to ease the process of content consumption for users."}
{"completion": "opus-mt-en-ROMANCE", "text": "document: A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID)."}
{"completion": "opus-mt-en-ROMANCE", "text": "query: Create a text translation system that can translate English text to Spanish. The considered input texts are related to booking a hotel."}
{"completion": "google/owlvit-base-patch32", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries."}
{"completion": "google/owlvit-base-patch32", "text": "query: Analyze an image and tell me the position of a cat and a dog in the image."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "query: How can we leverage this model to answer questions about tabular datasets? We need to analyze financial data and answer questions."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "document: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "query: We are a company specializing in promotional graphics. Our client would like an upscaled image of their server room for use in marketing materials."}
{"completion": "openmmlab/upernet-convnext-small", "text": "document: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s."}
{"completion": "openmmlab/upernet-convnext-small", "text": "query: We are a street maintenance crew and need to analyze the street images to segment sidewalks, trees, and vehicles."}
{"completion": "chavinlo/TempoFunk", "text": "document: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}
{"completion": "chavinlo/TempoFunk", "text": "query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text."}
{"completion": "microsoft/DialoGPT-small", "text": "document: DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread."}
{"completion": "microsoft/DialoGPT-small", "text": "query: I want to build a conversational AI agent to integrate with my application. This agent will generate responses based on user input in a conversation."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "document: Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "query: We are building a virtual assistant to help visitors in a museum. The assistant answers common questions regarding the museum history, exhibits information and general guidelines."}
{"completion": "MFawad/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "MFawad/sd-class-butterflies-32", "text": "query: We are organizing a butterfly-themed event, and we need to generate some butterfly images for promotional material."}
{"completion": "openai/clip-vit-base-patch16", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-base-patch16", "text": "query: We are an advertising agency and our clients want to launch their website. We should provide them with a clear set of images of certain products."}
{"completion": "Intel/dpt-large", "text": "document: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation."}
{"completion": "Intel/dpt-large", "text": "query: Our Real Estate client wants a computer vision model to predict the depth of rooms in indoor images."}
{"completion": "opus-mt-sv-en", "text": "document: A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece."}
{"completion": "opus-mt-sv-en", "text": "query: \"Hej, hur m\u00e5r du? Tack f\u00f6r m\u00f6tet ig\u00e5r! Det var mycket givande och vi ser fram emot att arbeta tillsammans med er i framtiden.\""}
{"completion": "bart-large-cnn-samsum-ChatGPT_v3", "text": "document: This model is a fine-tuned version of philschmid/bart-large-cnn-samsum on an unknown dataset."}
{"completion": "bart-large-cnn-samsum-ChatGPT_v3", "text": "query: Our client wants to create a summary for an article about a new tech product. We need to summarize the article for them."}
{"completion": "bert-base-multilingual-cased", "text": "document: BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-multilingual-cased", "text": "query: I am a journalist. I need to complete a French sentence based on my draft with a missing word."}
{"completion": "git-large-textvqa", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-textvqa", "text": "query: A visually impaired user has asked for assistance in understanding an image and answering a question about it."}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "document: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50."}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "query: Identify and give me the location of the bordered and borderless tables in document images."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "document: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "query: Design a banner for our new store launch event. The banner should have a picture of a grand opening with balloons, confetti, and a red ribbon."}
{"completion": "mio/Artoria", "text": "document: This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output."}
{"completion": "mio/Artoria", "text": "query: Develop a feature for an educational app that can read a given story. The story text needs to be converted into an audio file."}
{"completion": "Helsinki-NLP/opus-mt-en-es", "text": "document: This model is a translation model from English to Spanish using the Hugging Face Transformers library. It is based on the Marian framework and trained on the OPUS dataset. The model achieves a BLEU score of 54.9 on the Tatoeba test set."}
{"completion": "Helsinki-NLP/opus-mt-en-es", "text": "query: I need a quick translation of a news article from English to Spanish for my foreign business partners."}
{"completion": "data2vec-audio-base-960h", "text": "document: Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision."}
{"completion": "data2vec-audio-base-960h", "text": "query: We want to convert a political interview's audio file to a text transcript for our client's newsletter."}
{"completion": "mgp-str", "text": "document: MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images."}
{"completion": "mgp-str", "text": "query: We want a program that can analyze a picture of text and give us the written version of the text."}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "query: I have a collection of video frames and I need to classify their content into predefined categories."}
{"completion": "superb/hubert-large-superb-sid", "text": "document: Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification."}
{"completion": "superb/hubert-large-superb-sid", "text": "query: A new project demands us to identify speakers from an audio clip. We need to figure out an identifier for each speaker."}
{"completion": "deepset/bert-medium-squad2-distilled", "text": "document: This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library."}
{"completion": "deepset/bert-medium-squad2-distilled", "text": "query: I need to know the answer to a question I have in connection witha paragraph."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "query: We are building an app that converts spoken Russian language to text. Configure the speech recognition model according to the provided API."}
{"completion": "dreamlike-art/dreamlike-photoreal-2.0", "text": "document: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts."}
{"completion": "dreamlike-art/dreamlike-photoreal-2.0", "text": "query: Help me create a computer generated graphic of a superhero character for my upcoming sci-fi story."}
{"completion": "julien-c/pokemon-predict-hp", "text": "document: A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon."}
{"completion": "julien-c/pokemon-predict-hp", "text": "query: I am a data scientist working on the Pokemon dataset, and I need to predict the HP of a new Pokemon based on its input features."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "document: Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "query: We are building an AI customer service tool that can help companies analyze customers' sentiments from their tweets."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: Summarize the provided article in a clear and concise manner for a professional presentation."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "document: Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI)."}
{"completion": "emilyalsentzer/Bio_ClinicalBERT", "text": "query: We would like the assistant to predict the missing word in a clinical sentence using the Bio_ClinicalBERT model."}
{"completion": "sentence-transformers/paraphrase-mpnet-base-v2", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-mpnet-base-v2", "text": "query: A company specializing in SEO would like to check the similarity between a set of blog titles to avoid content duplication issues."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "document: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "query: Help me generate a marketing poster where a smiling businessman is holding a laptop in front of a row of wind turbines."}
{"completion": "facebook/wav2vec2-base-960h", "text": "document: Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files."}
{"completion": "facebook/wav2vec2-base-960h", "text": "query: I've recorded a series of product reviews in audio format, and I need to create text transcripts for them to be published on our website."}
{"completion": "d4data/Indian-voice-cloning", "text": "document: A model for detecting voice activity in Indian languages."}
{"completion": "d4data/Indian-voice-cloning", "text": "query: I am developing an app to automatically transcribe voice recordings in Indian languages. What can I use to identify which parts of the audio contain speech?"}
{"completion": "git-large-textcaps", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-textcaps", "text": "query: Develop a solution to caption images given their features."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "query: Our news portal is in need of an application to find the similarity between articles and display the top 5 related articles. How can we do that?"}
{"completion": "neulab/omnitab-large", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data."}
{"completion": "neulab/omnitab-large", "text": "query: Our company is preparing a report with multiple tables in it. Provide me with a simple tool that allows me to ask questions about the data."}
{"completion": "flair/ner-german", "text": "document: This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-german", "text": "query: Create a German language named entity recognition tool that can identify person names, location names, organization names, and other names."}
{"completion": "sentence-transformers/all-roberta-large-v1", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-roberta-large-v1", "text": "query: We have a list of customer reviews, and we want to find similar reviews by grouping them based on their semantic similarity."}
{"completion": "sepformer-wsj02mix", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset."}
{"completion": "sepformer-wsj02mix", "text": "query: We are building a music application, and I want to separate vocals from instruments."}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "document: Binary Classification model for Carbon Emissions prediction"}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "query: We need to develop a tool that can identify which buildings are more likely to have high carbon emissions based on various factors."}
{"completion": "bert-large-uncased", "text": "document: BERT large model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters. The model is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-large-uncased", "text": "query: \"I'm a fan of playing [MASK] games on my PC.\""}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "document: Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment", "text": "query: Our company is an eCommerce startup. Customers comment on products they buy. We need to find products' positive and negative points from customer reviews."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb", "text": "query: Sort surveillance footage of humans fighting from the recorded videos to detect any chances of violence."}
{"completion": "facebook/bart-large-cnn", "text": "document: BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs."}
{"completion": "facebook/bart-large-cnn", "text": "query: Our news app needs to provide short summaries of articles for users with limited time. Develop a method to generate summaries for readability."}
{"completion": "blip2-flan-t5-xxl", "text": "document: BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xxl", "text": "query: I need a conversational AI to create a chatbot that helps users get nutrition facts from an image of food."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: In the new CRM we are developing, we need sentence embeddings to be generated for a list of customer feedback on our new products. We would like to use those in a clustering algorithm to identify major areas of feedback."}
{"completion": "microsoft/unixcoder-base", "text": "document: UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks."}
{"completion": "microsoft/unixcoder-base", "text": "query: I want to create an AI-powered code analyzer to help me identify code pieces that need refactoring. I need a model to extract and process important features."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "document: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "query: A video game company requires some unique anime-style characters for their new action adventure game. They have provided a description of a character they want."}
{"completion": "videomae-base-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-base-ssv2", "text": "query: Our company is developing an exercise app, and we need to identify the types of exercises performed in user-submitted videos."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "query: Let's say you have a video and you need to classify the main action in the video. How would you implement it?"}
{"completion": "facebook/opt-125m", "text": "document: OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation."}
{"completion": "facebook/opt-125m", "text": "query: Develop a smart AI chatbot to instantaneously generate human-like informal text given a conversational input on any topic."}
{"completion": "decapoda-research/llama-7b-hf", "text": "document: LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English."}
{"completion": "decapoda-research/llama-7b-hf", "text": "query: We are implementing a virtual assistant to provide recommendations for nightlife events in San Francisco. Generate a short paragraph describing a fun event that suits the context."}
{"completion": "pygmalion-6b", "text": "document: Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."}
{"completion": "pygmalion-6b", "text": "query: Users want to engage in a conversation with a fictional character based on their persona. This conversation will be used as part of a script for an animation series."}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "document: Baseline Model trained on tips5wx_sbh5 to apply regression on tip"}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "query: We have a data set containing the price of gold prices. We would like to predict the price of gold given features such as the price of silver and the economic index."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment-latest", "text": "document: This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English."}
{"completion": "cardiffnlp/twitter-roberta-base-sentiment-latest", "text": "query: As a marketing company, we want to monitor customer feedback from social media. Find their sentiments and emotions about our products."}
{"completion": "VC1_BASE_NAME", "text": "document: The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation."}
{"completion": "VC1_BASE_NAME", "text": "query: Our company is developing an autonomous robot to navigate the office and pick up objects. Implement a model for visual understanding."}
{"completion": "facebook/dino-vits8", "text": "document: Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository."}
{"completion": "facebook/dino-vits8", "text": "query: Come up with a solution for detecting objects in images using features present in the image."}
{"completion": "904029577", "text": "document: This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams."}
{"completion": "904029577", "text": "query: Determine the entities present in a given text string."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes."}
{"completion": "keremberke/yolov8n-pothole-segmentation", "text": "query: Fill the potholes on the roads in our city with proper steps."}
{"completion": "decapoda-research/llama-13b-hf", "text": "document: LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA."}
{"completion": "decapoda-research/llama-13b-hf", "text": "query: I want to generate a compelling story about an adventurous rabbit having only \"Once upon a time in a large forest...\" as a prompt."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: I want a software to determine if an individual earns above or below $50,000 per year. Use the column information in dataset 'scikit-learn/adult-census-income'."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "document: TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "query: I plan to do table question answering in my system. Help me apply the google/tapas-medium-finetuned-sqa model."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "document: An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "query: Develop a service for a photo-sharing app that generates a caption for a user's uploaded picture."}
{"completion": "stabilityai/stable-diffusion-2", "text": "document: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes."}
{"completion": "stabilityai/stable-diffusion-2", "text": "query: Our customer is a travel agency, and they need a creative image based on \"vacation on a beautiful island\" to use in their advertisement."}
{"completion": "ocariz/universe_1400", "text": "document: This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs."}
{"completion": "ocariz/universe_1400", "text": "query: We are conducting a research project for an astronomy magazine's cover. We could use some images of the universe."}
{"completion": "mio/amadeus", "text": "document: This model was trained by mio using amadeus recipe in espnet."}
{"completion": "mio/amadeus", "text": "query: I need to create a system that generates spoken output for given text."}
{"completion": "facebook/maskformer-swin-base-ade", "text": "document: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation."}
{"completion": "facebook/maskformer-swin-base-ade", "text": "query: Our project's goal is to develop a system that can analyze a satellite image, identify different land cover types and classify them accordingly."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "document: This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "query: Write me a code to answer a question based on the given context."}
{"completion": "FSMN-VAD", "text": "document: FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library."}
{"completion": "FSMN-VAD", "text": "query: I need to create an application to track voice activity during video conference calls to record only when someone is talking."}
{"completion": "Alexei1/imdb", "text": "document: A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487."}
{"completion": "Alexei1/imdb", "text": "query: We are an online library. We need an AI model to categorize book/movie reviews."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "query: Can you show me how to run a BreakoutNoFrameskip-v4 game using the PPO algorithm?"}
{"completion": "sentence-transformers/distilbert-base-nli-stsb-mean-tokens", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/distilbert-base-nli-stsb-mean-tokens", "text": "query: Compare the similarity of the plots of three different movies. Identify the most similar pair of movies."}
{"completion": "google/pix2struct-textcaps-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks."}
{"completion": "google/pix2struct-textcaps-base", "text": "query: Convert the provided image to a descriptive text."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "document: A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "query: We are a company working with international clients. To deal with language barriers, what can we do to translate Dutch messages to English?"}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "document: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels."}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "query: Our client wants to offer a new species identification service for pets. Help them build it from scratch."}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "document: TAPAS mini model fine-tuned on Sequential Question Answering (SQA)"}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "query: We want to develop a tool to help people find answers from sales data in tables, like job titles, products and prices. Please use a suitable model to achieve this."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "document: A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "query: How can the information on a wikipedia page be translated from English to Italian?"}
{"completion": "CompVis/ldm-celebahq-256", "text": "document: Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs."}
{"completion": "CompVis/ldm-celebahq-256", "text": "query: I am a designer working on a high-quality portrait of a fictional celebrity for a magazine cover. How do I generate the image?"}
{"completion": "opus-mt-tc-big-en-pt", "text": "document: Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world."}
{"completion": "opus-mt-tc-big-en-pt", "text": "query: Our world-wide company wants to translate its English news to Portuguese so that it reaches more people."}
{"completion": "blip2-opt-6.7b", "text": "document: BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-opt-6.7b", "text": "query: A large coffee company is looking for ways to improve their social media presence. We are investigating if using generated captions on their images can assist in increasing engagement."}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "document: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50."}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "query: Our company specializes in document analysis, and I need a model to detect and extract tables from document images."}
{"completion": "sberbank-ai/sbert_large_mt_nlu_ru", "text": "document: BERT large model multitask (cased) for Sentence Embeddings in Russian language."}
{"completion": "sberbank-ai/sbert_large_mt_nlu_ru", "text": "query: Develop a program to convert sentences in Russian language into their respective sentence embeddings, using BERT large model."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "document: Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "query: We are trying to develop an application to help users learn new languages and accents with synthetic speech. Use this API to generate speech audio of an English text 'Hello World' in Taiwanese Hokkien accent."}
{"completion": "videomae-large", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "videomae-large", "text": "query: We are developing a new system for video surveillance that can detect and classify suspicious activities. Implement a model that can help us in identifying such incidents."}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "document: MetricGAN-trained model for Enhancement"}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "query: I have a noisy voice recording and want to enhance its quality using an audio-to-audio API. How do I do that?"}
{"completion": "FSMN-VAD", "text": "document: FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library."}
{"completion": "FSMN-VAD", "text": "query: We are organizing an online conference call. We'd like to get voice activity detection during the live call."}
{"completion": "lakahaga/novel_reading_tts", "text": "document: This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks."}
{"completion": "lakahaga/novel_reading_tts", "text": "query: Our team needs a solution to convert Korean text into speech for an e-learning application, which reads out educational content."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "document: A tiny random model for Visual Question Answering using the VILT framework."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "query: We need to develop a program that can answer questions about a given image."}
{"completion": "pygmalion-6b", "text": "document: Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue."}
{"completion": "pygmalion-6b", "text": "query: We are writing a script for a movie. We want to create a dialogue communication between different characters. Could you please generate a resonse for a character based on the following dialogue history?"}
{"completion": "MFawad/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "MFawad/sd-class-butterflies-32", "text": "query: We are running a butterfly-themed event and need to generate butterfly images for our marketing campaign."}
{"completion": "facebook/dino-vitb16", "text": "document: Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}
{"completion": "facebook/dino-vitb16", "text": "query: I need a program that can analyze pictures of art and classify them into different artistic styles."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "document: This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "query: We have an app that needs to record its users' voices to verify their identity. We need an audio classifier to analyze the user's voice."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: Provide instructions to convert spoken language into written text using the wav2vec2-xls-r-300m-phoneme model."}
{"completion": "pyannote/overlapped-speech-detection", "text": "document: Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file."}
{"completion": "pyannote/overlapped-speech-detection", "text": "query: We are adding a feature in transcription service where it can detect overlapped speech from an audio file. Can you help us extract those timestamps?"}
{"completion": "microsoft/trocr-small-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM."}
{"completion": "microsoft/trocr-small-printed", "text": "query: My boss needs all the data that is printed on a batch of invoices. I have the image URL of these invoices. Implement an OCR model to read the printed text on these invoices."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "document: This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "query: Help me understand the functionality of the following code snippet by generating a helpful code description for it."}
{"completion": "sentiment_analysis_generic_dataset", "text": "document: This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification."}
{"completion": "sentiment_analysis_generic_dataset", "text": "query: My company needs to make decisions on recommending products to users based on their text reviews. We need to analyze the sentiment of those reviews."}
{"completion": "tts_transformer-ar-cv7", "text": "document: Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset."}
{"completion": "tts_transformer-ar-cv7", "text": "query: We are developing an interactive application for learning the Arabic language, and we want the app to read various text inputs aloud to users in Arabic."}
{"completion": "ppo-PongNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-PongNoFrameskip-v4", "text": "query: A gaming company is developing a new game and wants to implement an AI that plays Pong with no frame skips. Help them implement a pre-trained model."}
{"completion": "tejas23/autotrain-amx2-1702259725", "text": "document: Multi-class Classification Model for Carbon Emissions"}
{"completion": "tejas23/autotrain-amx2-1702259725", "text": "query: Help me implement a solution to predict the carbon emissions for different models of vehicles using a pre-trained model."}
{"completion": "albert-base-v2", "text": "document: ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English."}
{"completion": "albert-base-v2", "text": "query: We have a short story written in a park environment, please generate a line with a plausible subject related to the present context."}
{"completion": "graphormer-base-pcqm4mv2", "text": "document: The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling."}
{"completion": "graphormer-base-pcqm4mv2", "text": "query: To create a software for analyzing chemical molecules, we should predict their properties with a pre-trained model."}
{"completion": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6", "text": "document: The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks."}
{"completion": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6", "text": "query: We need to compute semantic similarity scores between pairs of sentences to classify related news articles effectively."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: We are a company that processes contracts for clients. Design a solution to find the total amount mentioned in the contract."}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: A customer needs help choosing a new phone, and wants a conversational AI model to provide recommendations."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "query: Create an AI-based recommendation system for our news aggregation platform, where we can provide recommendations by finding articles similar to the one that the user is reading."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "query: We need to classify sports in a video feed for a sports analytics company."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "document: Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It's smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer."}
{"completion": "bhadresh-savani/distilbert-base-uncased-emotion", "text": "query: We are building a social media monitoring platform for clients. We need to categorize posts into emotions."}
{"completion": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224", "text": "document: BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering."}
{"completion": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224", "text": "query: Develop a tool to classify a medical image using BiomedCLIP."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "document: A tiny TAPAS model for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "query: I need a solution to help me answer questions about the population and the GDP of different countries from a table."}
{"completion": "facebook/bart-large", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-large", "text": "query: Please assist me in creating natural language responses to user inputs."}
{"completion": "wav2vec2-base-superb-sv", "text": "document: This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "wav2vec2-base-superb-sv", "text": "query: We are launching a new security system and we want to ensure that the person speaking is correctly identified by the system."}
{"completion": "Babelscape/wikineural-multilingual-ner", "text": "document: A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks."}
{"completion": "Babelscape/wikineural-multilingual-ner", "text": "query: Build a model that would help us extract names and locations from user reviews."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "query: A children's publisher would like a story illustration that features butterflies. Generate an image presenting cute butterflies."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "document: This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details)."}
{"completion": "cardiffnlp/twitter-xlm-roberta-base-sentiment", "text": "query: Analyze the sentiment of a foreign language tweet."}
{"completion": "pyannote/voice-activity-detection", "text": "document: A pretrained voice activity detection pipeline that detects active speech in audio files."}
{"completion": "pyannote/voice-activity-detection", "text": "query: The marketing team needs assistance in analyzing customer feedback through phone calls. We need to identify only the parts of the audio where customers are speaking."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "query: We are building a drone that can recognize various objects in the environment. We would like to process the images captured by the drone's camera for this purpose."}
{"completion": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804", "text": "document: A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech."}
{"completion": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804", "text": "query: Create a software that speaks the weather forecast of Tokyo in Japanese."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: I have a dataset of people's information (age, workclass, education, race, etc.) and I need to predict if they will have income greater than or less than $50,000 per year."}
{"completion": "microsoft/git-large-vqav2", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}
{"completion": "microsoft/git-large-vqav2", "text": "query: We are an app development company working on an app to help blind people recognize their surroundings. We need a function to answer their questions about a specific image."}
{"completion": "facebook/detr-resnet-101", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-101", "text": "query: We are building an AI-based software to help farmers identify infected plants from their fields. Determine the objects in a given image."}
{"completion": "keremberke/yolov8m-nlf-head-detection", "text": "document: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets."}
{"completion": "keremberke/yolov8m-nlf-head-detection", "text": "query: I am building a security system for a football stadium. The system needs to automatically detect helmets in the frame from the CCTV camera footage."}
{"completion": "dslim/bert-large-NER", "text": "document: bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC)."}
{"completion": "dslim/bert-large-NER", "text": "query: I need to extract all personal names, locations, and organization names from given text."}
{"completion": "finiteautomata/beto-sentiment-analysis", "text": "document: Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels."}
{"completion": "finiteautomata/beto-sentiment-analysis", "text": "query: A media agency wants to have a sentiment analysis on their customer reviews. Automate their sentiment analysis process."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "document: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "query: Design an aerial autonomous vehicle that scans an area, identifies, and segments buildings from satellite images."}
{"completion": "facebook/bart-base", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-base", "text": "query: \"The quick brown fox jumps over the lazy dog\"."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-dot-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-dot-v1", "text": "query: An environmentalist group needs to find tweets most relevant to their work from a massive dataset. They need a model to quickly sort out the most relevant information from the pool to save time."}
{"completion": "optimum/t5-small", "text": "document: T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization."}
{"completion": "optimum/t5-small", "text": "query: \"The weather today is sunny and beautiful.\""}
{"completion": "facebook/maskformer-swin-base-ade", "text": "document: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation."}
{"completion": "facebook/maskformer-swin-base-ade", "text": "query: We have aerial images of a city and we want to analyze them by segmenting different areas."}
{"completion": "table-question-answering-tapas", "text": "document: TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts."}
{"completion": "table-question-answering-tapas", "text": "query: Our team is working on building an application in education to help students with answering questions based on tables provided in textbooks. We need to classify types of exercise."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: The customer wants to run analytics on historical transactions of their e-commerce store. They have a large dataset with several columns, including date, product name, price, quantity, and category. Help them to answer \"what is the total revenue generated on July 15, 2021?\""}
{"completion": "cross-encoder/nli-roberta-base", "text": "document: Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-roberta-base", "text": "query: Write a recommendation of which online news sources I should read, focusing on articles about autonomous vehicles and urban planning."}
{"completion": "microsoft/deberta-v2-xxlarge", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data."}
{"completion": "microsoft/deberta-v2-xxlarge", "text": "query: We need a fill mask model that can automatically fill the spaces in sentences."}
{"completion": "sentence-transformers/bert-base-nli-mean-tokens", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/bert-base-nli-mean-tokens", "text": "query: We are trying to build an application with semantic search which leverages a sentence-transformer model for computing sentence embeddings."}
{"completion": "zhayunduo/roberta-base-stocktwits-finetuned", "text": "document: This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'."}
{"completion": "zhayunduo/roberta-base-stocktwits-finetuned", "text": "query: Provide me with a solution to infer the sentiment of stock-related comments to make better investment decisions."}
{"completion": "kazusam/kt", "text": "document: An ESPnet2 TTS model trained by mio using amadeus recipe in espnet."}
{"completion": "kazusam/kt", "text": "query: \"Welcome to the grand opening of our new store! Enjoy unbeatable prices and amazing discounts.\""}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "document: A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "query: Translate a machine error message from English to Italian."}
{"completion": "google/pegasus-newsroom", "text": "document: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks."}
{"completion": "google/pegasus-newsroom", "text": "query: The marketing team needs a summarization model for blog posts. Please create an API call to make it possible."}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "document: MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!"}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "query: As a Russian speaker, I want to turn a long article into a brief summary."}
{"completion": "google/ddpm-bedroom-256", "text": "document: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN."}
{"completion": "google/ddpm-bedroom-256", "text": "query: We need to generate an image of a bedroom for a new advertisement campaign."}
{"completion": "superb/hubert-large-superb-er", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-large-superb-er", "text": "query: We aim to capture people's emotions while they are speaking with a customer service representative. Help us classify the customer's emotion."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: Create a summary of the following financial report containing 84 words or less."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "document: This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "query: A company wants to analyze their customer support calls to identify the emotions of the customers. Let me know how to proceed."}
{"completion": "Zixtrauce/JohnBot", "text": "document: JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface."}
{"completion": "Zixtrauce/JohnBot", "text": "query: I need to chat with an AI who's well-trained in real-life chat and implement a conversation with it."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-finetuned-kinetics", "text": "query: We are developing a system to classify and organize videos based on their content. We need the assistant to help us with this."}
{"completion": "keremberke/yolov8s-hard-hat-detection", "text": "document: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications."}
{"completion": "keremberke/yolov8s-hard-hat-detection", "text": "query: Our company needs a safety system that can detect whether our workers are wearing hard hats on construction sites."}
{"completion": "impira/layoutlm-document-qa", "text": "document: A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}
{"completion": "impira/layoutlm-document-qa", "text": "query: Our customer needs an application to extract information from received invoices. The app needs to extract invoice number, date, and total amount."}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: Our music streaming service wants to analyze the audio content of songs to create playlists for different moods. We'll need to extract audio features from the songs."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "query: We are working on a background noise reduction tool to improve the quality of audio calls."}
{"completion": "EleutherAI/gpt-j-6B", "text": "document: GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI."}
{"completion": "EleutherAI/gpt-j-6B", "text": "query: Write an AI-based dinner party script involving four characters and a surprising event."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "document: A LayoutLM model for document question answering."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "query: Can you analyze important text details from medical forms and help create a medical history summary based on the input data?"}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-112116", "text": "query: Develop an application to estimate the depth of objects in an image for better navigation of drones in various outdoor environments."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "document: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "query: Our company needs to create an attention-grabbing poster. It should showcase a unicorn galloping on a snowy mountain."}
{"completion": "lllyasviel/control_v11p_sd15_scribble", "text": "document: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_scribble", "text": "query: I want to generate an image of a royal chamber with a fancy bed using AI."}
{"completion": "google/flan-t5-small", "text": "document: FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application."}
{"completion": "google/flan-t5-small", "text": "query: \"Experience the ultimate comfort with our premium-quality shoes.\""}
{"completion": "impira/layoutlm-document-qa", "text": "document: A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}
{"completion": "impira/layoutlm-document-qa", "text": "query: Customers need to extract invoice information by uploading an image of the invoice. The system should answer specific questions about the invoice."}
{"completion": "GreeneryScenery/SheepsControlV5", "text": "document: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation."}
{"completion": "GreeneryScenery/SheepsControlV5", "text": "query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?"}
{"completion": "srg/outhimar_64-Close-regression", "text": "document: Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt."}
{"completion": "srg/outhimar_64-Close-regression", "text": "query: Our finance team needs to predict the closing price of a particular stock. We require a simple baseline model for the task."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "document: This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "query: Classify an audio clip to see if it is a digit between 0 and 9."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "document: A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "query: We want to build a program where user can provide text in German for zero-shot classification to identify the topics in a text."}
{"completion": "impira/layoutlm-invoices", "text": "document: This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head."}
{"completion": "impira/layoutlm-invoices", "text": "query: We have a pile of invoices from various suppliers, and we need to find which one has the highest total on the invoice."}
{"completion": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "text": "document: SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output."}
{"completion": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "text": "query: We want to analyze scientific publications on COVID-19 and extract relevant information. Get a unified representation for specific terms."}
{"completion": "google/byt5-small", "text": "document: ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5. ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is usable on a downstream task. ByT5 works especially well on noisy text data, e.g., google/byt5-small significantly outperforms mt5-small on TweetQA."}
{"completion": "google/byt5-small", "text": "query: Our mobile app development team is trying to translate user messages from English to French. Please help them to demonstrate how they can integrate the language translation code."}
{"completion": "videomae-small-finetuned-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-small-finetuned-ssv2", "text": "query: Provide a solution to apply video classification on user-generated content to identify whether it is a personal, sports, or educational clip."}
{"completion": "openai/whisper-small", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages."}
{"completion": "openai/whisper-small", "text": "query: We want to record voice meetings and quickly create the transcriptions that can be attached to meeting minutes."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "document: A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-ar", "text": "query: The company needs to communicate with Arabic-speaking clients. Develop a system to translate English text into Arabic."}
{"completion": "blip-image-captioning-large", "text": "document: BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}
{"completion": "blip-image-captioning-large", "text": "query: I am looking to create a personal AI-powered decor assistant which can provide short descriptions for the uploaded pictures of furniture for my online shop."}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "document: MetricGAN-trained model for Enhancement"}
{"completion": "speechbrain/metricgan-plus-voicebank", "text": "query: I have an old recording with a lot of noise. I want to clean it and make it more audible."}
{"completion": "kredor/punctuate-all", "text": "document: A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian."}
{"completion": "kredor/punctuate-all", "text": "query: We are developing an internationalization platform for smartphone applications. We need to punctuate translation automatically."}
{"completion": "lysandre/tiny-vit-random", "text": "document: A tiny-vit-random model for image classification using Hugging Face Transformers."}
{"completion": "lysandre/tiny-vit-random", "text": "query: We are building an app that needs to classify images. Can you provide a code example using a pre-trained image classification model from Hugging Face?"}
{"completion": "videomae-base-finetuned-RealLifeViolenceSituations-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations."}
{"completion": "videomae-base-finetuned-RealLifeViolenceSituations-subset", "text": "query: We are providing a surveillance video to evaluate and predict for potential violence occurance."}
{"completion": "anilbs/segmentation", "text": "document: Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Herv\u00e9 Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space."}
{"completion": "anilbs/segmentation", "text": "query: We are building an automated video conferencing system. Now we need to detect when participants are speaking."}
{"completion": "flair/ner-english-large", "text": "document: This is the large 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on document-level XLM-R embeddings and FLERT."}
{"completion": "flair/ner-english-large", "text": "query: A friend wants to automate the process of extracting names, locations, and organizations from paragraphs of text. Explain how they can do it with the Flair library."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "query: Prepare a system that will answer questions about statistical information in tabular data."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "document: BrandonBot4Epochs is a conversational model trained on the GPT-2 architecture for text generation. It can be used to generate responses in a chatbot-like interface."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "query: Recently I started learning pyschology, I want to ask some questions about a certain topic. I would like a chatbot to help me with that."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "document: This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "query: There are journalists writing articles about food. We need to categorize these articles into different genres like French cuisine, Italian cuisine, Chinese cuisine, vegan, and dessert."}
{"completion": "openai-gpt", "text": "document: openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long-range dependencies."}
{"completion": "openai-gpt", "text": "query: I am developing a Mars simulation. I need to create random Mars-related events and descriptions."}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "document: Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack"}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "query: We are building a digital assistant and want to be able to distinguish between questions and statements made by users."}
{"completion": "facebook/mbart-large-50-many-to-many-mmt", "text": "document: mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper."}
{"completion": "facebook/mbart-large-50-many-to-many-mmt", "text": "query: We are translating a product description from French to English for our international website."}
{"completion": "google/pegasus-newsroom", "text": "document: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks."}
{"completion": "google/pegasus-newsroom", "text": "query: We are a news agency, and we want to generate a summary of an article."}
{"completion": "sentence-transformers/paraphrase-distilroberta-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-distilroberta-base-v2", "text": "query: We need a way to determine if two product descriptions on our e-commerce website are duplicates."}
{"completion": "904029577", "text": "document: This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams."}
{"completion": "904029577", "text": "query: Analyze the given text and identify entities such as names, organizations, and locations in it."}
{"completion": "mazkooleg/0-9up-ast-ft", "text": "document: This model is a fine-tuned version of MIT/ast-finetuned-speech-commands-v2 on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0210, Accuracy: 0.9979"}
{"completion": "mazkooleg/0-9up-ast-ft", "text": "query: Develop an application to recognize spoken digits from 0 to 9 in voice recordings of customers."}
{"completion": "lllyasviel/control_v11p_sd15s2_lineart_anime", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images."}
{"completion": "lllyasviel/control_v11p_sd15s2_lineart_anime", "text": "query: My daughter loves anime and wants a new wallpaper for her computer. Help me generate a new anime-style image following her description of a warrior girl in the jungle."}
{"completion": "google/pegasus-large", "text": "document: google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks."}
{"completion": "google/pegasus-large", "text": "query: Generate a short summary of a news article, which will be suitable for sharing on social media platforms."}
{"completion": "google/flan-t5-small", "text": "document: FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application."}
{"completion": "google/flan-t5-small", "text": "query: Create an AI system that translates input text from one language to another, specifically German to English."}
{"completion": "bert-base-multilingual-cased", "text": "document: BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-multilingual-cased", "text": "query: You are working on a book which language you are not familiar with, you want to consult to fill the blanks of a sentence."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "text": "query: A social media platform wants to provide personalized content for their users. We want to analyze the images users post and classify them into interest categories."}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "document: Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0. The model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning Wav2Vec2-Large-Robust on MSP-Podcast (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An ONNX export of the model is available from doi:10.5281/zenodo.6221127. Further details are given in the associated paper and tutorial."}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "query: A therapist's client is reporting feelings of anxiety. The therapist would like to analyze the client's emotions during a session using a model trained on speech to better understand and help them."}
{"completion": "facebook/regnet-y-008", "text": "document: RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository."}
{"completion": "facebook/regnet-y-008", "text": "query: Last week we went to the zoo, and now we want to classify the pictures of the animals."}
{"completion": "table-question-answering-tapas", "text": "document: TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts."}
{"completion": "table-question-answering-tapas", "text": "query: The marketing team wants to analyze a customer dataset to know more about their purchase habits. Help them answer questions about the dataset."}
{"completion": "microsoft/tapex-large-sql-execution", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large-sql-execution", "text": "query: Our client wants a tool that can query financial data tables in natural language so that they can get specific information effortlessly."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "document: A tabular classification model for predicting carbon emissions in grams, trained using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "query: A client of mine wants to build an application deployed in a factory to send specific values from the machines to monitor the carbon emissions in real time."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "query: We want to make an application for users to play Breakout against an AI trained by an algorithm able to learn from the scenario."}
{"completion": "neulab/omnitab-large-finetuned-wtq", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab."}
{"completion": "neulab/omnitab-large-finetuned-wtq", "text": "query: I manage a delivery company that has a dataset of customer details like name, address, contact number, package details, and delivery date. I need to find the address of a specific customer named \"John Doe\"."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "query: The marketing team is analyzing user reviews to improve the products. They need to find out which comments are related to each other."}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "document: Baseline Model trained on tips5wx_sbh5 to apply regression on tip"}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "query: I own a restaurant and I want to predict the tips that servers will receive based on various factors like total bill amount, gender of the server, if they smoke, day of the week, time of day, and size of the group served."}
{"completion": "facebook/opt-13b", "text": "document: OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective."}
{"completion": "facebook/opt-13b", "text": "query: I have a prompt, \"A superhero jumps over a building and saves the world from\" andLooking for a short creative story based on that prompt."}
{"completion": "sentence-transformers/all-MiniLM-L12-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L12-v1", "text": "query: I am doing my homework for which I need  to understand the semantic similarity between sentences. Can I use a model that can help?"}
{"completion": "facebook/regnet-y-008", "text": "document: RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository."}
{"completion": "facebook/regnet-y-008", "text": "query: We are developing a mobile app for helping people count carbs in their food, like bread or pasta. We need to classify images of these food items."}
{"completion": "keremberke/yolov8s-table-extraction", "text": "document: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set."}
{"completion": "keremberke/yolov8s-table-extraction", "text": "query: A researcher needs to extract a table from a document to analyze the data they need."}
{"completion": "google/pegasus-large", "text": "document: google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks."}
{"completion": "google/pegasus-large", "text": "query: I need a summary of what happened during an eventful family vacation, highlighting the key experiences."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "document: DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "query: Recommend an NLP model for a mobile app that can answer questions based on a provided context or text. The model should be relatively lightweight and have good accuracy."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: We would like to know the total amount of an invoice with its image and provide an input question."}
{"completion": "blip2-opt-6.7b", "text": "document: BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-opt-6.7b", "text": "query: Explain a user-friendly solution for creating image captions, answering visual questions, and engaging in chat-like conversations using the AVAILABLE API."}
{"completion": "facebook/mask2former-swin-large-coco-panoptic", "text": "document: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-large-coco-panoptic", "text": "query: To improve the quality of our software for identifying plants and animals, we need to segment the images into different regions."}
{"completion": "deepset/bert-medium-squad2-distilled", "text": "document: This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library."}
{"completion": "deepset/bert-medium-squad2-distilled", "text": "query: I own a travel agency. I want my clients to be able to ask questions about the destinations. The final answer must be extracted from the text that describes provided travel itineraries."}
{"completion": "utyug1/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "utyug1/sd-class-butterflies-32", "text": "query: Our client is working on a new children's book featuring cute butterflies. Generate an image of a cute butterfly."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-092352", "text": "query: An civil engineer company is working on constructing a building, and we need a depth estimation of the proposed site by analyzing the image."}
{"completion": "plguillou/t5-base-fr-sum-cnndm", "text": "document: This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization."}
{"completion": "plguillou/t5-base-fr-sum-cnndm", "text": "query: As a journalist, I need to write a summary for an article in my next French news release."}
{"completion": "microsoft/speecht5_vc", "text": "document: SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks."}
{"completion": "microsoft/speecht5_vc", "text": "query: We are conducting a demo for a machine learning tool for converting speech of one person to another person's voice. We are going to use a sample speech from a dataset to demonstrate voice conversion."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "document: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only."}
{"completion": "stabilityai/stable-diffusion-2-base", "text": "query: I want a chatbot for a customer whose primary purpose is to get images from their textual description."}
{"completion": "google/pix2struct-chartqa-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}
{"completion": "google/pix2struct-chartqa-base", "text": "query: My company deals with chart analysis. We need a tool to analyze charts and answer our questions about them."}
{"completion": "saltacc/anime-ai-detect", "text": "document: A BEiT classifier to see if anime art was made by an AI or a human."}
{"completion": "saltacc/anime-ai-detect", "text": "query: I'm working on a project to detect whether an anime art is generated by an AI or made by a human artist. Let me know the best model for this problem."}
{"completion": "Jean-Baptiste/camembert-ner", "text": "document: camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities."}
{"completion": "Jean-Baptiste/camembert-ner", "text": "query: \"Apple a \u00e9t\u00e9 fond\u00e9e le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs \u00e0 Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitu\u00e9e sous forme de soci\u00e9t\u00e9 le 3 janvier 1977 \u00e0 l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refl\u00e9ter la diversification de ses produits, le mot \u00ab computer \u00bb est retir\u00e9 le 9 janvier 2015.\""}
{"completion": "cl-tohoku/bert-base-japanese", "text": "document: This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization."}
{"completion": "cl-tohoku/bert-base-japanese", "text": "query: We have a half-written Japanese marketing statement, and we need your help to complete it by filling in the blanks."}
{"completion": "speechbrain/sepformer-wham", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise."}
{"completion": "speechbrain/sepformer-wham", "text": "query: The company is working on audio processing and needs to separate an audio file with multiple speakers."}
{"completion": "Minecraft-Skin-Diffusion", "text": "document: Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods."}
{"completion": "Minecraft-Skin-Diffusion", "text": "query: As the marketing team wants to prepare a new engaging Minecraft campaign, they asked for a unique, distinctive Minecraft skin for their main character."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "query: I have a security camera at the entrance of my house, and I want to detect whether a person or something else is passing by."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "document: Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-small", "text": "query: Determine if the given sentences are related or if they contradict each other."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "query: I am building an expert system for an automotive repair shop. I want to match customer complaints with potential solutions based on similarity."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "document: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "query: I am working on a construction site safety compliance project. I need a solution to automatically analyze images of the site and identify if workers are wearing hard hats."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "document: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "query: Detect objects in a given video game screenshot for better gameplay insights and visualizations."}
{"completion": "nitrosocke/nitro-diffusion", "text": "document: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use."}
{"completion": "nitrosocke/nitro-diffusion", "text": "query: We are working on a children's book, and we need to create an illustration of a \"friendly dragon playing with kids in a park\"."}
{"completion": "google/pegasus-newsroom", "text": "document: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks."}
{"completion": "google/pegasus-newsroom", "text": "query: We need to find a model that can help us create an automatic summary for a long news article received from our source."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "document: This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "query: We are creating an audio guide for a museum. Turn the texts about paintings into audio files."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "document: BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "query: Please develop a program to help travelers remember city capital names. If the traveler asks you for the capital of a specific country, you should give the correct answer."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "query: Develop a service that detects whether a video is related to playing soccer or practicing yoga."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "document: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "query: We have a client who wants to extract tables from scanned documents. Build a program that analyzes a given image and identifies tables within the document."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "query: Architects want to analyze the building elements in each floor for automation purposes. They need to semantically divide and categorize them."}
{"completion": "tiny-wav2vec2-stable-ln", "text": "document: A tiny wav2vec2 model for Automatic Speech Recognition"}
{"completion": "tiny-wav2vec2-stable-ln", "text": "query: A digital assistant embedded in the phone would transcribe an audio file into text, so that it could be easily shared at a later point."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "document: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "query: I am creating an application for detecting the presence of planes in aerial images. I need to use a pre-trained model to identify the planes."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "query: We need to generate an image of a cute butterfly for a children's book cover."}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "document: A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data."}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "query: Our company is looking at measuring annual carbon emissions in real-time to comply with environmental regulations. Help us organize the data."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "document: This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "query: Our corporation is analyzing press releases to better understand the content. We need to extract named entities from these press releases."}
{"completion": "edbeeching/decision-transformer-gym-halfcheetah-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment"}
{"completion": "edbeeching/decision-transformer-gym-halfcheetah-expert", "text": "query: We are working on proposing an AI-driven robotic cheetah. How do we use the pre-trained AI to drive the cheetah?"}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "document: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50."}
{"completion": "TahaDouaji/detr-doc-table-detection", "text": "query: Detect tables within documents to automatically process data. We should be able to identify both bordered and borderless tables."}
{"completion": "vintedois-diffusion-v0-1", "text": "document: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps."}
{"completion": "vintedois-diffusion-v0-1", "text": "query: I'm writing a children's book, and I need a picture of a friendly-looking elephant playing soccer in a sunny meadow."}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "document: This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language."}
{"completion": "bert-large-uncased-whole-word-masking-squad2", "text": "query: A student has a reading comprehension test about the importance of model conversion. Generate an answer to the question \"Why is model conversion important?\" from the given context."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "document: A tiny random model for Visual Question Answering using the VILT framework."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "query: We have a virtual tour application and want to answer users' questions about a picture they are seeing."}
{"completion": "xm_transformer_unity_en-hk", "text": "document: Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain."}
{"completion": "xm_transformer_unity_en-hk", "text": "query: We are developing a speech-to-speech translation system for a client. We need to convert English audio into Hokkien speech."}
{"completion": "t5-base", "text": "document: T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library."}
{"completion": "t5-base", "text": "query: \"Recent studies show that owning a dog is beneficial for your health. Researchers found that dog owners tend to have lower levels of anxiety and depression. Additionally, many dog owners report increased levels of physical activity due to regular dog walks.\""}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "document: Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "query: We have a folder of scanned documents and need to extract information by asking questions about the documents."}
{"completion": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", "text": "query: The company is working on a chatbot that needs to understand similar phrases from different users. We need a way to identify these phrases."}
{"completion": "cpierse/wav2vec2-large-xlsr-53-esperanto", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "cpierse/wav2vec2-large-xlsr-53-esperanto", "text": "query: Parse a speech file that contains business instructions in Esperanto."}
{"completion": "flair/ner-english-fast", "text": "document: This is the fast 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-fast", "text": "query: We are a business publication, and we need to extract the names of people and companies from a given text."}
{"completion": "facebook/mask2former-swin-tiny-coco-instance", "text": "document: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation."}
{"completion": "facebook/mask2former-swin-tiny-coco-instance", "text": "query: Determine which fruits are in an image and differentiate each individual fruit in the picture."}
{"completion": "lysandre/tapas-temporary-repo", "text": "document: TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up."}
{"completion": "lysandre/tapas-temporary-repo", "text": "query: We need to find a salesperson's total revenue by finding the revenue for each product sold by him last month."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023", "text": "document: A LayoutLMv2 model for document question answering."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023", "text": "query: Our legal department needs to extract specific information from scanned documents. They want to retrieve answers to their questions based on the content of these documents."}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "document: This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder."}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "query: We are working on a project of visual storytelling. We need to generate images from text descriptions."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "document: LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "query: We have a database of legal documents and need to find the best model to complete sentences in these documents."}
{"completion": "git-large-coco", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}
{"completion": "git-large-coco", "text": "query: We need to create an article related to image recognition technologies. The article will require captioning images based on their contents. Can you help us with this task?"}
{"completion": "lewtun/tiny-random-mt5", "text": "document: A tiny random mt5 model for text generation"}
{"completion": "lewtun/tiny-random-mt5", "text": "query: I'm planning to build an AI story generator for my blog. Can you recommend a solution for generating a short story from a sentence?"}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: As a representative of the governor's office, you need to answer questions from the public using an AI-based chat system."}
{"completion": "MCG-NJU/videomae-base-short-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-short-finetuned-kinetics", "text": "query: Our software needs to analyze videos of sport activities to detect and classify joggers, swimmers, and cyclists."}
{"completion": "opus-mt-sv-en", "text": "document: A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece."}
{"completion": "opus-mt-sv-en", "text": "query: Translate a Swedish message for a user into English for better understanding."}
{"completion": "facebook/blenderbot-400M-distill", "text": "document: BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX."}
{"completion": "facebook/blenderbot-400M-distill", "text": "query: Our company wants to use a chatbot for handling customer support. We want the chatbot to be capable of understanding human emotions and providing reasonable responses to customer queries."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "query: We run an online podcasting service. Our new requirement is to help our users easily separate speakers in their recordings."}
{"completion": "google/tapas-base-finetuned-sqa", "text": "document: TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up."}
{"completion": "google/tapas-base-finetuned-sqa", "text": "query: Our product needs to answer questions based on data tables. Recommend an API that can do that."}
{"completion": "sileod/deberta-v3-base-tasksource-nli", "text": "document: DeBERTa-v3-base fine-tuned with multi-task learning on 520 tasks of the tasksource collection. This checkpoint has strong zero-shot validation performance on many tasks, and can be used for zero-shot NLI pipeline (similar to bart-mnli but better)."}
{"completion": "sileod/deberta-v3-base-tasksource-nli", "text": "query: In a IT support group, we need to categorize incoming requests into hardware, software, or network issues."}
{"completion": "facebook/bart-large-mnli", "text": "document: This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities."}
{"completion": "facebook/bart-large-mnli", "text": "query: Develop an algorithm that will find which category to put product reviews in."}
{"completion": "naver-clova-ix/donut-base", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base", "text": "query: As a teacher, I need to generate a short summary from an image to teach my students."}
{"completion": "camembert-base", "text": "document: CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks."}
{"completion": "camembert-base", "text": "query: I am learning French, and I need to fill in the missing words in some sentences correctly."}
{"completion": "damo-vilab/text-to-video-ms-1.7b", "text": "document: A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications."}
{"completion": "damo-vilab/text-to-video-ms-1.7b", "text": "query: A director needs synthetic video clips of various thrilling movie scenes based on the text descriptions he provided."}
{"completion": "duncan93/video", "text": "document: A text-to-video model trained on OpenAssistant/oasst1 dataset."}
{"completion": "duncan93/video", "text": "query: Develop a system that can generate short video clips from a given textual description of a scene."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "document: A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "query: As an executive assistant in an international trading company, I often receive emails in English, and I need to translate them into Italian for my boss to read."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "query: Our company deals with answering questions, and we need to rank the answers relevance to provide the best answer to the customers."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "document: This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "query: I need to classify an online article in a language different from English; please help me classify this text written in a foreign language."}
{"completion": "clip-vit-base-patch32-ko", "text": "document: Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data."}
{"completion": "clip-vit-base-patch32-ko", "text": "query: A Korean doctor will use the model to classify diseases resarch from the given medical image."}
{"completion": "google/ncsnpp-celebahq-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-celebahq-256", "text": "query: We are a company providing product mockups and we need to generate images of people to be placed in the mockups."}
{"completion": "google/ddpm-church-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference."}
{"completion": "google/ddpm-church-256", "text": "query: The company wants to generate realistic images of churches for a virtual tour application."}
{"completion": "dqn-CartPole-v1", "text": "document: This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "dqn-CartPole-v1", "text": "query: A gaming company is engaging in the creation of a new player-vs-player game, and they need a computer opponent that can play the game."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "document: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "query: Building a monitoring system for manufacturing, to check for any defects on printed circuit boards (PCBs). Help me identify PCB defects."}
{"completion": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", "text": "query: Our company develops an AI-based text similarity tool. We need to know if two sentences have similar meanings."}
{"completion": "speechbrain/sepformer-wham", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise."}
{"completion": "speechbrain/sepformer-wham", "text": "query: How can I separate vocal from the music in an audio file consisting of vocals and music mixed together?"}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "document: bert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER)."}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "query: We are a news agency and we need a system for automatically extracting the names of persons, organizations, and locations from articles in multiple languages."}
{"completion": "frizwankhan/entity-linking-model-final", "text": "document: A Document Question Answering model based on layoutlmv2"}
{"completion": "frizwankhan/entity-linking-model-final", "text": "query: A legal office needs a model to extract essential information, such as names, dates, and relations, from scanned documents while answering relevant questions. Help them create a suitable model."}
{"completion": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6", "text": "document: The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks."}
{"completion": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6", "text": "query: Our company is in language translation. We need to analyze two translated texts to identify their similarity."}
{"completion": "superb/hubert-large-superb-er", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-large-superb-er", "text": "query: My company wishes to analyze customer care calls and automatically identify the emotions of the callers."}
{"completion": "bert-base-multilingual-cased", "text": "document: BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-multilingual-cased", "text": "query: We have a CMS moving to international markets, so we need to analyze content in multiple languages."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "query: Create a gaming bot that leverages reinforcement learning to play Atari's Breakout game."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "query: We are a landscape architecture consultancy and need to develop an autonomous drone for site mapping. The drone will use images to generate semantic segmentation to help us better understand the environment."}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "document: CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER"}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "query: We need to analyze medical terms and identify their embeddings to build a medical glossary for an app. Please help in using the appropriate API."}
{"completion": "glpn-nyu-finetuned-diode-221116-104421", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-104421", "text": "query: A drone company wants to estimate the depth of objects captured by its drone's camera using a pre-trained model. Estimate the depth of a given image from the drone's camera."}
{"completion": "google/pix2struct-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}
{"completion": "google/pix2struct-base", "text": "query: We have a user interface design company, and we need to generate the HTML code from images representing website design."}
{"completion": "google/pegasus-newsroom", "text": "document: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks."}
{"completion": "google/pegasus-newsroom", "text": "query: We are building question-and-answer system to provide the summary of news articles as answer."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "document: This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "query: A group of young developers is developing a soccer game for mobile and wants to create an AI agent to control their in-game soccer team. They need to make use of the available model."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: Transform this text from a customer review without punctuation into a more readable format with punctuation marks."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023", "text": "document: A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023", "text": "query: We're building an app that can find information in photos of documents, even if the text is formatted differently. I need the assistant to extract the answers."}
{"completion": "castorini/doc2query-t5-base-msmarco", "text": "document: A T5 model trained on the MS MARCO dataset for generating queries from documents."}
{"completion": "castorini/doc2query-t5-base-msmarco", "text": "query: Our newspaper website has an article about space exploration. To improve search engine optimization, we need to generate related search queries based on the article text."}
{"completion": "MCG-NJU/videomae-large-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-large-finetuned-kinetics", "text": "query: We are collaborating with a health and fitness company. We need to classify their workout videos into different types."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: The company requires an effective information retrieval system to quickly and accurately search through their internal knowledge base and documentation."}
{"completion": "xlm-roberta-large", "text": "document: XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "xlm-roberta-large", "text": "query: We are building a language model for an online educational platform, and the model will be used in essays and documents creation. The model should be able to predict the masked word effectively."}
{"completion": "blip-image-captioning-large", "text": "document: BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}
{"completion": "blip-image-captioning-large", "text": "query: We're developing an app that generates descriptions for images. We want to implement a pretrained model that can generate captions for images without any additional text input."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "query: Help me find the most relatable answer from a list of candidate answers to the question \"What is the significance of the rainforest?\""}
{"completion": "distilbert-base-uncased", "text": "document: DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. It was pretrained with three objectives: Distillation loss, Masked language modeling (MLM), and Cosine embedding loss. This model is uncased and can be used for masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task."}
{"completion": "distilbert-base-uncased", "text": "query: We are working on an AI to help kids with their homeworks. We want to plug in a predicted word in a masked sentence using natural language processing."}
{"completion": "MCG-NJU/videomae-large-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-large-finetuned-kinetics", "text": "query: I am hosting a sports event and need to determine the type of sport being played based on video recordings."}
{"completion": "SYSPIN/Telugu_Male_TTS", "text": "document: A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face."}
{"completion": "SYSPIN/Telugu_Male_TTS", "text": "query: An audio instruction for blind students is in demand. Let's create one in Telugu language."}
{"completion": "CZ_DVQA_layoutxlm-base", "text": "document: A Document Question Answering model based on LayoutXLM."}
{"completion": "CZ_DVQA_layoutxlm-base", "text": "query: I am an HR manager, I need help understanding a particular section of a contract. Can you please answer my question about the contract?"}
{"completion": "ckiplab/bert-base-chinese-pos", "text": "document: This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition)."}
{"completion": "ckiplab/bert-base-chinese-pos", "text": "query: We are building an app for Chinese language students. Among other things, the app should analyze and breakdown Chinese sentences with automatic tokenization and part-of-speech tagging."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: The botanic research center needs a quick way to classify different types of Iris flowers when each has measurements about its petals and sepals. How can we devise a reliable system for them?"}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "document: Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "query: I am designing a trivia game and I need to generate questions automatically from a given context and answer."}
{"completion": "MCG-NJU/videomae-base-short", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "MCG-NJU/videomae-base-short", "text": "query: The sports club is looking for a tool to analyze basketball videos and categorize them by the types of plays being executed. They want the tool to analyze videos, such as layups, dunks, three-pointers, etc."}
{"completion": "openai/whisper-base", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-base", "text": "query: As an educational content creator, I need a transcription service for my videos. I want the extracted text to subdub my content for a broader audience."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "document: Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "query: I have a list of answers that I need to append an answer statement into the question. The matching context to give the model is included in the programming."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L6-v2", "text": "query: We are creating an AI tool to find semantic matches for user-inputted keywords."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "document: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "query: Our client is an esport gaming company. They want to develop an object detection model especially for the game Valorant."}
{"completion": "videomae-small-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "videomae-small-finetuned-kinetics", "text": "query: Sally needs adialtive chatting for her presentation. She needs to understand the type of movie she is watching based on a video clip."}
{"completion": "flair/ner-english", "text": "document: This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english", "text": "query: We are going to create an assistant for a news website that can analyze the given texts and highlight all the named entities."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "document: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects."}
{"completion": "keremberke/yolov8m-valorant-detection", "text": "query: A gaming company is developing an AI-powered cheat detection system that flags suspicious activity in the game screenshots. We need to detect different objects in those images."}
{"completion": "sentence-transformers/gtr-t5-base", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search."}
{"completion": "sentence-transformers/gtr-t5-base", "text": "query: The newsroom needs a tool that quickly analyzes news articles and finds which ones discuss similar topics. Please use sentence embeddings to accomplish this."}
{"completion": "graphormer-base-pcqm4mv2", "text": "document: The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling."}
{"completion": "graphormer-base-pcqm4mv2", "text": "query: I am a chemist. I would like a machine learning model to help me classify molecules based on their properties. Design a model to help me achieve this goal."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "document: A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "query: Create a summary of a blog post about autonomous vehicles."}
{"completion": "bert-large-uncased", "text": "document: BERT large model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters. The model is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-large-uncased", "text": "query: \"It was a beautiful day, so we decided to go to the\""}
{"completion": "glpn-kitti-finetuned-diode", "text": "document: This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset."}
{"completion": "glpn-kitti-finetuned-diode", "text": "query: We are a team of researchers working on autonomous vehicles. We need to estimate depth information from a single image."}
{"completion": "clipseg-rd64-refined", "text": "document: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation."}
{"completion": "clipseg-rd64-refined", "text": "query: We have a drone company that needs our help in image segmentation for mapping purposes. It is crucial for us to identify various land surfaces based on the images."}
{"completion": "facebook/bart-large", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-large", "text": "query: We would like to build a story plot generator. Give us guidelines on how to use the API."}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "document: Binary Classification model for Carbon Emissions prediction"}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "query: As part of our transition to environmentally friendly energy consumption, we need to predict which homes have higher carbon emissions using historical consumption data."}
{"completion": "tts-hifigan-german", "text": "document: A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram."}
{"completion": "tts-hifigan-german", "text": "query: As a virtual assistant company, we are asked to produce a German audio file for some text to be featured in a multimedia presentation about the history of the German language."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "document: This is a trained model of a DQN agent playing MountainCar-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "query: We have created a simulation game based on car racing. We want to train an AI to play the game well."}
{"completion": "GreeneryScenery/SheepsControlV3", "text": "document: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data."}
{"completion": "GreeneryScenery/SheepsControlV3", "text": "query: We are a creative digital marketing agency. We want to create advertisements based on an input image and text guidance for our client."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "query: We have a dataset including several factors such as fuel consumption and engine size. Our goal is to predict the carbon emission rates for different vehicles."}
{"completion": "lllyasviel/sd-controlnet-openpose", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-openpose", "text": "query: Our company develops video games where players perform dance moves using Kinect technology! We want to simulate the characters poses according to the moves."}
{"completion": "microsoft/tapex-large", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large", "text": "query: I have a table loaded with financial data, I need to get the top 5 more profitable items."}
{"completion": "rajistics/california_housing", "text": "document: A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices."}
{"completion": "rajistics/california_housing", "text": "query: As a real estate developer, we need to predict the prices of houses in California based on the given attributes."}
{"completion": "facebook/blenderbot_small-90M", "text": "document: Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation."}
{"completion": "facebook/blenderbot_small-90M", "text": "query: I want to create an interactive chatbot for customers to ask questions about our store and products."}
{"completion": "glpn-nyu-finetuned-diode-221122-044810", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221122-044810", "text": "query: Planning to install an automated irrigation system that waters plants using computer vision. Determine the distance of plants from the sensor to optimize water usage."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "document: This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "query: I have a Python code snippet and I need an AI model to generate a brief summary of the code for documentation purposes. What model would you suggest?"}
{"completion": "videomae-base-finetuned-RealLifeViolenceSituations-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations."}
{"completion": "videomae-base-finetuned-RealLifeViolenceSituations-subset", "text": "query: Determine if a video contains violence or not."}
{"completion": "904029577", "text": "document: This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams."}
{"completion": "904029577", "text": "query: \"Microsoft acquires Github in 2018 for $7.5 billion\"."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "document: This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "query: positive, negative, or neutral."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "query: I am a civil engineer, and I need to build an AI system to estimate the depth of an object in an image."}
{"completion": "sentence-transformers/gtr-t5-base", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search."}
{"completion": "sentence-transformers/gtr-t5-base", "text": "query: The managers want to reduce time wasted on reading irrelevant emails. They need a system able to pair similar emails to speed up the reading process."}
{"completion": "pyannote/segmentation", "text": "document: A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework."}
{"completion": "pyannote/segmentation", "text": "query: I receive many telephone calls in my office. Create a solution to detect when there are multiple people speaking at the same time in the conversation."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: I have a scanned document and need to extract information from it. Help me extract the desired information by answering questions about the document."}
{"completion": "keremberke/yolov8m-blood-cell-detection", "text": "document: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset."}
{"completion": "keremberke/yolov8m-blood-cell-detection", "text": "query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?"}
{"completion": "bert-base-multilingual-cased", "text": "document: BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-multilingual-cased", "text": "query: Recently, our company started working in the international market, and we now receive emails in different languages. We need a language model that can understand and complete sentences in multiple languages."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "document: Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "query: Our team is trying to display high-quality generated butterfly images for a virtual garden. Provide a way to create images of butterflies."}
{"completion": "flair/upos-english", "text": "document: This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/upos-english", "text": "query: I want to automatically annotate the syntax roles of the words in a given English sentence using the best available API."}
{"completion": "microsoft/git-base-textvqa", "text": "document: GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}
{"completion": "microsoft/git-base-textvqa", "text": "query: A customer wants to know if a product in a provided image fulfill their requirements. Find necessary information by answering questions about the product."}
{"completion": "xlm-roberta-large", "text": "document: XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "xlm-roberta-large", "text": "query: I am a student learning different natural languages. I want to complete a sentence with a missing word."}
{"completion": "903929564", "text": "document: A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score."}
{"completion": "903929564", "text": "query: Our team needs to extract entities from user texts to better understand their content for tracking purposes, without exposing sensitive data."}
{"completion": "Alexei1/imdb", "text": "document: A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487."}
{"completion": "Alexei1/imdb", "text": "query: A movie review website uses a temperature-based color coding to represent the review sentiments. For a given movie, provide the color coding based on the ratings and reviews of the movies."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "query: An indoor game management software company needs to identify different types of indoor games played in the videos uploaded by users."}
{"completion": "yiyanghkust/finbert-tone", "text": "document: FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task."}
{"completion": "yiyanghkust/finbert-tone", "text": "query: Our company needs a system for analyzing financial news articles to assess the positive or negative market sentiment."}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "document: TAPAS mini model fine-tuned on Sequential Question Answering (SQA)"}
{"completion": "google/tapas-mini-finetuned-sqa", "text": "query: We need help from an AI to answer a series of questions based on this table."}
{"completion": "microsoft/tapex-base", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base", "text": "query: \"Company\", \"MarketCap\", and \"DividendYieldPercent\". Design a query that selects the company with the highest market cap."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "query: We need to compare product review comments and find the similar comments."}
{"completion": "NeuML/ljspeech-jets-onnx", "text": "document: ESPnet JETS Text-to-Speech (TTS) Model for ONNX exported using the espnet_onnx library. Can be used with txtai pipeline or directly with ONNX."}
{"completion": "NeuML/ljspeech-jets-onnx", "text": "query: Propose a solution to convert a news article into an audio file for a visually impaired person."}
{"completion": "pszemraj/long-t5-tglobal-base-16384-book-summary", "text": "document: A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text."}
{"completion": "pszemraj/long-t5-tglobal-base-16384-book-summary", "text": "query: We are a publishing company. Our user sent a long-description on their book. We need to create a summary of the provided book description."}
{"completion": "saltacc/anime-ai-detect", "text": "document: A BEiT classifier to see if anime art was made by an AI or a human."}
{"completion": "saltacc/anime-ai-detect", "text": "query: Evaluate if the submitted artwork for our online gallery was created by an AI or a human artist."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "query: We acquired a confidential business document containing important information. We need to locate any details about 'revenue.' Please guide me how to extract the revenue information using this model."}
{"completion": "Awais/Audio_Source_Separation", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset."}
{"completion": "Awais/Audio_Source_Separation", "text": "query: Our client is a podcast company who needs to separate speakers in recorded conversations."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "document: Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "query: We are an ad company, and we need to generate a photorealistic image of a red sports car in a mountainous setting."}
{"completion": "michellejieli/NSFW_text_classifier", "text": "document: DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API."}
{"completion": "michellejieli/NSFW_text_classifier", "text": "query: I need to build a filter that prevents NSFW content from being posted on a social media platform. Help me classify the content."}
{"completion": "google/ddpm-celebahq-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46."}
{"completion": "google/ddpm-celebahq-256", "text": "query: We are now building a VR game. I want to generate a celeb face using an AI model to put as a game character."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "query: Create an artificial image of a bedroom using the DDPM model."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "query: Develop a program to create an image of a butterfly using artificial intelligence."}
{"completion": "DCUNet_Libri1Mix_enhsingle_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}
{"completion": "DCUNet_Libri1Mix_enhsingle_16k", "text": "query: I have an old recording from my grandparents, and it has a lot of background noise. Help me improve the audio quality."}
{"completion": "microsoft/tapex-large-sql-execution", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large-sql-execution", "text": "query: I need to find out which years correspond to the Olympic Games held in Paris from a dataset."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased."}
{"completion": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: Your company is creating an AI-enabled document assistant. Implement a functionality to answer questions based on the content of business documents."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-english", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-english", "text": "query: We need to convert some audio recordings to text to analyze customer feedback for an e-commerce company."}
{"completion": "prithivida/parrot_paraphraser_on_T5", "text": "document: Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models."}
{"completion": "prithivida/parrot_paraphraser_on_T5", "text": "query: I need to paraphrase some sentences about travel to promote my travel agency. It should be more appealing and engaging."}
{"completion": "optimum/t5-small", "text": "document: T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization."}
{"completion": "optimum/t5-small", "text": "query: I want to build an app to translate English text to French in real-time."}
{"completion": "flexudy/t5-base-multi-sentence-doctor", "text": "document: Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text."}
{"completion": "flexudy/t5-base-multi-sentence-doctor", "text": "query: We need to create a sentence corrector for our chatbot. Please make a sentence corrector to have our chatbot provide error-free responses."}
{"completion": "microsoft/resnet-18", "text": "document: ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision."}
{"completion": "microsoft/resnet-18", "text": "query: Our company requires a system to better sort the images of cats and dogs. Implement this functionality."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "query: As a sports performance analysis company, we need to analyze the videos of athletes playing various sports."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "query: My company needs an AI system that can classify videos based on a pre-defined set of categories. Can you describe a model that could be used for this purpose?"}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al."}
{"completion": "facebook/timesformer-hr-finetuned-k400", "text": "query: I'm developing a fitness app and I want to automatically detect a person's activity in the video."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "query: Our company is building an autonomous vehicle system. We want to understand an image's depth information with a neural network."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "document: A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "query: Our marketing team has compiled a table of potential customers and their preferences. They require a way to pull out the most relevant records of customers who meet certain criteria."}
{"completion": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli", "text": "document: This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark."}
{"completion": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli", "text": "query: Our company needs to sort customer support messages into categories like refund, technical issue, account related, and general inquiry. Suggest a method using NLP model."}
{"completion": "google/owlvit-base-patch16", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features."}
{"completion": "google/owlvit-base-patch16", "text": "query: There is a customer wanting to detect cats and dogs in the images uploaded to their website."}
{"completion": "videomae-small-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "videomae-small-finetuned-kinetics", "text": "query: Our team recently integrated cameras into our robots that patrol the company parking lot. We want the robots to be able to identify actions performed by individuals."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "document: This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data."}
{"completion": "speechbrain/spkrec-xvect-voxceleb", "text": "query: In our audio filtering system, we want to identify the speaker's identity to categorize audio files by speaker."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "query: You are a student working on a research project, and you need to find relevant information related to a specific topic. Rank the passages provided based on their relevance to your research question."}
{"completion": "glpn-nyu-finetuned-diode-221116-110652", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}
{"completion": "glpn-nyu-finetuned-diode-221116-110652", "text": "query: A developer working on a computer vision project requires a depth estimation model for their prototype. Suggest a suitable pre-trained model for their use."}
{"completion": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", "text": "document: A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", "text": "query: I've a picture of a vehicle, and I'd like to know if it's a car or a motorcycle."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-base-finetuned-ssv2", "text": "query: Our AI bot is designed to analyze videos and find inappropriate content, such as nudity, violence, or profanity. We need to classify activities in the videos."}
{"completion": "princeton-nlp/unsup-simcse-roberta-base", "text": "document: An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture."}
{"completion": "princeton-nlp/unsup-simcse-roberta-base", "text": "query: We need a tool to create clusters of text which are semantically similar."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "query: We were requested to build a video recommendation system that classifies videos based on their content in order to target users' preferences more accurately. We need to group them into related categories such as sports, technology, and animals."}
{"completion": "lllyasviel/control_v11p_sd15_openpose", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images."}
{"completion": "lllyasviel/control_v11p_sd15_openpose", "text": "query: Please generate an image based on the description \"a person running in a park\" using control points generated from a reference image of a person walking in the same park."}
{"completion": "Randeng-Pegasus-238M-Summary-Chinese", "text": "document: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs."}
{"completion": "Randeng-Pegasus-238M-Summary-Chinese", "text": "query: I want to summarize a long news article in Chinese so I can quickly read about the main topic."}
{"completion": "nateraw/vit-age-classifier", "text": "document: A vision transformer finetuned to classify the age of a given person's face."}
{"completion": "nateraw/vit-age-classifier", "text": "query: I'm working on a project to categorize user profiles based on their age group in our mobile app. We need to analyze the profile pictures and identify the age ranges."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "document: This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "query: Retrieve information about popular topics from a given Spanish news excerpt."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: At our economics department, we need to predict CO2 emissions for a given dataset, using XGBoost."}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "document: This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions."}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "query: I've been researching Charles Darwin. Here is a paragraph I found describing his work. Can you tell me his main contribution to the field of biology?"}
{"completion": "Rakib/roberta-base-on-cuad", "text": "document: This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents."}
{"completion": "Rakib/roberta-base-on-cuad", "text": "query: A client wants to build an application that can answer questions in legal documents. Can you build me a simple text-based question-answer model?"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: We are a digital encyclopedia company and want to find the most relevant content from a list of passages to answer specific questions."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "document: A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540."}
{"completion": "autotrain-dragino-7-7-1860763606", "text": "query: I'm exploring ways to predict carbon emission based on features in our tabular data. Write a script that loads a trained model to predict those values."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "document: A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "query: I'm living in Italy now and I receive some communication like emails written in Italian. Convert them to English for me."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "query: We are building an application that generates art from text descriptions. Users provide text prompts, and we want to display art inspired by what they write."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "document: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead']."}
{"completion": "keremberke/yolov8n-csgo-player-detection", "text": "query: Global Offensive (CS:GO) match."}
{"completion": "facebook/opt-125m", "text": "document: OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation."}
{"completion": "facebook/opt-125m", "text": "query: We need to create a short story to demonstrate our AI understanding a prompt with a rich context."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "document: BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "query: Create an AI product for a television channel to help analyzing images and answering questions related to content on real-time during a broadcast."}
{"completion": "facebook/bart-large", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-large", "text": "query: I want to analyze the sentiments of my novel's main character over time. Please help me generate the sentiment scores for each chapter."}
{"completion": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli", "text": "document: This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper."}
{"completion": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli", "text": "query: We are hosting a news portal and want to categorize news articles automatically. The categories include politics, economy, entertainment, and environment. Provide a solution for this task."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "document: This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "query: I am working on a project where I have a lot of long paragraphs and I need to extract answers based on certain questions I provide. How can I do this?"}
{"completion": "laion/CLIP-ViT-B-16-laion2B-s34B-b88K", "text": "document: A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model is intended for research purposes and can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "laion/CLIP-ViT-B-16-laion2B-s34B-b88K", "text": "query: I'd like to create an application that helps users identify whether a picture has a cat or a dog."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "document: A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "query:  The real estate company wants to predict housing prices. Find the best way to predict and justify it."}
{"completion": "keras-io/timeseries-anomaly-detection", "text": "document: This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics."}
{"completion": "keras-io/timeseries-anomaly-detection", "text": "query: We've been monitoring server performance metrics, and we need to be alerted when the data points to anomalous behavior. Design an automatic anomaly detection system."}
{"completion": "valhalla/longformer-base-4096-finetuned-squadv1", "text": "document: This is longformer-base-4096 model fine-tuned on SQuAD v1 dataset for question answering task. Longformer model created by Iz Beltagy, Matthew E. Peters, Arman Coha from AllenAI. As the paper explains it, Longformer is a BERT-like model for long documents. The pre-trained model can handle sequences with up to 4096 tokens."}
{"completion": "valhalla/longformer-base-4096-finetuned-squadv1", "text": "query: We are building an application that searches through large documents to find answers to questions. Develop a solution that can parse text and answer questions accurately."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "document: A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "query: I am an individual who loves traveling. Describe the scene in the picture I took during my last trip."}
{"completion": "facebook/maskformer-swin-tiny-coco", "text": "document: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository."}
{"completion": "facebook/maskformer-swin-tiny-coco", "text": "query: Build a tool that can remove the background in images with multiple objects."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "query: Our company is working on a project that requires extracting answers to questions from design blueprints and technical documents. We need to process both text and spatial layout information in these documents."}
{"completion": "edbeeching/decision-transformer-gym-halfcheetah-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment"}
{"completion": "edbeeching/decision-transformer-gym-halfcheetah-expert", "text": "query: A video game company wants to implement a reinforcement learning model for an AI-controlled character. The character called \"Half-Cheetah\" needs to move as efficiently as possible."}
{"completion": "prithivida/parrot_adequacy_model", "text": "document: Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser."}
{"completion": "prithivida/parrot_adequacy_model", "text": "query: I am a chatbot developer; I need to optimize my paraphrase creation for my replies. I aim to select the most adequate generated paraphrase."}
{"completion": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best", "text": "document: A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese."}
{"completion": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best", "text": "query: Our company wants to create a smart speaker device for Chinese users. We need a solution that can convert Chinese text into speech."}
{"completion": "microsoft/trocr-large-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-handwritten", "text": "query: We need to extract the text that is embedded in a cursive, hand-written image."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "query: I am an amateur astronomer and I would like a generated image of a galaxy."}
{"completion": "kan-bayashi_ljspeech_vits", "text": "document: A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}
{"completion": "kan-bayashi_ljspeech_vits", "text": "query: ###Instruction:I want to develop an application that allows users to input text and then the system generates speech based on the text."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "query: The car company wants to test the effect of different models on the perception of distance. They are now asking for a way to estimate the depth of an image."}
{"completion": "MCG-NJU/videomae-base", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches."}
{"completion": "MCG-NJU/videomae-base", "text": "query: We need a method to identify several sports in short video clips."}
{"completion": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2", "text": "document: An unconditional image generation model for generating Minecraft skin images using the diffusion model."}
{"completion": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2", "text": "query: We need to create a unique Minecraft skin for the user. Please create a procedural skin generation."}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819"}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "query: I am working on a project to extract fields from invoices, and I need a pretrained model to recognize text from document images."}
{"completion": "videomae-small-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "videomae-small-finetuned-kinetics", "text": "query: We are building a touchless system for hand gesture detection in a warehouse. Please provide me with a suggestion for a video-based gesture detection model."}
{"completion": "keremberke/yolov8m-blood-cell-detection", "text": "document: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset."}
{"completion": "keremberke/yolov8m-blood-cell-detection", "text": "query: We are a company working on building a diagnostic tool for medical staff. We need to detect blood cells in images."}
{"completion": "hubert-large-ll60k", "text": "document: Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss."}
{"completion": "hubert-large-ll60k", "text": "query: I am searching for a great all-around audio model that will work in a variety of specific applications such as audio classification, speaker identification, and emotion recognition."}
{"completion": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup", "text": "document: A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768)."}
{"completion": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup", "text": "query: We are building an app that allows users to upload images and it will automatically identify the object in the image. It can recognize simple objects like a cat, dog, bird, etc., as well as more complex objects like landmarks or specific species of plants."}
{"completion": "facebook/bart-large", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-large", "text": "query: I am a musician, I want to create a song lyrics for a pop song about love. Generate some song lyrics for me."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221221-102136", "text": "query: A real estate company needs an app that estimates the depth of rooms in photos to be uploaded to their website."}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "document: This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions."}
{"completion": "thatdramebaazguy/roberta-base-squad", "text": "query: Create a question-answer pipeline to help users find relevant information in a given text."}
{"completion": "valhalla/distilbart-mnli-12-9", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is used for zero-shot text classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-9", "text": "query: A business researcher needs to classify customer opinions about local restaurants. We need a way to identify the most relevant category for each review."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "document: Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "query: We want to implement a chatbot that can engage with users in natural conversations."}
{"completion": "clip-vit-base-patch32-ko", "text": "document: Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data."}
{"completion": "clip-vit-base-patch32-ko", "text": "query: Classify images of different food dishes and provide information if it's a pasta, pizza, burger, sushi or salad."}
{"completion": "ppo-seals-CartPole-v0", "text": "document: This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-seals-CartPole-v0", "text": "query: We are an educational technology company creating a reinforcement learning agent for an educational game. Show us the steps to load a pre-trained agent for CartPole."}
{"completion": "microsoft/git-base-textvqa", "text": "document: GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}
{"completion": "microsoft/git-base-textvqa", "text": "query: We are a car photoshoot company, and sometimes we need to answer our customer\u2019s question about the car picture. So, we need an AI module to help answer the questions about an image."}
{"completion": "pygmalion-1.3b", "text": "document: Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-1.3b", "text": "query: Please give me a solution that allows me to create a dialogue system for my web app. One that can involve a character persona."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "document: Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "query: In my medical startup, I need to share patients' records between doctors but keep their data private. I would like to use an automatic tool that hides personal information."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "document: Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "query: Quickly process an incoming message to scan for any potential personally identifiable information (PII) that might need to be redacted."}
{"completion": "pyannote/brouhaha", "text": "document: Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library."}
{"completion": "pyannote/brouhaha", "text": "query: A team is developing an audio conferencing app and needs to improve the audio quality for the participants by monitoring background noise and reverberation. "}
{"completion": "keremberke/yolov8m-plane-detection", "text": "document: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "query: We are working on an application to track planes in images taken from the Earth's surface. We need a model to identify airplanes in the images."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "query: We have some audio files, we need to separate the vocals and the instruments."}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "document: A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model."}
{"completion": "laion/CLIP-ViT-L-14-laion2B-s32B-b82K", "text": "query: To improve the visual search functionality of an online shopping app, we need to classify an image without explicit training."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "document: A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "query: I work for a movie website and I want to know the general sentiment of movie reviews before publishing them on our website."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "document: This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "query: Use the model to compare two pieces of text and return the logical relationship between them."}
{"completion": "microsoft/codebert-base", "text": "document: Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective."}
{"completion": "microsoft/codebert-base", "text": "query: We are building a project recommendation system that can recommend new code repositories based on users' previous interaction with programming languages. Extract features from the code and description using a pre-trained model."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "query: Our team is dedicated to proposing solutions that reduce CO2 emissions. We need to predict carbon emissions from a given dataset."}
{"completion": "google/ddpm-ema-cat-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17."}
{"completion": "google/ddpm-ema-cat-256", "text": "query: Our client needs to generate a unique image of a cat for their website. Let's use the DDPM model to do that."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "document: A model trained for binary classification of carbon emissions using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "query: A business is considering making environmentally friendly investments. Please analyze the carbon emissions data of potential investment companies in a tabular format to assist them in their decision-making process."}
{"completion": "michellejieli/emotion_text_classifier", "text": "document: DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise."}
{"completion": "michellejieli/emotion_text_classifier", "text": "query: The advertising department asked us to identify the emotions in the text of various advertisement slogans, as they believe that this will help them tailor their campaigns more effectively."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "document: MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "query: We are collaborating with a wildlife conservation agency. They need help in classifying animals in their natural habitat."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: I want to predict whether a person's income is higher or lower than $50,000 based on their demographic and socio-economic information."}
{"completion": "bigscience/bloom-560m", "text": "document: BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads."}
{"completion": "bigscience/bloom-560m", "text": "query: A Science Fiction enthusiast wants to write a short story about the possible future impact of Artificial Intelligence."}
{"completion": "tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa", "text": "query: Jennifer wants to create a chatbot for the students. The chatbot needs to receive questions as input and return answers after processing the school documents."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: I am working on a book club app that recommends new books. We receive images of book covers, and they usually contain the title and author name. Extract the title and author name from the book cover image."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "document: A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "query: Develop a chatbot to filter out gibberish messages before answering user queries."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "document: A model trained for binary classification of carbon emissions using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958767", "text": "query: We are a company that produces cars. Make a classification model that can predict whether our car will have a high or low carbon emission based on its characteristics."}
{"completion": "Helsinki-NLP/opus-mt-en-es", "text": "document: This model is a translation model from English to Spanish using the Hugging Face Transformers library. It is based on the Marian framework and trained on the OPUS dataset. The model achieves a BLEU score of 54.9 on the Tatoeba test set."}
{"completion": "Helsinki-NLP/opus-mt-en-es", "text": "query: \"Introducing our new product line, available now in stores and online.\""}
{"completion": "facebook/m2m100_1.2B", "text": "document: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token."}
{"completion": "facebook/m2m100_1.2B", "text": "query: We are developing a travel website where users can receive information about different countries in their native language. We need to translate various content to multiple languages to offer a personalized experience."}
{"completion": "damo-vilab/text-to-video-ms-1.7b", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input."}
{"completion": "damo-vilab/text-to-video-ms-1.7b", "text": "query: Generate a short video clip of a person practicing Tai Chi based on given text input."}
{"completion": "Antheia/Hanna", "text": "document: Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset."}
{"completion": "Antheia/Hanna", "text": "query: The company is working on a new robotics project. We need to estimate the control parameters of robotic arms."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "document: Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "query: I want an assistant to read out an article in a Taiwanese Hokkien accent."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "document: Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model."}
{"completion": "darkstorm2150/Protogen_x5.8_Official_Release", "text": "query: We are designing a new banner for our website about pets. We want an image to be generated for our banner."}
{"completion": "tts-hifigan-german", "text": "document: A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram."}
{"completion": "tts-hifigan-german", "text": "query: Our German-speaking customer is asking for a speech recording of the text \"Mary hatte ein kleines Lamm\". Create an audio file for this task."}
{"completion": "microsoft/git-base-coco", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "microsoft/git-base-coco", "text": "query: Design an advertisement for our smart speaker that automatically generates great image captions."}
{"completion": "facebook/dpr-question_encoder-single-nq-base", "text": "document: Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019)."}
{"completion": "facebook/dpr-question_encoder-single-nq-base", "text": "query: I want to create a question encoder that can be used to find an answer to any question about dogs."}
{"completion": "bert-base-uncased", "text": "document: BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-uncased", "text": "query: We have a text but one word is missing. Help us fill in the missing word."}
{"completion": "mo-di-bear-guitar", "text": "document: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style."}
{"completion": "mo-di-bear-guitar", "text": "query: A film studio wants to create a trailer for their upcoming animated film \"The Enchanted Forest\". They need a scene where the main character, a brave squirrel, is running away from a fearsome dragon."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "document: A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "query: We own a small restaurant and we would like a menu recommendation system for our customers based on the orders they make."}
{"completion": "swin2SR-classical-sr-x4-64", "text": "document: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution."}
{"completion": "swin2SR-classical-sr-x4-64", "text": "query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task."}
{"completion": "rasa/LaBSE", "text": "document: LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages."}
{"completion": "rasa/LaBSE", "text": "query: Our application needs to work with multiple langauges. In order to do this, we want to find the sentence embeddings of user input in different languages."}
{"completion": "typeform/distilbert-base-uncased-mnli", "text": "document: This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task."}
{"completion": "typeform/distilbert-base-uncased-mnli", "text": "query: general knowledge, science, and sports."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on Portuguese using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "query: Help me develop a Portuguese audio transcription service."}
{"completion": "microsoft/beit-base-patch16-224", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224."}
{"completion": "microsoft/beit-base-patch16-224", "text": "query: We want to automatically tag items in our e-commerce website with corresponding category labels."}
{"completion": "blip-image-captioning-base", "text": "document: BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone)."}
{"completion": "blip-image-captioning-base", "text": "query: We are a travel agency, and we want to automatically generate captions for images to improve our website."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "document: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "query: To ensure quality, we have to detect the defects of printed circuit boards (PCBs), visually."}
{"completion": "chavinlo/TempoFunk", "text": "document: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}
{"completion": "chavinlo/TempoFunk", "text": "query: Generate a video about a day in the life of a software engineer for our conference."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "query: \"The dog is playing fetch\" and \"The canine is retrieving the ball\"."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: We are building an app that suggests possible captions for photos uploaded by users."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "document: This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral."}
{"completion": "cointegrated/rubert-base-cased-nli-threeway", "text": "query: We are a team building a discussion forum. We need to know if user comments contradicts each other or not."}
{"completion": "openai/whisper-small", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages."}
{"completion": "openai/whisper-small", "text": "query: I have an audio file of my customer's call. I want to transcribe and translate it to text."}
{"completion": "sshleifer/tiny-marian-en-de", "text": "document: A tiny English to German translation model using the Marian framework in Hugging Face Transformers."}
{"completion": "sshleifer/tiny-marian-en-de", "text": "query: Describe how to create a program that translates an English email message into German."}
{"completion": "princeton-nlp/unsup-simcse-roberta-base", "text": "document: An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture."}
{"completion": "princeton-nlp/unsup-simcse-roberta-base", "text": "query: We have a list of documents and are looking for a way to find the similarity between any two documents."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "query: We are a video production company that wants to use AI to generate video clips from text descriptions."}
{"completion": "optimum/roberta-base-squad2", "text": "document: This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library."}
{"completion": "optimum/roberta-base-squad2", "text": "query: Predict the answer to a question based on the given text."}
{"completion": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", "text": "query: Create a system to extract and compare similarity between sentences to find the most relevant ones to a query."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "document: roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "query: Extract the named entities like people, organizations, and locations mentioned in a news article about a recent event."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "document: BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "query: We want to have a command-line tool to summarize very long scientific research papers."}
{"completion": "Salesforce/codet5-base", "text": "document: CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection."}
{"completion": "Salesforce/codet5-base", "text": "query: We want to develop a program to analyze a single code snippet and automatically generate a human-readable summary explaining the purpose of the provided code."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "document: A model that classifies images as hotdog or not hotdog."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "query: A fast food restaurant wants to keep track of the number of hotdogs in their images of food items. They want to automate this by classifying images as hotdog or not hotdog."}
{"completion": "blip2-flan-t5-xl", "text": "document: BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xl", "text": "query: How many people are in the given picture?"}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645."}
{"completion": "sayakpaul/videomae-base-finetuned-ucf101-subset", "text": "query: A company is developing a smart monitoring camera for client security. It requires the camera model to classify video feeds into different activities."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "document: This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "query: Our client is a simulation company. They want a neural network model that is trained on MNIST dataset with a specified data cap."}
{"completion": "stabilityai/stable-diffusion-2-depth", "text": "document: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only."}
{"completion": "stabilityai/stable-diffusion-2-depth", "text": "query: We are developing an image generation tool for customer needs where we need to generate an image based on customer's text description."}
{"completion": "google/tapas-base-finetuned-wtq", "text": "document: TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table."}
{"completion": "google/tapas-base-finetuned-wtq", "text": "query: We need a tool that can answer questions related to data in tables. Use Google's TAPAS algorithm to enable that capability."}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "document: Barthez model finetuned on orangeSum for abstract generation in French language"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "query: Help our French news agency in summarizing an article to provide a quick overview of the topic in French."}
{"completion": "temp_vilt_vqa", "text": "document: A visual question answering model for answering questions related to images using the Hugging Face Transformers library."}
{"completion": "temp_vilt_vqa", "text": "query: My child just asked me about the name of building in an image. I don't know the answer, can you help me?"}
{"completion": "lvwerra/distilbert-imdb", "text": "document: This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set."}
{"completion": "lvwerra/distilbert-imdb", "text": "query: A movie theater is collecting customer feedback and reached out to us to analyze the general sentiment within the responses."}
{"completion": "pyannote/speaker-diarization", "text": "document: This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers."}
{"completion": "pyannote/speaker-diarization", "text": "query: We are a startup developing an online meeting platform. We need to identify when different people speak during a meeting."}
{"completion": "google/tapas-small-finetuned-wikisql-supervised", "text": "document: TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table."}
{"completion": "google/tapas-small-finetuned-wikisql-supervised", "text": "query: I am creating a database system for my company's sales team. They need to be able to ask questions about the data and have it answer them, like \"What is the total revenue of products made in 2020?\""}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "document: A tabular classification model for predicting carbon emissions in grams, trained using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "query: A car company is looking for a solution to predict the carbon emissions per distance of their vehicles, based on their specific attributes."}
{"completion": "ppo-seals-CartPole-v0", "text": "document: This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-seals-CartPole-v0", "text": "query: A company wants to solve a game with a deep reinforcement learning model. The model has been trained for CartPole."}
{"completion": "google/pix2struct-textcaps-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks."}
{"completion": "google/pix2struct-textcaps-base", "text": "query: Our customers that use our OCR software are finding the applications helpful. Can we have a summary of the content of the picture without classifying the type of image?"}
{"completion": "neulab/omnitab-large-finetuned-wtq", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab."}
{"completion": "neulab/omnitab-large-finetuned-wtq", "text": "query: You are asked to develop a system to answer questions about sales data provided in a table."}
{"completion": "MCG-NJU/videomae-base-short", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "MCG-NJU/videomae-base-short", "text": "query: I need an application to analyze short videos and classify the activities within them automatically."}
{"completion": "xlnet-base-cased", "text": "document: XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context."}
{"completion": "xlnet-base-cased", "text": "query: Write an engaging opening sentence for a story about a runaway circus elephant."}
{"completion": "google/mobilenet_v2_1.0_224", "text": "document: MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v2_1.0_224", "text": "query: Check if an image contains a specific object by classifying it using a pre-trained model."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "query: I have a table filled with upcoming movies. I need to find which are the highest-grossing film of the year 2023."}
{"completion": "utyug1/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "utyug1/sd-class-butterflies-32", "text": "query: Create an application that generates images of butterflies."}
{"completion": "t5_sentence_paraphraser", "text": "document: A T5 model for paraphrasing sentences"}
{"completion": "t5_sentence_paraphraser", "text": "query: I am the teacher's assistant, and we want to rephrase a given sentence to avoid plagiarism in our materials, keeping the same meaning."}
{"completion": "dreamlike-art/dreamlike-diffusion-1.0", "text": "document: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art."}
{"completion": "dreamlike-art/dreamlike-diffusion-1.0", "text": "query: Generate an image of a futuristic cityscape with flying cars, using vivid colors and a warm but dystopian atmosphere, inspired by a mix of Blade Runner and Akira. The image should have sharp focus and a complex, detailed background."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: We are building a transcription service that transcribes speech into text. The product needs to add punctuation marks automatically in the generated text."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "document: BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "query: In our company, we have discovered numerous scientific papers, and we need to summarize them all to facilitate decision making."}
{"completion": "keremberke/yolov8s-table-extraction", "text": "document: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set."}
{"completion": "keremberke/yolov8s-table-extraction", "text": "query: A publishing company needs a system to detect tables in scanned documents. We need to use a model specifically for table detection."}
{"completion": "bert-base-uncased", "text": "document: BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-uncased", "text": "query: I have written \"My office is located in the [MASK] floor,\" but I have forgotten which floor it is on. Can you please predict what the correct word should be?"}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "document: A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images."}
{"completion": "JosephusCheung/GuanacoVQAOnConsumerHardware", "text": "query: Tim needs some help with determining the number of fruits in his lunch bowl. Can you help him with that?"}
{"completion": "gpt2", "text": "document: GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences."}
{"completion": "gpt2", "text": "query: We are organizing a business meeting next week, and all participants need a short introduction message we can send to them."}
{"completion": "microsoft/trocr-small-stage1", "text": "document: TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."}
{"completion": "microsoft/trocr-small-stage1", "text": "query: We want to extract textual data from an image. The image is a screenshot of a website with data about a seminar."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation."}
{"completion": "glpn-nyu-finetuned-diode-221121-063504", "text": "query: As a real estate developer, I want to estimate the depth of a room using a depth estimation model."}
{"completion": "facebook/bart-large-mnli", "text": "document: This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities."}
{"completion": "facebook/bart-large-mnli", "text": "query: A sports magazine is looking for a system that classifies news articles into different categories like football, basketball, hockey, cricket, etc."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "query: Our company is working on a project to reduce carbon emissions. We want a tool to predict carbon emissions based on some features."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "document: A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results."}
{"completion": "ydshieh/vit-gpt2-coco-en", "text": "query: We want to integrate a feature that automatically generates captions for images on our website."}
{"completion": "kazusam/kt", "text": "document: An ESPnet2 TTS model trained by mio using amadeus recipe in espnet."}
{"completion": "kazusam/kt", "text": "query: A video game company wants to embed an advanced Text-to-Speech tool into their game to generate realistic voices for their characters."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "document: Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "query: \"How many people are in the room?\" and \"Where is the sofa located?\""}
{"completion": "martin-ha/toxic-comment-model", "text": "document: This model is a fine-tuned version of the DistilBERT model to classify toxic comments."}
{"completion": "martin-ha/toxic-comment-model", "text": "query: As a social media platform, we want to identify and flag toxic comments to maintain a healthy online environment."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Arabic. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Arabic using the train and validation splits of Common Voice 6.1 and Arabic Speech Corpus."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "query: Implement a model to transcribe Arabic speech to text."}
{"completion": "glpn-kitti-finetuned-diode", "text": "document: This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset."}
{"completion": "glpn-kitti-finetuned-diode", "text": "query: The urban planning department needs to gather depth data from aerial images. They have high-resolution imagery, and need to process it to obtain depth information for every pixel."}
{"completion": "caidas/swin2SR-classical-sr-x2-64", "text": "document: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository."}
{"completion": "caidas/swin2SR-classical-sr-x2-64", "text": "query: My company needs an advanced image processing tool to improve the resolution of the photos for our website."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "document: An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "query: A social media platform wants to generate image captions to make their platform more accessible. Help them with this task."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions."}
{"completion": "philschmid/distilbert-onnx", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1."}
{"completion": "philschmid/distilbert-onnx", "text": "query: We are designing a system which can answer the question from the given context. The model will use an efficient architecture called DistilBERT."}
{"completion": "neulab/omnitab-large-1024shot-finetuned-wtq-1024shot", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab. neulab/omnitab-large-1024shot-finetuned-wtq-1024shot (based on BART architecture) is initialized with neulab/omnitab-large-1024shot and fine-tuned on WikiTableQuestions in the 1024-shot setting."}
{"completion": "neulab/omnitab-large-1024shot-finetuned-wtq-1024shot", "text": "query: We want to create a chatbot that can answer questions from tables."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "query: We are a multinational company and we need to transcribe a Chinese recording. Please provide a solution for this problem."}
{"completion": "Narsil/deberta-large-mnli-zero-cls", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task."}
{"completion": "Narsil/deberta-large-mnli-zero-cls", "text": "query: I want to design a question and answer based support system for our website. The users can ask questions and receive a correct answer among several possible responses."}
{"completion": "facebook/mask2former-swin-large-cityscapes-semantic", "text": "document: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency."}
{"completion": "facebook/mask2former-swin-large-cityscapes-semantic", "text": "query: An urban planner wants to analyze the composition of a cityscape image to decide on the best course of action for improving urban spaces."}
{"completion": "distilgpt2", "text": "document: DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more."}
{"completion": "distilgpt2", "text": "query: We are a publishing company and we need to create text teasers for a new book we are promoting."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese", "text": "query: We need to transcribe speeches given in Japanese language and store it as text file for furthur analysis. Code needed please!"}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "document: GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs."}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "query: I need help with my yearly personal finance report. I want it to accurately track my revenue and expenses, and report the yearly balance. Additionally, I need this to be done empathically."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "nikcheerla/nooks-amd-detection-realtime", "text": "query: I need to find similar questions from a set of frequently asked questions in my customer support section."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco"}
{"completion": "cross-encoder/ms-marco-MiniLM-L-12-v2", "text": "query: A colleague of mine has collected a dataset containing the answers to many commonly asked questions. I want to sort these answers based on my dataset to provide the suitable answer to specific queries."}
{"completion": "convnext_base.fb_in1k", "text": "document: A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings."}
{"completion": "convnext_base.fb_in1k", "text": "query: We are building a system to classify images on the production line. We want to build a machine learning model to do this task."}
{"completion": "sentiment_analysis_generic_dataset", "text": "document: This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification."}
{"completion": "sentiment_analysis_generic_dataset", "text": "query: \"I bought the headphone last week, and I am absolutely loving the sound quality and battery life!\""}
{"completion": "facebook/nllb-200-distilled-600M", "text": "document: NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation."}
{"completion": "facebook/nllb-200-distilled-600M", "text": "query: Create a function that translates the given input text into English."}
{"completion": "vennify/t5-base-grammar-correction", "text": "document: This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG."}
{"completion": "vennify/t5-base-grammar-correction", "text": "query: We want to improve our English writing. Figure out a way to automatically fix grammatical mistakes."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "query: I am searching for a QA model that is capable of learning from tables. Any help is appreciated."}
{"completion": "MCG-NJU/videomae-base", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches."}
{"completion": "MCG-NJU/videomae-base", "text": "query: Develop a model that can detect and label objects in a video sequence. The objects should be bounding boxes and automatically created object labels."}
{"completion": "Linaqruf/anything-v3.0", "text": "document: A text-to-image model that generates images from text descriptions."}
{"completion": "Linaqruf/anything-v3.0", "text": "query: We are launching an app that creates a souvenir from a novel event. The user will type in a brief description of the experience, and the app will generate an image of the event."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "query: I wish to create a fill-in-the-blank solution for our language learning app, which requires the generation of masked sentences."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "query: We need to generate images of a bedroom using the DDPM model for an interior design software."}
{"completion": "google/vit-base-patch16-384", "text": "document: Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."}
{"completion": "google/vit-base-patch16-384", "text": "query: Can you help me to enhance the quality of my advertisement's image using GPT API?"}
{"completion": "bert-base-uncased", "text": "document: BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-uncased", "text": "query: Develop a suggestive phrase for a dentist billboard using a masked language model."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "document: A tiny TAPAS model for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "query: Our manager wants a summary of a complex dataset. We need to extract insights from the dataset in a question-answering format."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: I have a set of iris flower measurements in a CSV file. I need to classify them using a logistic regression model."}
{"completion": "valhalla/distilbart-mnli-12-6", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-6", "text": "query: I need a system that classifies submitted products into categories such as \"sports\", \"electronics\", \"fashion\", \"automotive\", and \"books\"."}
{"completion": "openai/whisper-large", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large", "text": "query: Develop an automatic speech recognition software for a podcast app to transcribe human speech to text."}
{"completion": "sentence-transformers/all-MiniLM-L12-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L12-v2", "text": "query: Our company is working on a customer support ticket classification system, and we need to identify which tickets are similar."}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "document: GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs."}
{"completion": "microsoft/GODEL-v1_1-large-seq2seq", "text": "query: We have created a chatbot which is supposed to respond to customer queries. Help it to respond empathically."}
{"completion": "microsoft/trocr-large-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-handwritten", "text": "query: We are a calligraphy archive and want to transcribe handwritten text from the images of ancient scripts."}
{"completion": "facebook/opt-66b", "text": "document: OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation."}
{"completion": "facebook/opt-66b", "text": "query: Create a chatbot that can provide users with business advice and resources on how to start a company."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "document: A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "query: We are designing a website that displays pictures of various products. Upon image upload, we need to identify the top 5 product categories."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "query: I bought a robot dog, and I expect the robot to understand the walking actions of the robot dog in the environment."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "query: We are a Chinese research team focusing on analyzing the visual features and zero-shot classification of Chinese food. "}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "document: BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "query: As a legal firm, we deal with many long and complex documents. We'd like to create summaries of these documents to save time for our staff."}
{"completion": "facebook/convnext-base-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification."}
{"completion": "facebook/convnext-base-224", "text": "query: Analyze the image to identify the primary object and its category."}
{"completion": "FSMN-VAD", "text": "document: FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library."}
{"completion": "FSMN-VAD", "text": "query: We are building a phone call operator. Help us implement an audio processing component that will be able to separate speech from silence."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "document: Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent."}
{"completion": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS", "text": "query: I'm building a voice assistant that needs to read out text in a Taiwanese Hokkien accent. How can I use the Text-to-Speech model to convert text to speech in this accent?"}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "document: This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China."}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "query: Create a system that can answer questions based on an image for an app that helps visually impaired users."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics."}
{"completion": "glpn-nyu-finetuned-diode-221122-014502", "text": "query: We are planning to build an indoor navigation system for visually-impaired people. We need our app to provide depth estimation."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "document: roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "query: Detect the named entities in a given piece of text."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "query: I'm creating an AI model to monitor the river near the bridge and classify whether the condition is \"normal\", \"overflowing\", or \"polluted\"."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "document: A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "query: We are an e-commerce platform. We need to classify the products that users are uploading."}
{"completion": "deepset/minilm-uncased-squad2", "text": "document: MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library."}
{"completion": "deepset/minilm-uncased-squad2", "text": "query: We got a lot of customer service inquiries this week. Let's analyze the content and generate automatic replies to some common questions."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "query: I'm building a news aggregator app, and I need a system to categorize news articles into topics like 'politics', 'sports', 'entertainment', 'technology', 'economy', 'health', and 'world'."}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "document: CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER"}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "query: A healthcare startup needs to extract features from medical terms for their app's search functionality. They asked for your help."}
{"completion": "DialoGPT-medium-PALPATINE2", "text": "document: A DialoGPT model trained for generating human-like conversational responses."}
{"completion": "DialoGPT-medium-PALPATINE2", "text": "query: Create a program to have a conversation with a chatbot that can generate responses like a human."}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: I want to create a Spanish text-to-speech system that converts text inputs into spoken words to produce a natural-sounding male voice."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "query: The marketing team needs to promote a product by creating a banner with a clear background. We need to separate the product from the background."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "document: This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "query: As a public relations company, we need to help our clients with voiceovers for their ads. Please generate a voiceover from a text script."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "query: There is a new traffic management technology being developed, and we need to provide an AI module that can help classify images from the traffic cameras."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "query: I own a company focusing on green building infrastructures. I need to segment images to identify the environment and landscape components, including buildings and vegetation."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023", "text": "document: A LayoutLMv2 model for document question answering."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023", "text": "query: The company is working on a new product that summarizes legal documents. To save development time, they would like to use a pre-trained model that helps answer questions about a document."}
{"completion": "openai/clip-vit-base-patch32", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-base-patch32", "text": "query: We need to recognize whether the uploaded image refers to a cat or a dog."}
{"completion": "wavymulder/Analog-Diffusion", "text": "document: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}
{"completion": "wavymulder/Analog-Diffusion", "text": "query: We need to create analog-style images for our online store displaying various types of clothing."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "document: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required."}
{"completion": "keremberke/yolov8m-hard-hat-detection", "text": "query: Our company is on a mission to build a better AI-powered workplace safety system. We are now using AI technology to track whether employees wear their hard hats properly at the construction site."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "document: A text-to-speech model trained on mtedx, covost2, europarl_st, and voxpopuli datasets for English, French, Spanish, and Italian languages. Licensed under cc-by-nc-4.0."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "query: You are participating in a group conversation and you want to check the quality of the voiceover given the text input. The conversation was in English."}
{"completion": "google/owlvit-large-patch14", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages."}
{"completion": "google/owlvit-large-patch14", "text": "query: A researcher wants to use a zero-shot object detection model. Explain how it can be used to analyze an image of a city with cars and people."}
{"completion": "google/tapas-small-finetuned-wtq", "text": "document: TAPAS small model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-small-finetuned-wtq", "text": "query: I want to analyze a table with housing statistics about different cities, find out which city has the highest average income."}
{"completion": "openai/whisper-large", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large", "text": "query: A startup is building an innovative voice user interface for their mobile application that can transcribe spoken words into written text."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "query: I have a list of tasks that need to be grouped based on similarity. Help me identify the similar tasks."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "document: This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese."}
{"completion": "papluca/xlm-roberta-base-language-detection", "text": "query: We have a multilingual community page, where people post in different languages. Prepare a language identifier to categorize the language of the post."}
{"completion": "bert-base-cased", "text": "document: BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task."}
{"completion": "bert-base-cased", "text": "query: \"I have always wanted to [MASK] a marathon.\""}
{"completion": "oliverguhr/german-sentiment-bert", "text": "document: This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews."}
{"completion": "oliverguhr/german-sentiment-bert", "text": "query: Our e-commerce platform receives a large amount of German customer feedback. We need to determine their sentiments for improvement."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "query: We are an insurance company that processes a lot of documents daily. Process the documents to respond to customer queries."}
{"completion": "t5_sentence_paraphraser", "text": "document: A T5 model for paraphrasing sentences"}
{"completion": "t5_sentence_paraphraser", "text": "query: We are an AI enterprise, and we've designed a chatbot for customer support in the aviation industry. Please help us paraphrase the conversation between a passenger and the chatbot."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "query: Use a generative model to create images of people's faces for a marketing campaign. The images should be 1024x1024 pixels in size."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "query: We have just moved into our new office. It is a multi-use space. We need to make floor plans for each area by identifying furniture and utilities."}
{"completion": "yiyanghkust/finbert-tone", "text": "document: FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task."}
{"completion": "yiyanghkust/finbert-tone", "text": "query: Our client is an investment company. They want to analyze news articles to make better decisions. Help us classify financial texts."}
{"completion": "deepset/roberta-large-squad2", "text": "document: A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context."}
{"completion": "deepset/roberta-large-squad2", "text": "query: Design a function that summarizes the key aspects of a text and provides answers to questions about the text."}
{"completion": "rajistics/california_housing", "text": "document: A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices."}
{"completion": "rajistics/california_housing", "text": "query: We need to predict housing prices for a real estate company in California. Develop an accurate model for this purpose."}
{"completion": "wav2vec2-xlsr-53-russian-emotion-recognition", "text": "document: A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness."}
{"completion": "wav2vec2-xlsr-53-russian-emotion-recognition", "text": "query: I'm working on a call center project. We need to analyze the emotions of our Russian-speaking customers."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: A person is learning a new language and wants to know the phonetic transcription of words. Help him with this process."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "document: A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k."}
{"completion": "convnextv2_huge.fcmae_ft_in1k", "text": "query: Find the possible classes and their probabilities for the given image URL."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "document: This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "query: I am creating an art exhibit and would like to generate an original painting inspired by the WikiArt dataset."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-ffhq-1024", "text": "query: As content creators, we are planning to generate new 1024x1024 images for our upcoming project."}
{"completion": "lang-id-voxlingua107-ecapa", "text": "document: This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. It covers 107 different languages."}
{"completion": "lang-id-voxlingua107-ecapa", "text": "query: As the head of the research team, I need to classify the spoken language within a variety of audio clips collected from around the world."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "document: A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT."}
{"completion": "madhurjindal/autonlp-Gibberish-Detector-492513457", "text": "query: Implement a program to detect if the given text input is gibberish or not."}
{"completion": "michellejieli/emotion_text_classifier", "text": "document: DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise."}
{"completion": "michellejieli/emotion_text_classifier", "text": "query: We are a production company working on a TV show, and we want to identify the emotions expressed by the characters in our show's dialogues."}
{"completion": "google/flan-t5-base", "text": "document: FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering."}
{"completion": "google/flan-t5-base", "text": "query: Write a python function that translates a given English text to German."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: Our company wants to help users transcribe multilingual speech using wav2vec2 model, which supports multiple languages for Automatic Speech Recognition."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "document: This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "query: I want to detect the emotion in a given text. It should be able to classify emotions like happy, sad, angry, etc."}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: Our customer is developing an educational app for learning Spanish. They need to create example audio content from given Spanish words and phrases."}
{"completion": "deepset/minilm-uncased-squad2", "text": "document: MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library."}
{"completion": "deepset/minilm-uncased-squad2", "text": "query: Develop a solution to answer questions related to a specific input context."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "query: We are a clothing retailer who wants to create images of garments with certain patterns or designs based on text input. Provide a solution for this problem."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "document: This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "query: An app needs to extract named entities, such as people, organizations, and locations, from sentences."}
{"completion": "chavinlo/TempoFunk", "text": "document: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}
{"completion": "chavinlo/TempoFunk", "text": "query: Can you tell me more about the text-to-video in the Hugging Face? We are working to run a use case on generating a video description of what happened in the provided text. "}
{"completion": "facebook/m2m100_1.2B", "text": "document: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token."}
{"completion": "facebook/m2m100_1.2B", "text": "query: It's  the middle of the night and I have insomnia. I need my text translated from English to Russian to send some information to my collagues in Russia."}
{"completion": "table-question-answering-tapas", "text": "document: TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts."}
{"completion": "table-question-answering-tapas", "text": "query: I have a list of DVD features and prices. I need a model to be able to answer questions like \"What is the price of the DVD with the highest resolution?\" based on the provided table."}
{"completion": "prithivida/parrot_adequacy_model", "text": "document: Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser."}
{"completion": "prithivida/parrot_adequacy_model", "text": "query: I need to improve a chatbot that I'm developing, it needs to generate paraphrases for a given text using Parrot."}
{"completion": "lllyasviel/control_v11p_sd15_lineart", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images."}
{"completion": "lllyasviel/control_v11p_sd15_lineart", "text": "query: Our company is developing a browser extension that converts text-to-hand-drawn images. We need the model that is ideal for this."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "document: Multilingual XLM-RoBERTa large model for extractive question answering on various languages. Trained on SQuAD 2.0 dataset and evaluated on SQuAD dev set, German MLQA, and German XQuAD."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "query: \"Gazpacho is a Spanish cold tomato soup that offers numerous health benefits. It contains vitamins, minerals and antioxidants that help regulate digestion, prevent inflammation, and support cardiovascular health.\""}
{"completion": "distilbert-base-cased-distilled-squad", "text": "document: DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "query: Create a feature for a tourism website that answers questions from visitors about popular places and attractions."}
{"completion": "camenduru/text2-video-zero", "text": "document: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more."}
{"completion": "camenduru/text2-video-zero", "text": "query: I need a description of a video for an English teaching video with the theme of \u201cgreetings and introductions\u201d for beginners."}
{"completion": "speechbrain/sepformer-wham16k-enhancement", "text": "document: This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k."}
{"completion": "speechbrain/sepformer-wham16k-enhancement", "text": "query: I have a noisy audio recording, and I want to enhance it using this pretrained model."}
{"completion": "cross-encoder/ms-marco-TinyBERT-L-2-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco."}
{"completion": "cross-encoder/ms-marco-TinyBERT-L-2-v2", "text": "query: To improve our customer support quality, we are building an automated FAQ answering system. We need to find the most relevant answer for a query from our knowledge base."}
{"completion": "facebook/m2m100_418M", "text": "document: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token."}
{"completion": "facebook/m2m100_418M", "text": "query: I want to translate a document from Russian to Spanish. The document is too technical, that's why I a choose a powerful tool like yours."}
{"completion": "sentence-transformers/distiluse-base-multilingual-cased-v1", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/distiluse-base-multilingual-cased-v1", "text": "query: I want to build a trip assistant who can gather the information, translate it, and find the compatibility among English, Russian, and Chinese."}
{"completion": "bigscience/bloom-7b1", "text": "document: BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text."}
{"completion": "bigscience/bloom-7b1", "text": "query: Our online gaming platform needs a way to generate catchy and engaging descriptions for new video games. We want to give it the name or the setting to create a captivating paragraph."}
{"completion": "keremberke/yolov8s-csgo-player-detection", "text": "document: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead']."}
{"completion": "keremberke/yolov8s-csgo-player-detection", "text": "query: Global Offensive (CS:GO) based on images."}
{"completion": "kredor/punctuate-all", "text": "document: A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian."}
{"completion": "kredor/punctuate-all", "text": "query: The customer needs help for a multilingual chat app to punctuate the content for better understanding."}
{"completion": "GreeneryScenery/SheepsControlV3", "text": "document: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data."}
{"completion": "GreeneryScenery/SheepsControlV3", "text": "query: We are working on an art project and would like to generate an image that combines elements from an input image with some text guidance."}
{"completion": "facebook/maskformer-swin-base-coco", "text": "document: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository."}
{"completion": "facebook/maskformer-swin-base-coco", "text": "query: Help me make an automatic cropping tool for my app. I need the app to detect the location of products in food images and make a box around them for cropping."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "document: A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "query: Help me summarize an important news article to get the main information quickly."}
{"completion": "stabilityai/stable-diffusion-2-inpainting", "text": "document: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts."}
{"completion": "stabilityai/stable-diffusion-2-inpainting", "text": "query: As a photographer, I want to create a unique digital artwork. I need a high-resolution image of a red dragon breathing fire in a dark cave."}
{"completion": "pcoloc/autotrain-only-rssi-1813762559", "text": "document: A tabular regression model trained using AutoTrain for estimating carbon emissions from given features."}
{"completion": "pcoloc/autotrain-only-rssi-1813762559", "text": "query: Our company needs to calculate the carbon emissions of our production processes. We have a dataset containing related features."}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "document: Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class"}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "query: We want to create an artwork project with a vintage style look. Let's generate a vintage-style image to use as inspiration."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "document: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images."}
{"completion": "gsdf/Counterfeit-V2.5", "text": "query: I am building an app, I want to create animal character that generates images based on text prompts."}
{"completion": "wav2vec2-base-superb-sv", "text": "document: This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "wav2vec2-base-superb-sv", "text": "query: A friend of mine wants to identify a speaker using a recorded 16kHz speech sample."}
{"completion": "keras-io/timeseries-anomaly-detection", "text": "document: This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics."}
{"completion": "keras-io/timeseries-anomaly-detection", "text": "query: Let's say we have to work on monitoring HVAC units in a commercial building. Identification of the anomalous behaviour of the system is of utmost importance."}
{"completion": "redshift-man-skiing", "text": "document: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'."}
{"completion": "redshift-man-skiing", "text": "query: Create a video for our social media advertisement campaign. The video should show a happy family enjoying a beach vacation."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "query: A new government regulation requires predicting the carbon emissions of electronic devices. Use the provided model to determine the emissions for each device data in a CSV file."}
{"completion": "pyannote/segmentation", "text": "document: A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework."}
{"completion": "pyannote/segmentation", "text": "query: I am writing an audio processing application that will automatically analyze audio recordings of telephone conversations. I want to separate the voice tracks of each speaker and identify if they are talking simultaneously. Please provide a code example of how I can achieve this."}
{"completion": "videomae-base-finetuned-ucf101", "text": "document: VideoMAE Base model fine tuned on UCF101 for Video Action Recognition"}
{"completion": "videomae-base-finetuned-ucf101", "text": "query: Create a media player application that can recognize the actions performed by people in the video, tag them, and display the results to the user."}
{"completion": "OFA-Sys/chinese-clip-vit-large-patch14-336px", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-large-patch14-336px", "text": "query: We want to create an app that translates images and their text properties to help people with impaired vision."}
{"completion": "lanwuwei/BERTOverflow_stackoverflow_github", "text": "document: BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow."}
{"completion": "lanwuwei/BERTOverflow_stackoverflow_github", "text": "query: I am developing a large-scale question and answer site for developers. I want to extract named entities and code snippets from the corpus in order to build an organized database."}
{"completion": "microsoft/swinv2-tiny-patch4-window8-256", "text": "document: Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images."}
{"completion": "microsoft/swinv2-tiny-patch4-window8-256", "text": "query: As a mobile app developer we need to classify the given picture in order to sort it into the appropriate category."}
{"completion": "sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "sd-class-butterflies-32", "text": "query: We are developing a creative app to generate images of butterflies. Provide a code snippet for that."}
{"completion": "google/tapas-small-finetuned-wikisql-supervised", "text": "document: TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table."}
{"completion": "google/tapas-small-finetuned-wikisql-supervised", "text": "query: Our company is providing car rental services. We have a table representing the car selection available and the rental price. We need to analyze renters' questions and provide answers to their queries about our cars."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "document: DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering."}
{"completion": "distilbert-base-cased-distilled-squad", "text": "query: I am a student, and I would like to create a Q&A bot to help me. It should be capable of answering queries from my classmates."}
{"completion": "financial-summarization-pegasus", "text": "document: This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization."}
{"completion": "financial-summarization-pegasus", "text": "query: I need a report to be summarized for a meeting. Perform summarization of the text provided."}
{"completion": "lllyasviel/control_v11p_sd15_inpaint", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images."}
{"completion": "lllyasviel/control_v11p_sd15_inpaint", "text": "query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas."}
{"completion": "Zixtrauce/JohnBot", "text": "document: JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface."}
{"completion": "Zixtrauce/JohnBot", "text": "query: We are designing an online surfing forum. We need a chatbot to help guid the new users."}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "document: bert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER)."}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "query: I run a news company that delivers news from around the world. Create a language model that can identify Named Entities (such as Person, Organization, and Location) in my articles."}
{"completion": "pygmalion-6b", "text": "document: Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue."}
{"completion": "pygmalion-6b", "text": "query: Develop a high-quality dialog for a conversational AI built for a video game based on a given character persona."}
{"completion": "nikcheerla/nooks-amd-detection-v2-full", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search."}
{"completion": "nikcheerla/nooks-amd-detection-v2-full", "text": "query: We need to identify similar news headlines in order to personalize our website according to the interests of readers."}
{"completion": "tiny-random-CLIPSegModel", "text": "document: A tiny random CLIPSegModel for zero-shot image classification."}
{"completion": "tiny-random-CLIPSegModel", "text": "query: I want to build an app that recognizes the types of food by just uploading an image, classify the food type from the image of a dish."}
{"completion": "microsoft/wavlm-large", "text": "document: WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}
{"completion": "microsoft/wavlm-large", "text": "query: We are analysing phone conversations. We need to transcribe the speech into text."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "query: Our company wants to provide a service that transcribes audio from Russian-speaking customers to text, to improve our communication and support."}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "document: MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!"}
{"completion": "Kirili4ik/mbart_ruDialogSum", "text": "query: Help me summarize Russian dialogues with a model."}
{"completion": "Salesforce/codet5-base", "text": "document: CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection."}
{"completion": "Salesforce/codet5-base", "text": "query: Our company wants to provide code snippets to programmers for commonly used tasks. Create a system that can generate code based on text inputs."}
{"completion": "google/ncsnpp-ffhq-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-ffhq-256", "text": "query: We are designing an app and need to generate a unique avatar for each of our users."}
{"completion": "fcakyon/timesformer-large-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al."}
{"completion": "fcakyon/timesformer-large-finetuned-k400", "text": "query: We are working on a platform to automatically categorize sports videos to be shared with users according to their preferences."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "document: A tabular classification model for predicting carbon emissions in grams, trained using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "query: Find a way to estimate the carbon emissions from the data provided by an automotive company."}
{"completion": "xm_transformer_unity_hk-en", "text": "document: A speech-to-speech translation model with two-pass decoder (UnitY) trained on Hokkien-English data from TED, drama, and TAT domains. It uses Facebook's Unit HiFiGAN for speech synthesis."}
{"completion": "xm_transformer_unity_hk-en", "text": "query: We need to translate a Hokkien audio speech into English and generate an English audio speech."}
{"completion": "Zixtrauce/JohnBot", "text": "document: JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface."}
{"completion": "Zixtrauce/JohnBot", "text": "query: A person wants quick but meaningful responses to their questions in everyday conversation."}
{"completion": "nitrosocke/nitro-diffusion", "text": "document: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use."}
{"completion": "nitrosocke/nitro-diffusion", "text": "query: We want to create a virtual world based on text-to-image generation. Generate an image of a mountain range with snowfall at sunset."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: Our team is exploring plant species in the ecosystem. Identify the species of a flower with its attributes."}
{"completion": "deepset/minilm-uncased-squad2", "text": "document: MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library."}
{"completion": "deepset/minilm-uncased-squad2", "text": "query: A professor is conducting a study on the importance of model conversions in the world of AI. Help them understand why model conversion is important."}
{"completion": "google/tapas-small-finetuned-sqa", "text": "document: TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-small-finetuned-sqa", "text": "query: I have a list of sales data with information about products, prices, and quantities sold. I need assistance to answer questions about this data."}
{"completion": "microsoft/tapex-base-finetuned-wikisql", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wikisql", "text": "query: We are a sports company and need statistics about past Olympic Games. Find in which year Beijing hosted the Olympic Games."}
{"completion": "openai/clip-vit-large-patch14", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-large-patch14", "text": "query: I want a system for tagging my Instagram posts with meaningful tags so they attract the right audience."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "query: I need a tool to predict coloring on scribbled images based on the given texts. Help me to create a model for that."}
{"completion": "osanseviero/wine-quality", "text": "document: A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya."}
{"completion": "osanseviero/wine-quality", "text": "query: We are a startup that sells wine. We're looking to predict wine quality based on its characteristics."}
{"completion": "microsoft/DialoGPT-large", "text": "document: DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads."}
{"completion": "microsoft/DialoGPT-large", "text": "query: We are running a chatbot for a bank as part of their customer service team. It needs to answer client questions."}
{"completion": "google/ncsnpp-ffhq-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-ffhq-256", "text": "query: We are contacted by an animation company to help them generate realistic facial images as reference for their character designs."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "query: I need to create an AI model to classify images with Chinese texts and descriptions written in Chinese."}
{"completion": "Eklavya/ZFF_VAD", "text": "document: A Voice Activity Detection model by Eklavya, using the Hugging Face framework."}
{"completion": "Eklavya/ZFF_VAD", "text": "query: We need to implement a feature to know when someone is speaking or not in a video call."}
{"completion": "speechbrain/mtl-mimic-voicebank", "text": "document: This repository provides all the necessary tools to perform enhancement and\nrobust ASR training (EN) within\nSpeechBrain. For a better experience we encourage you to learn more about\nSpeechBrain. The model performance is:\nRelease\nTest PESQ\nTest COVL\nValid WER\nTest WER\n22-06-21\n3.05\n3.74\n2.89\n2.80\nWorks with SpeechBrain v0.5.12"}
{"completion": "speechbrain/mtl-mimic-voicebank", "text": "query: Our customer service team is having difficulties understanding callers in noisy environments. We need a tool to enhance the voice quality."}
{"completion": "google/pegasus-cnn_dailymail", "text": "document: PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset."}
{"completion": "google/pegasus-cnn_dailymail", "text": "query: We are a news agency that needs to provide a short summary of each article we post. Please help us build this system."}
{"completion": "mattmdjaga/segformer_b2_clothes", "text": "document: SegFormer model fine-tuned on ATR dataset for clothes segmentation."}
{"completion": "mattmdjaga/segformer_b2_clothes", "text": "query: In a fashion retail company, we want to create an app that automatically segments clothes from an image for designing virtual fitting rooms."}
{"completion": "microsoft/git-base-vqav2", "text": "document: GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}
{"completion": "microsoft/git-base-vqav2", "text": "query: Develop a search engine using visual question answering to make search results more specific."}
{"completion": "bert-base-uncased", "text": "document: BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "bert-base-uncased", "text": "query: Let's build a helper function to autofill incomplete sentences."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "document: This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "query: Find out why model conversion is important for a user."}
{"completion": "xlnet-base-cased", "text": "document: XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context."}
{"completion": "xlnet-base-cased", "text": "query: I need some help in writing part of the annual report for my company. The theme is sustainability and our focus on renewable energy."}
{"completion": "Raiden-1001/poca-Soccerv7.1", "text": "document: A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "Raiden-1001/poca-Soccerv7.1", "text": "query: We are creating a game and want to add AI soccer players that can play against human opponents. Use the pretrained agents to add this functionality."}
{"completion": "Helsinki-NLP/opus-mt-ru-en", "text": "document: A Russian to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Transformer-align architecture and trained on the OPUS dataset. The model can be used for translation and text-to-text generation tasks."}
{"completion": "Helsinki-NLP/opus-mt-ru-en", "text": "query: We are a publication house that needs content translated from Russian into English."}
{"completion": "microsoft/trocr-small-stage1", "text": "document: TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."}
{"completion": "microsoft/trocr-small-stage1", "text": "query: We are receiving scanned handwritten documents and need to extract the text from them for further processing."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "document: A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews."}
{"completion": "desertdev/autotrain-imdb-sentiment-analysis-44994113085", "text": "query: We want to design a movie review platform that automatically classifies movie reviews as positive or negative based on their content."}
{"completion": "araffin/dqn-LunarLander-v2", "text": "document: This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library."}
{"completion": "araffin/dqn-LunarLander-v2", "text": "query: In order to add a lunar lander game analyzer in my gaming app, I want to evaluate the gaming scores achieved by the DQN agent over 20 episodes."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "document: RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1]."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "query: We have a large amount of text data in Russian and we need to get a numerical representation of it to use in Machine Learning algorithms."}
{"completion": "facebook/maskformer-swin-base-ade", "text": "document: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation."}
{"completion": "facebook/maskformer-swin-base-ade", "text": "query: Develop a solution for our app where users can detect objects in their photos."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "query: I need to find the similarity between different semantically similar statements in a dataset."}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: Our company is implementing a virtual assistant that needs to read text in a Spanish male voice for our visually impaired users."}
{"completion": "clip-vit-base-patch32-ko", "text": "document: Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data."}
{"completion": "clip-vit-base-patch32-ko", "text": "query: We have built a mobile application for users to share photos of their plants. Help the users determine the plant genus from a URL they provide."}
{"completion": "microsoft/swin-tiny-patch4-window7-224", "text": "document: Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks."}
{"completion": "microsoft/swin-tiny-patch4-window7-224", "text": "query: Find out what object is present in a given image URL."}
{"completion": "mo-di-bear-guitar", "text": "document: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style."}
{"completion": "mo-di-bear-guitar", "text": "query: Our company offers custom video greeting cards. We would like to create a video based on the given text \"A joyful dancing penguin with a birthday cake.\""}
{"completion": "git-large-r-textcaps", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-r-textcaps", "text": "query: Our customer is a gardening company requesting a summary of an image of a plant's health."}
{"completion": "chinese-clip-vit-large-patch14", "text": "document: Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks."}
{"completion": "chinese-clip-vit-large-patch14", "text": "query: Develop a way for our art gallery to sort images based on certain descriptions such as \"landscape painting\" or \"portrait of a woman\"."}
{"completion": "satvikag/chatbot", "text": "document: DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You."}
{"completion": "satvikag/chatbot", "text": "query: Create a small chatbot that takes the user's input, analyzes the data, and produces an output based on a casual conversation with a game character."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "document: Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."}
{"completion": "StanfordAIMI/stanford-deidentifier-base", "text": "query: We are a health organization who has a lot of documents like doctors online reports and their patient list. We need to de-identify these personal information."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "query: An online education platform wants to categorize their math video tutorials on Number Theory and Algebra. The model will help them to do so based on video content."}
{"completion": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli", "text": "document: This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper."}
{"completion": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli", "text": "query: I need to analyze articles for a newspaper publication. Help me classify them into Politics, Economy, Entertainment, and Environment categories."}
{"completion": "deepset/tinyroberta-squad2", "text": "document: This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model."}
{"completion": "deepset/tinyroberta-squad2", "text": "query: I have a set of texts from user interviews, and I am trying to extract answers to some of the common questions that are being asked in these interviews."}
{"completion": "pyannote/overlapped-speech-detection", "text": "document: Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file."}
{"completion": "pyannote/overlapped-speech-detection", "text": "query: The team is trying to find out the points in a conference call where multiple people are speaking at the same time."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "document: A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "query: Create a software function to translate a Dutch review of the restaurant to examine customer feedback."}
{"completion": "BaptisteDoyen/camembert-base-xnli", "text": "document: Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French."}
{"completion": "BaptisteDoyen/camembert-base-xnli", "text": "query: Determine the predominant theme one can use for journal topics when they move to France."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "document: A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "query: The board members have requested the quarterly review to be translated into Chinese."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "document: Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "query: We are working on a device that requires keyword recognition. Given a spoken word or command, please detect the spoken keyword."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "document: A tiny random model for Visual Question Answering using the VILT framework."}
{"completion": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "text": "query: We have developed an app that shows memes. We want to add a feature that will answer questions related to the content of the meme for our users."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "document: Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset."}
{"completion": "Helsinki-NLP/opus-mt-es-en", "text": "query: The company recently received a batch of documents in Spanish. Translate these documents into English to facilitate easier understanding and processing."}
{"completion": "ingen51/DialoGPT-medium-GPT4", "text": "document: A GPT-4 model for generating conversational responses in a dialogue setting."}
{"completion": "ingen51/DialoGPT-medium-GPT4", "text": "query: I am a content creator, and I want to create a chatbot that will communicate with users on my behalf."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "document: This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "query: A company would like to train a speech recognition model with numbers. Recommend a suitable pre-trained model for digit recognition and provide instructions on how to set up the model."}
{"completion": "facebook/maskformer-swin-base-coco", "text": "document: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository."}
{"completion": "facebook/maskformer-swin-base-coco", "text": "query: Working as a traffic police officer, I need to identify different vehicle types on roads using image segmentation."}
{"completion": "abhishek/autotrain-dog-vs-food", "text": "document: A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework."}
{"completion": "abhishek/autotrain-dog-vs-food", "text": "query: I took a photo and want to know if it contains a dog or food."}
{"completion": "glpn-kitti", "text": "document: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-kitti", "text": "query: We are trying to estimate depth from an RGB image captured by a vehicle-mounted camera to evaluate object distances for autonomous driving applications."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "document: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy."}
{"completion": "keremberke/yolov8s-building-segmentation", "text": "query: Our goal is to analyze satellite images by identifying and segmenting the buildings."}
{"completion": "facebook/timesformer-base-finetuned-k400", "text": "document: TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels."}
{"completion": "facebook/timesformer-base-finetuned-k400", "text": "query: I have a collection of physical activities videos, and I need to identify the activity from each video."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: Our product works on matching user-generated content like social media. How can we compare the similarity of the sentences generated by different users?"}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: We are given an audio file of a political speech, and we aim to visualize it as a spectrogram to analyze the frequency patterns during speech."}
{"completion": "distilroberta-base", "text": "document: DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "distilroberta-base", "text": "query: \"Science is constantly <mask> new discoveries and challenging our understanding of the universe.\""}
{"completion": "glpn-nyu", "text": "document: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-nyu", "text": "query: I need to calculate the depth of an image from the given URL to detect distances in the scene."}
{"completion": "valhalla/distilbart-mnli-12-3", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop."}
{"completion": "valhalla/distilbart-mnli-12-3", "text": "query: Our company is in charge of organizing events. We would like to build an AI model that could detect if an email is about an event proposal, a client's feedback, or a request for an appointment."}
{"completion": "openai/clip-vit-base-patch16", "text": "document: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."}
{"completion": "openai/clip-vit-base-patch16", "text": "query: Our company is processing thousands of images daily, and we want to be able to classify new objects or scenes in the images without needing to retrain the model from scratch."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761512", "text": "document: A model trained using AutoTrain for predicting US housing prices with single column regression. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a CO2 Emissions of 50.5369 grams."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761512", "text": "query: Real estate companies need a tool to predict the prices of US housing. Develop a simple API for them."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-albert-small-v2", "text": "query: Offer me a way to compare the similarity of two sentences based on the semantic meaning of words."}
{"completion": "speechbrain/sepformer-wham", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise."}
{"completion": "speechbrain/sepformer-wham", "text": "query: Our company needs a system to separate the spoken words from background noise in recorded customer service calls to improve the quality of transcriptions."}
{"completion": "git-large-coco", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}
{"completion": "git-large-coco", "text": "query: I work for an advertising company. I am looking for generating a tagline for a new yoga apparel product that my company is promoting. I have an image of the product that I want to use as input for describing the product."}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}
{"completion": "glpn-nyu-finetuned-diode-230103-091356", "text": "query: We are currently working on a project in which our robots need to estimate the depth of objects for navigation. Can you help us with setting that up?"}
{"completion": "finiteautomata/beto-sentiment-analysis", "text": "document: Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels."}
{"completion": "finiteautomata/beto-sentiment-analysis", "text": "query: For a smooth UX of my news website in Spanish, I want to only display articles that are positive. Please analyze an input article and return whether the sentiment is positive or not."}
{"completion": "Antheia/Hanna", "text": "document: Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset."}
{"completion": "Antheia/Hanna", "text": "query: I need to improve my robot's ability to understand natural language commands, please advise a solution"}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "document: A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data."}
{"completion": "tejas23/autotrain-amx2-1702259728", "text": "query: We want to provide our users with personalized recommendations on ways to reduce carbon emissions. We already have a dataset of different activities that reduce emissions. Let's figure out what activities or habits add the most emissions and should be avoided."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "document: Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "query: The user wants to develop an application to answer questions based on images."}
{"completion": "tuner007/pegasus_summarizer", "text": "document: PEGASUS fine-tuned for summarization"}
{"completion": "tuner007/pegasus_summarizer", "text": "query: Create a summary of the news article about a cricket match between India and England."}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "document: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels."}
{"completion": "keremberke/yolov8m-csgo-player-detection", "text": "query: Global Offensive."}
{"completion": "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks."}
{"completion": "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K", "text": "query: To ensure the security of the building, the new system needs to recognize and classify different types of vehicles entering and leaving the campus."}
{"completion": "openmmlab/upernet-convnext-small", "text": "document: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s."}
{"completion": "openmmlab/upernet-convnext-small", "text": "query: We were approached by a civil engineering company. They asked for our help in designing their solar-energy housing models. Tell me which method you use and how you would do it?"}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "document: This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder."}
{"completion": "stabilityai/sd-vae-ft-ema", "text": "query: I am designing a game and I want the model to generate the game's characters, weapons, and tools. These generated images should be based on textual descriptions provided by the game designers."}
{"completion": "openai/whisper-large", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large", "text": "query: A Language learning platform has a transcript creation feature that transcribes spoken language to text. Implement a transcriber for the platform."}
{"completion": "lvwerra/distilbert-imdb", "text": "document: This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set."}
{"completion": "lvwerra/distilbert-imdb", "text": "query: Build a sentiment analysis model to add a rating system for the movies on the website. Positive and negative scores should be assigned based on the reviews."}
{"completion": "distilbert-base-uncased-distilled-squad", "text": "document: DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark."}
{"completion": "distilbert-base-uncased-distilled-squad", "text": "query: A teacher is asking the question, \"What is the capital of France?\" and we want to help her students find the answer."}
{"completion": "google/tapas-base-finetuned-wtq", "text": "document: TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table."}
{"completion": "google/tapas-base-finetuned-wtq", "text": "query: \"Who scored the most goals in the season?\""}
{"completion": "facebook/dino-vitb16", "text": "document: Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}
{"completion": "facebook/dino-vitb16", "text": "query: Detect important features from images to help our new information filtering service."}
{"completion": "wavymulder/Analog-Diffusion", "text": "document: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}
{"completion": "wavymulder/Analog-Diffusion", "text": "query: A client asked us to produce a website header with the image of \"analog style city at night.\""}
{"completion": "videomae-small-finetuned-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-small-finetuned-ssv2", "text": "query: We are an online monitoring site. We need to classify videos automatically."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "document: Multilingual XLM-RoBERTa large model for extractive question answering on various languages. Trained on SQuAD 2.0 dataset and evaluated on SQuAD dev set, German MLQA, and German XQuAD."}
{"completion": "deepset/xlm-roberta-large-squad2", "text": "query: I want to have an AI on my app to answer questions about software documentation based on a given context."}
{"completion": "903929564", "text": "document: A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score."}
{"completion": "903929564", "text": "query: Please create a way of extracting named entities from unstructured data to help keep track of important details in meeting minutes."}
{"completion": "blip2-opt-2.7b", "text": "document: BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-opt-2.7b", "text": "query: I have an image, and I want the AI to describe what's happening in it."}
{"completion": "bibekbehera/autotrain-numeric_prediction-40376105019", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "bibekbehera/autotrain-numeric_prediction-40376105019", "text": "query: We run a data center, and we want to predict carbon emissions for the data center based on input features."}
{"completion": "CompVis/stable-diffusion-v1-4", "text": "document: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models."}
{"completion": "CompVis/stable-diffusion-v1-4", "text": "query: Our company manufactures custom art pieces for interior design. We need you to generate an artwork with a beautiful sunset scenery in the woods."}
{"completion": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 384-dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1", "text": "query: We are comparing how well the content we write matches a list of questions our readers have. Please guide us."}
{"completion": "903929564", "text": "document: A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score."}
{"completion": "903929564", "text": "query: Help me analyze a paragraph and identify the different entities within the text. For example, find organizations, locations, and more."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "document: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "query: We need to find tables in the document and classify them as a bordered or borderless table."}
{"completion": "abhishek/autotrain-iris-xgboost", "text": "document: A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%."}
{"completion": "abhishek/autotrain-iris-xgboost", "text": "query: We would like to classify various plant species using their characteristics."}
{"completion": "results-yelp", "text": "document: This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative."}
{"completion": "results-yelp", "text": "query: A restaurant chain is trying to identify and address customer complaints. Analyze restaurant reviews to determine whether they are positive or negative."}
{"completion": "FSMN-VAD", "text": "document: FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library."}
{"completion": "FSMN-VAD", "text": "query: Analyze an audio file to identify if there is any voice activity in it."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "document: A tabular classification model for predicting carbon emissions in grams, trained using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "query: The client, an environmental agency, needs a tool to estimate carbon emissions based on a given set of input features."}
{"completion": "superb/hubert-base-superb-ks", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-base-superb-ks", "text": "query: I am designing a smart home system and need to recognize spoken commands. How would I use your model to classify audio inputs?"}
{"completion": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')", "text": "query: We need a system for fast image recognition in our inventory management software that can recognize products based on their images."}
{"completion": "t5-large", "text": "document: T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks."}
{"completion": "t5-large", "text": "query: We want to analyze an interview transcript from a podcast and get a summary of the key points being discussed."}
{"completion": "google/ncsnpp-church-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024."}
{"completion": "google/ncsnpp-church-256", "text": "query: We are building an online tool for auto-generating images of churches. Write a code to generate a church image."}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "document: BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-bigpatent", "text": "query: The executive wants a summary of a research paper for a shareholders' meeting. Provide a template for generating the summary."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "document: Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "query: To increase customer satisfaction, we want an automatic reply for our company email. A lead has asked a question about the pricing of our product."}
{"completion": "TehVenom/PPO_Pygway-V8p4_Dev-6b", "text": "document: TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%."}
{"completion": "TehVenom/PPO_Pygway-V8p4_Dev-6b", "text": "query: I need a creative text generation model to write a short story about space exploration."}
{"completion": "pygmalion-2.7b", "text": "document: Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-2.7b", "text": "query: I'm creating a chatbot for my game. I need a virtual friend character who loves going on adventures and exploring new places. Can't wait to talk to players about their quests."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "query: I need to extract information from bills and invoices, such as the total amount and the billing date. Can you help me set up a model for that?"}
{"completion": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 384-dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1", "text": "query: Develop a tool to measure relevance of articles by how much they relate to a given query."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-MiniLM2-L6-H768", "text": "query: For our customer service department, we need a tool that can automatically determine if a given customer question is related to shipping or technical issues."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "document: A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "query: Our client is a real estate agency. Currently, they require a prediction of house prices based on their features."}
{"completion": "facebook/s2t-medium-librispeech-asr", "text": "document: s2t-medium-librispeech-asr is a Speech to Text Transformer (S2T) model trained for automatic speech recognition (ASR). The S2T model was proposed in this paper and released in this repository."}
{"completion": "facebook/s2t-medium-librispeech-asr", "text": "query: I need to transcribe an interview recording. Please recommend me an API."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "query: We are building an online chat platform, and the platform needs to have a function to find similar messages."}
{"completion": "openai/whisper-large", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large", "text": "query: Our startup is developing an AI assistant, and we want to convert user voice commands to text for further processing."}
{"completion": "deepset/deberta-v3-large-squad2", "text": "document: This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-large-squad2", "text": "query: A legal consultant working for our company will need a smart personal assistant to find answers in legal documents. Implement the model that can answer questions based on a text context."}
{"completion": "microsoft/tapex-base", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base", "text": "query: The company is specializing in corporate event planning. They make use of tables to manage various documents. They need to extract information from tables in a  natural language query format."}
{"completion": "facebook/dino-vitb16", "text": "document: Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}
{"completion": "facebook/dino-vitb16", "text": "query: Analyze images for a new ad campaign and determine key visual features to improve engagement."}
{"completion": "microsoft/resnet-18", "text": "document: ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision."}
{"completion": "microsoft/resnet-18", "text": "query: I am trying to classify an image of my cat or dog."}
{"completion": "google/ncsnpp-celebahq-256", "text": "document: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."}
{"completion": "google/ncsnpp-celebahq-256", "text": "query: We're launching an application that allows users to create stunning profile pictures. Help us integrate an algorithm to generate these pictures."}
{"completion": "facebook/opt-6.7b", "text": "document: OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example."}
{"completion": "facebook/opt-6.7b", "text": "query: You are building new tech alert system such that a user can enter a problem or a query, and an email with an appropriate solution is sent."}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "document: Binary Classification model for Carbon Emissions prediction"}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "query: A local consultant has been hired to categorize whether certain vehicle types will be within the limit allowed for acceptable carbon emissions. In order to enhance the categorization process by providing a faster and more accurate prediction, provide an actionable example of how to use the machine learning model previously trained."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "query: Working at a self-driving car company, we need a depth estimation model to help us process images from the vehicle's camera."}
{"completion": "flair/ner-english-ontonotes", "text": "document: This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-ontonotes", "text": "query: Implement a named entity recognition system for an app that will extract names of people, organizations, and monetary values in a given text."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "document: This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "query: Analyze a financial news article and extract all the mentioned company names and their stock ticker codes."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "query: I'm building a video recommendation system. Therefore, I want to classify videos into categories based on their content."}
{"completion": "microsoft/tapex-large-finetuned-wikisql", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset."}
{"completion": "microsoft/tapex-large-finetuned-wikisql", "text": "query: We have a large database of football player statistics, and we want to build a product that can answer different questions. For example, the product should output the number of goals scored by a player when we input their name and season."}
{"completion": "google/flan-t5-base", "text": "document: FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering."}
{"completion": "google/flan-t5-base", "text": "query: Please prepare a utility for me to translate \"Life is beautiful\" to Italian."}
{"completion": "google/pegasus-pubmed", "text": "document: The PEGASUS model is designed for abstractive summarization. It is pretrained on a mixture of C4 and HugeNews datasets and stochastically samples important sentences. The model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that encodes newline characters."}
{"completion": "google/pegasus-pubmed", "text": "query: There's a huge article on recent advancements in cancer research, and I need a concise summary of it."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "query: An end-user left a review on our website. We would like our language model to classify the topic of the review."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "document: This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "query: We are creating a football game application and want to develop an AI-based football player suitable for our new game."}
{"completion": "allenai/cosmo-xl", "text": "document: COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation."}
{"completion": "allenai/cosmo-xl", "text": "query: Create a conversational model to help in analyzing a meeting scenario where the speaker talks about project updates. The model has to respond like an attentive audience providing feedback and asking relevant questions."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "document: Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "query: Generate a picture of a butterfly with background for an article about butterflies."}
{"completion": "datadmg/autotrain-test-news-44534112235", "text": "document: This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality."}
{"completion": "datadmg/autotrain-test-news-44534112235", "text": "query: Detect potential leaks or wastage of resources in our facility using the CO2 footprints dataset. We need information to aid us in cutting our Carbon footprint."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "document: A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "query: I'm a student and I want to quickly extract information from a digital invoice in a photo taken by my smartphone."}
{"completion": "mio/Artoria", "text": "document: This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output."}
{"completion": "mio/Artoria", "text": "query: Develop a tool to convert a text description of a customer support issue into speech for our customer service robot."}
{"completion": "EimisAnimeDiffusion_1.0v", "text": "document: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI."}
{"completion": "EimisAnimeDiffusion_1.0v", "text": "query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions."}
{"completion": "albert-base-v2", "text": "document: ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English."}
{"completion": "albert-base-v2", "text": "query: A company is building a conversational assistant which requires smart response suggestions. We need a functionality related to masked language modeling."}
{"completion": "microsoft/table-transformer-structure-recognition", "text": "document: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables."}
{"completion": "microsoft/table-transformer-structure-recognition", "text": "query: I am working on a project focusing on extracting data from various tables. The computer vision model needs to recognize rows and columns from a given image of a table."}
{"completion": "sentence-transformers/paraphrase-mpnet-base-v2", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-mpnet-base-v2", "text": "query: In order to recommend books to users, we want to identify books with similar themes or topics. Analyze book descriptions to find similarity."}
{"completion": "t5_sentence_paraphraser", "text": "document: A T5 model for paraphrasing sentences"}
{"completion": "t5_sentence_paraphraser", "text": "query: Working on an email campaign, I need to engage the users. I have a basic idea of the email, but a paraphrase version would be helpful to draft multiple emails."}
{"completion": "ckiplab/bert-base-chinese-pos", "text": "document: This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition)."}
{"completion": "ckiplab/bert-base-chinese-pos", "text": "query: I am a teacher at a university for the Chinese language department. I would like you to help me analyze the parts of speech of a given Chinese sentence."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-mlsd", "text": "query: We have an educational application for high school students. We want to create an interactive learning module that generates images based on the detected straight lines in the input image."}
{"completion": "microsoft/trocr-small-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM."}
{"completion": "microsoft/trocr-small-printed", "text": "query: An app project needs to copy the captured images from the live environment and paste the text copy over the app chat."}
{"completion": "kykim/bertshared-kor-base", "text": "document: Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks."}
{"completion": "kykim/bertshared-kor-base", "text": "query: Please help me to create a text summary of a Korean news article."}
{"completion": "CompVis/ldm-celebahq-256", "text": "document: Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs."}
{"completion": "CompVis/ldm-celebahq-256", "text": "query: I would like to generate high-quality, photorealistic images of human faces."}
{"completion": "904029577", "text": "document: This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams."}
{"completion": "904029577", "text": "query: I am a human resources manager in a multinational company, and I need to extract personal names from a given text."}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "document: Barthez model finetuned on orangeSum for abstract generation in French language"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "query: Design a summarization program that will generate abstracts in French."}
{"completion": "nvidia/segformer-b2-finetuned-cityscapes-1024-1024", "text": "document: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b2-finetuned-cityscapes-1024-1024", "text": "query: Our customer is a city planner, looking for input to better understand green spaces and structures in a city. We are working on dividing the image captured by a drone into several meaningful semantic regions based on their content."}
{"completion": "microsoft/DialoGPT-small", "text": "document: DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread."}
{"completion": "microsoft/DialoGPT-small", "text": "query: I want to make a chatbot for my website that simulates a conversation using microsoft/DialoGPT-small."}
{"completion": "martin-ha/toxic-comment-model", "text": "document: This model is a fine-tuned version of the DistilBERT model to classify toxic comments."}
{"completion": "martin-ha/toxic-comment-model", "text": "query: Build a solution to scan and monitor comments on a website for detecting inappropriate content or toxic behavior."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "document: A LayoutLM model for document question answering."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "query: We need help with a question-answering service for construction material. We have a document with a list of materials and their specifications."}
{"completion": "videomae-small-finetuned-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-small-finetuned-ssv2", "text": "query: Our company is planning to build a video management system. We want to be able to classify user-uploaded videos automatically."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "document: LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "query: We are building an app for law students that helps review contract documents. Predict the most appropriate word to complete a sentence."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "document: High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics."}
{"completion": "google/ddpm-ema-celebahq-256", "text": "query: We are a matchmaking application. Our AI-based matching algorithm uses face detection to match similar facial features. We need to generate a variety of human faces for testing the algorithm."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "document: MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}
{"completion": "google/mobilenet_v1_0.75_192", "text": "query: We have a problem with autonomous vehicles and classification of traffic signs. Could you find a solution and classify the images we provide?"}
{"completion": "MCG-NJU/videomae-base-short", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks."}
{"completion": "MCG-NJU/videomae-base-short", "text": "query: I want to analyze my soccer match videos, classify the actions of the players, and how the game is being played."}
{"completion": "hf-tiny-model-private/tiny-random-GLPNForDepthEstimation", "text": "document: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library."}
{"completion": "hf-tiny-model-private/tiny-random-GLPNForDepthEstimation", "text": "query: We are going to create a set of 3d images. For that, we need to have the depth estimation."}
{"completion": "prompthero/openjourney-v4", "text": "document: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs."}
{"completion": "prompthero/openjourney-v4", "text": "query: An architecture firm is working on a new project and would like you to create a concept image of a modern building based on their description."}
{"completion": "lysandre/tiny-vit-random", "text": "document: A tiny-vit-random model for image classification using Hugging Face Transformers."}
{"completion": "lysandre/tiny-vit-random", "text": "query: Our client want to explore a new category of products. We need to help them classify the images of the items they have in their online store."}
{"completion": "microsoft/trocr-small-stage1", "text": "document: TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."}
{"completion": "microsoft/trocr-small-stage1", "text": "query: How would you go about extracting text from an old handwritten document?"}
{"completion": "kykim/bertshared-kor-base", "text": "document: Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks."}
{"completion": "kykim/bertshared-kor-base", "text": "query: I am an insurance officer who works with Korean customers. My company receives a claim in Korean, and I want to write a response to the clients in Korean."}
{"completion": "xm_transformer_s2ut_hk-en", "text": "document: Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain."}
{"completion": "xm_transformer_s2ut_hk-en", "text": "query: You are asked to create a system that translates spoken language from Hokkien to English, maintaining the audio format. You should use an existing Speech-to-speech translation model for this purpose."}
{"completion": "distilroberta-base", "text": "document: DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "distilroberta-base", "text": "query: \"The most famous soccer player in the world is <mask>.\""}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "document: A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library."}
{"completion": "Helsinki-NLP/opus-mt-en-it", "text": "query: We are looking to establish a business relationship in Italy, and we need to translate our company's introduction documents to Italian."}
{"completion": "bigscience/bloom-560m", "text": "document: BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads."}
{"completion": "bigscience/bloom-560m", "text": "query: The editor of a science magazine asks you to come up with an introduction paragraph for an article on artificial intelligence."}
{"completion": "lllyasviel/control_v11p_sd15_softedge", "text": "document: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_softedge", "text": "query: Create a fantasy-inspired image based on the description \"dreamy fairy forest with sparkling waterfalls\"."}
{"completion": "runwayml/stable-diffusion-v1-5", "text": "document: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input."}
{"completion": "runwayml/stable-diffusion-v1-5", "text": "query: We want to generate an image for a blog post about an astronaut riding a horse on Mars."}
{"completion": "facebook/blenderbot-3B", "text": "document: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library."}
{"completion": "facebook/blenderbot-3B", "text": "query: We need to develop a conversational engine to integrate into our chatbot to answer common business-related questions and engage customers."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "query: Identify the type of objects shown in this image - https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg"}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: We are managing a multi-lingual news website. Help us to generate summaries for the articles."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "document: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'."}
{"completion": "keremberke/yolov8n-table-extraction", "text": "query: We are a finance company and going over 10 documents. We would like to get the tables extracted in those financial documents."}
{"completion": "videomae-base-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-base-ssv2", "text": "query: I have a collection of videos from my life experiences, and I want to build an AI video categorization system that can classify the content of these videos into defined categories such as sports, birthday parties, and weddings."}
{"completion": "fcakyon/timesformer-hr-finetuned-k400", "text": "document: TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al."}
{"completion": "fcakyon/timesformer-hr-finetuned-k400", "text": "query: Our company creates a video platform for recommendations, we need to classify videos based on their content."}
{"completion": "microsoft/trocr-small-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository."}
{"completion": "microsoft/trocr-small-handwritten", "text": "query: A historian needs help to recognize, read and translate handwritten texts."}
{"completion": "ToddGoldfarb/Cadet-Tiny", "text": "document: Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model."}
{"completion": "ToddGoldfarb/Cadet-Tiny", "text": "query: Develop a lightweight conversational AI agent to interact with users on a Raspberry Pi device."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "document: A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10."}
{"completion": "t5-efficient-large-nl36_fine_tune_sum_V2", "text": "query: The company wants to generate a summary of the company's meeting minutes."}
{"completion": "google/flan-t5-xl", "text": "document: FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models."}
{"completion": "google/flan-t5-xl", "text": "query: I am now working on a multilingual chatbot. It should be able to answer any question, including translation, arithmetic problems and generative storytelling."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "document: Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder."}
{"completion": "OFA-Sys/chinese-clip-vit-base-patch16", "text": "query: Our company deals with e-commerce in China. We are looking for a solution to classify images based on Chinese text descriptions."}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "document: Baseline Model trained on tips5wx_sbh5 to apply regression on tip"}
{"completion": "merve/tips5wx_sbh5-tip-regression", "text": "query: Let's create a baseline model for a restaurant that wants to predict tips based on features such as total bill, sex, smoker, day, time, and table size."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "document: This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram."}
{"completion": "speechbrain/tts-tacotron2-ljspeech", "text": "query: Develop an application that takes a quote and converts it into audio so it can be used as a daily motivational reminder."}
{"completion": "facebook/opt-350m", "text": "document: OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, developed by Meta AI. It is designed to enable reproducible and responsible research at scale and bring more voices to the table in studying the impact of large language models. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation. It can also be fine-tuned on a downstream task using the CLM example."}
{"completion": "facebook/opt-350m", "text": "query: We are now working to complete a novel, but we have been focused on character development. We need help generating content for the story."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "query: Create a synthesized video of a dog playing with a Frisbee in the park based on a text input."}
{"completion": "facebook/convnext-tiny-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification."}
{"completion": "facebook/convnext-tiny-224", "text": "query: We are building a cat recognition system for automated cat feeders. We need to make sure that the cat is on the list before issuing food."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset."}
{"completion": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: We need an application to assist financial analysts in quickly finding specific information from financial reports."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "document: This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "query: We are building a medical FAQ bot that can answer users' questions based on the provided information."}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "document: Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class"}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "query: We want to generate images of people with a vintage style as assets for a history-related project."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "query: I have a robot that needs to identify different species of fish. Figure out a method to classify fish images."}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "document: Binary Classification model for Carbon Emissions prediction"}
{"completion": "jwan2021/autotrain-jwan-autotrain1-1768961489", "text": "query: I need to predict companies that have high carbon emission based on a tabular dataset containing several features."}
{"completion": "0xid/poca-SoccerTwos", "text": "document: A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "0xid/poca-SoccerTwos", "text": "query: As a video game developer, I need to observe a trained soccer playing agent for potential integration into my new game."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "document: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion."}
{"completion": "lambdalabs/sd-image-variations-diffusers", "text": "query: The end-user wants unique images that are visually appealing to print and put in their home."}
{"completion": "tuner007/pegasus_paraphrase", "text": "document: PEGASUS fine-tuned for paraphrasing"}
{"completion": "tuner007/pegasus_paraphrase", "text": "query: I need to create a tool that allows me to rephrase sentences to enhance my written communication."}
{"completion": "opus-mt-de-es", "text": "document: A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization."}
{"completion": "opus-mt-de-es", "text": "query: Translate the given German text to Spanish without knowing any specific language names."}
{"completion": "903429548", "text": "document: A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text."}
{"completion": "903429548", "text": "query: We are a startup company looking to identify new partner companies. Extract company names from the given article."}
{"completion": "Randeng-Pegasus-238M-Summary-Chinese", "text": "document: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs."}
{"completion": "Randeng-Pegasus-238M-Summary-Chinese", "text": "query: The marketing firm is working on publishing Chinese news articles on their website. They need a brief summary of each article for readers."}
{"completion": "nguyenvulebinh/wav2vec2-base-vietnamese-250h", "text": "document: Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio."}
{"completion": "nguyenvulebinh/wav2vec2-base-vietnamese-250h", "text": "query: Build a system to transcribe Vietnamese in sound file format from interviews."}
{"completion": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup", "text": "document: A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768)."}
{"completion": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup", "text": "query: I would like to categorize mushrooms from a photo I took in the park, and I am trying to understand if it is edible or poisonous. Can you provide me with a code snippet for zero-shot image classification?"}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "query: I want to transcribe Chinese spoken audio to text. Build me something that can help."}
{"completion": "facebook/dino-vits8", "text": "document: Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository."}
{"completion": "facebook/dino-vits8", "text": "query: I have just taken a picture of my new shoes. Help me to understand what a model may infer from the picture."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "document: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "query: Please help me identify any defects in a printed circuit board (PCB) and the location of the defects based on an image."}
{"completion": "git-large-textcaps", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-textcaps", "text": "query: A popular IT blog equipped with a treasure trove of information has asked our team to pull content from an image with embedded-text so that it can be used for reading purposes."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "document: This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code."}
{"completion": "code_trans_t5_base_code_documentation_generation_python", "text": "query: I want to generate the documentation for my Python code."}
{"completion": "opus-mt-de-en", "text": "document: A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library."}
{"completion": "opus-mt-de-en", "text": "query: As a language school, we need a system to translate from German to English for incoming student applications."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "document: This is a trained model of a DQN agent playing MountainCar-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "query: We need an AI application that can help us to control a car climbing up a steep hill according to the surrounding environment."}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "document: bert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER)."}
{"completion": "Davlan/bert-base-multilingual-cased-ner-hrl", "text": "query: We have a multinational company and we need to identify the names of people, organizations, and locations in a text."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: There is an audio clip from a business meeting, we want to transcribe it into text."}
{"completion": "facebook/mask2former-swin-small-coco-instance", "text": "document: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-small-coco-instance", "text": "query: I want to develop a solution that can automatically identify and categorize different objects in an image. I would like to know which API I should use for this task and how to use it."}
{"completion": "gpt2-large", "text": "document: GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective."}
{"completion": "gpt2-large", "text": "query: I am building a social media platform to engage with users. I need to write contents using language models to attract the audience."}
{"completion": "lakahaga/novel_reading_tts", "text": "document: This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks."}
{"completion": "lakahaga/novel_reading_tts", "text": "query: We want to create an audiobook for a Korean novel. What code will we write to convert the given Korean text to speech?"}
{"completion": "facebook/blenderbot_small-90M", "text": "document: Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation."}
{"completion": "facebook/blenderbot_small-90M", "text": "query: A user wants to chat with our AI regarding their favorite film, which they cannot seem to decide on."}
{"completion": "facebook/blenderbot-400M-distill", "text": "document: BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX."}
{"completion": "facebook/blenderbot-400M-distill", "text": "query: We want to enhance our company's customer support service. Can you help build a conversational AI model?"}
{"completion": "google/ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "google/ddpm-cifar10-32", "text": "query: Design a news cover image of a food festival that is going to be held next week."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "document: This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset."}
{"completion": "bigwiz83/sapbert-from-pubmedbert-squad2", "text": "query: We are now working on a project about different species of birds. Our client wants to know which bird species lay eggs in conifer trees."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "document: This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "ppo-BreakoutNoFrameskip-v4", "text": "query: Organize a learning-based agent to play Breakout without skipping any frames."}
{"completion": "google/ddpm-ema-cat-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17."}
{"completion": "google/ddpm-ema-cat-256", "text": "query: Our company develops a cat only social media platform and we need a system to create realistic cat images to test our app."}
{"completion": "sentence-transformers/nli-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/nli-mpnet-base-v2", "text": "query: As a researcher, I want to compare the similarity of research paper abstracts so I can find papers that are closely related to my topic."}
{"completion": "opus-mt-tc-big-en-pt", "text": "document: Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world."}
{"completion": "opus-mt-tc-big-en-pt", "text": "query: Create a solution to provide real-time translation for participants in a multi-language conference call."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur", "text": "document: Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur", "text": "query: We are building a multilingual smart speaker and want to translate English speech to Spanish, French, and Italian speech."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "document: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "query: We're going to create a poster that includes a fantasy-themed and high-quality anime character image."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "damo-vilab/text-to-video-ms-1.7b-legacy", "text": "query: Develop a platform for virtual tours with generated videos based on user description. Tourists can input a description of a city or tourist spot and see it through generated videos."}
{"completion": "mrm8488/bert2bert_shared-spanish-finetuned-summarization", "text": "document: Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization"}
{"completion": "mrm8488/bert2bert_shared-spanish-finetuned-summarization", "text": "query: As a consultant to various news agencies, provide a summary of today's headlines in Spanish."}
{"completion": "pygmalion-350m", "text": "document: This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors."}
{"completion": "pygmalion-350m", "text": "query: In a city tour app, we have a chat feature where users can make an inquiry about the city. Write a solution to respond to user's inquiries."}
{"completion": "redshift-man-skiing", "text": "document: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'."}
{"completion": "redshift-man-skiing", "text": "query: A director wants to make a commercial for a new environmentally friendly electric car. Provide a sample generated video with rainforest trees surrounding a charging station."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "document: Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0."}
{"completion": "cerebras/Cerebras-GPT-111M", "text": "query: Generate a text paragraph on the topic \"Impact of Artificial Intelligence on Healthcare\""}
{"completion": "mrm8488/t5-base-finetuned-summarize-news", "text": "document: Google's T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017."}
{"completion": "mrm8488/t5-base-finetuned-summarize-news", "text": "query: My English teacher asked me to summarize a news article. Can you help me do that?"}
{"completion": "google/ddpm-ema-bedroom-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "query: I am an architect looking to develop a virtual bedroom model. Create an image of a bedroom that I can use for my project."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: A transcription project requires the punctuation restoration for a text in multiple languages, including English, French, Italian, and German."}
{"completion": "martin-ha/toxic-comment-model", "text": "document: This model is a fine-tuned version of the DistilBERT model to classify toxic comments."}
{"completion": "martin-ha/toxic-comment-model", "text": "query: We are trying to build a moderation system for our website comments section. We would like to prevent harmful content from being posted by checking the text for toxicity before it is published."}
{"completion": "google/tapas-mini-finetuned-wtq", "text": "document: TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table."}
{"completion": "google/tapas-mini-finetuned-wtq", "text": "query: To offer customization options to our users we need to extract data from their sales report. Design a solution to extract data from their sales report."}
{"completion": "facebook/blenderbot-90M", "text": "document: BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead."}
{"completion": "facebook/blenderbot-90M", "text": "query: John wants to chat with an AI like he is talking to a friend."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "document: A text-to-speech model trained on mtedx, covost2, europarl_st, and voxpopuli datasets for English, French, Spanish, and Italian languages. Licensed under cc-by-nc-4.0."}
{"completion": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10", "text": "query: Narrate the given text in French."}
{"completion": "glpn-kitti", "text": "document: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-kitti", "text": "query: I would like to obtain depth estimation for given pictures to use in an augmented reality game."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "document: This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order."}
{"completion": "cross-encoder/ms-marco-MiniLM-L-6-v2", "text": "query: Our client wants to find the most relevant information from a list of passages. They are looking for information on a specific question."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "document: A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "query: We have bought a package from an online store recently. We need to know about the total amount on the invoice."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-wtq", "text": "query: Adam has a startup that sells food products to consumers. He is in need of a recommendation system to display relevant nutritional information. Generate a model that can accomplish this."}
{"completion": "stabilityai/stable-diffusion-2-1-base", "text": "document: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models."}
{"completion": "stabilityai/stable-diffusion-2-1-base", "text": "query: Create a digital art piece that features a serene landscape with mountains and a river in the style of a watercolor painting."}
{"completion": "SYSPIN/Marathi_Male_TTS", "text": "document: A Marathi Male Text-to-Speech model using ESPnet framework."}
{"completion": "SYSPIN/Marathi_Male_TTS", "text": "query: Can you assist me in synthesizing a Marathi male voice to say \"\u0906\u092a\u0923 \u0915\u0938\u0947 \u0906\u0939\u093e\u0924?\"?"}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "document: Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "query: We have a user-facing app that collects customer feedback. It uses an image sent by the customer and a relevant question. Provide a mechanism to get appropriate answers to the given question."}
{"completion": "dqn-CartPole-v1", "text": "document: This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "dqn-CartPole-v1", "text": "query: Developing a learning agent to play CartPole to compete with the human players."}
{"completion": "MCG-NJU/videomae-base-short-finetuned-kinetics", "text": "document: VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository."}
{"completion": "MCG-NJU/videomae-base-short-finetuned-kinetics", "text": "query: We need to develop a system for classifying sports video clips."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: A publisher just sent us two paragraphs. We need to check if they are semantically similar or if they are simply paraphrasing the same idea."}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819"}
{"completion": "DataIntelligenceTeam/eurocorpV4", "text": "query: Invoice Number, Date, Total Amount, and Company Name."}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: We are a robotics research team working on audio-event detection and localization. We need to process audio from microphones placed around our environment and extract spectrogram features."}
{"completion": "financial-summarization-pegasus", "text": "document: This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization."}
{"completion": "financial-summarization-pegasus", "text": "query: Any suggestions on how to summarize financial news articles? We are building a chat platform for the financial sector."}
{"completion": "EimisAnimeDiffusion_1.0v", "text": "document: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI."}
{"completion": "EimisAnimeDiffusion_1.0v", "text": "query: Create an illustration for our web article, \"How the use of technology in agriculture can safeguard our tomorrow\"."}
{"completion": "google/pix2struct-textcaps-base", "text": "document: Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks."}
{"completion": "google/pix2struct-textcaps-base", "text": "query: For our marketing team, please extract a textual description of a given image."}
{"completion": "google/flan-t5-small", "text": "document: FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application."}
{"completion": "google/flan-t5-small", "text": "query: \"What time is it now?\""}
{"completion": "Alexei1/imdb", "text": "document: A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487."}
{"completion": "Alexei1/imdb", "text": "query: A cellphone company wants you to create an automated tool for quickly determining if the movie review text is positive or negative."}
{"completion": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset."}
{"completion": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2", "text": "query: My company's security team wants to classify video footage of intruders."}
{"completion": "t5-small", "text": "document: T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks."}
{"completion": "t5-small", "text": "query: Our client wants to build an AI model to serve news summaries of daily updates."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: There is a painting exhibition, and we need to generate captions for each painting to explain them."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "document: A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761511", "text": "query: Create a script to predict the housing prices for a given dataset and provide a summary of the performance metrics."}
{"completion": "bigcode/santacoder", "text": "document: The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations."}
{"completion": "bigcode/santacoder", "text": "query: I am a developer, I have a Python class that needs a method to calculate the area_coverage. Please generate the Python code for that method."}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: Identify the most prevalent features of an audio clip to help us analyze and classify different sounds."}
{"completion": "optimum/t5-small", "text": "document: T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization."}
{"completion": "optimum/t5-small", "text": "query: Could you translate an English text into French using optimum/t5-small pre-trained model?"}
{"completion": "google/owlvit-base-patch16", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features."}
{"completion": "google/owlvit-base-patch16", "text": "query: I want to design a photo application that will take users' text queries and highlight the specific objects mentioned in the queries in the images."}
{"completion": "swin2SR-lightweight-x2-64", "text": "document: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution."}
{"completion": "swin2SR-lightweight-x2-64", "text": "query: Develop a system that can upscale low-resolution images to improve their quality for a photography application."}
{"completion": "keremberke/yolov5m-license-plate", "text": "document: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy."}
{"completion": "keremberke/yolov5m-license-plate", "text": "query: Our client has a parking lot management system. They need to automatically detect license plates in the images taken by their parking lot cameras."}
{"completion": "lakahaga/novel_reading_tts", "text": "document: This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks."}
{"completion": "lakahaga/novel_reading_tts", "text": "query: John is studying Korean, and he wants an app that can read Korean text to him."}
{"completion": "ckiplab/bert-base-chinese-ws", "text": "document: This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition)."}
{"completion": "ckiplab/bert-base-chinese-ws", "text": "query: We are a Chinese e-commerce company looking to analyze customer reviews and focus on improving aspects of our products. We need to tokenize the Chinese text from the reviews."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "document: BrandonBot4Epochs is a conversational model trained on the GPT-2 architecture for text generation. It can be used to generate responses in a chatbot-like interface."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "query: In our customer support system, we need an AI to chat with people when there aren't any customer support agents available."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "document: Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts."}
{"completion": "mrm8488/t5-base-finetuned-common_gen", "text": "query: 'dog', 'run', 'park'. Can you use a model that generates a sentence using these words?"}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "document: Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0. The model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning Wav2Vec2-Large-Robust on MSP-Podcast (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An ONNX export of the model is available from doi:10.5281/zenodo.6221127. Further details are given in the associated paper and tutorial."}
{"completion": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "text": "query: We need to analyze the emotional tone of a podcast episode to better understand the audience's reaction."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "document: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit."}
{"completion": "keremberke/yolov8s-pcb-defect-segmentation", "text": "query: Please inspect an image of a printed circuit board, and then provide information on defects and their locations."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "document: A tiny TAPAS model for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "query: The board members in our company need a way to pull up specific data from tables through questions."}
{"completion": "glpn-nyu", "text": "document: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}
{"completion": "glpn-nyu", "text": "query: We have an image URL and want to estimate the depth information from it."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "document: This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "Raiden-1001/poca-Soccerv7", "text": "query: We are developing a new game similar to football and need an artificial intelligence agent to control the players."}
{"completion": "DialoGPT-medium-PALPATINE2", "text": "document: A DialoGPT model trained for generating human-like conversational responses."}
{"completion": "DialoGPT-medium-PALPATINE2", "text": "query: I want to create an AI conversation with an chatbot to discuss Star Wars Universe"}
{"completion": "facebook/tts_transformer-es-css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10."}
{"completion": "facebook/tts_transformer-es-css10", "text": "query: Design a software that will read the Spanish language for an audiobook targeted for Spanish native speakers."}
{"completion": "t5-3b", "text": "document: T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks."}
{"completion": "t5-3b", "text": "query: Our company needs to translate a user manual from English to French."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb", "text": "query: A social media platform wants to categorize videos uploaded by users based on their content."}
{"completion": "nvidia/mit-b0", "text": "document: SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes."}
{"completion": "nvidia/mit-b0", "text": "query: We are a company providing an image recognition solution. We are asked to build a tool that can identify the most likely class of an object in an image."}
{"completion": "hustvl/yolos-tiny", "text": "document: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model."}
{"completion": "hustvl/yolos-tiny", "text": "query: Develop a tool to automatically detect and count the number of cars in a parking lot from surveillance images."}
{"completion": "bert-base-chinese", "text": "document: This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling."}
{"completion": "bert-base-chinese", "text": "query: Create a system that can fill in the missing words in a given Chinese sentence."}
{"completion": "mio/amadeus", "text": "document: This model was trained by mio using amadeus recipe in espnet."}
{"completion": "mio/amadeus", "text": "query: Please create a text-to-speech system for call centers, so the computer can speak with customers."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "query: We are a team of music producers, and we want to separate vocals from the instruments in a recorded music track."}
{"completion": "google/owlvit-base-patch32", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries."}
{"completion": "google/owlvit-base-patch32", "text": "query: I want to create an app for tourstists. They will upload an image and the app recognizes the highlights of the city."}
{"completion": "julien-c/pokemon-predict-hp", "text": "document: A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon."}
{"completion": "julien-c/pokemon-predict-hp", "text": "query: The game company wants to develop a new Pokemon game. However, they need to predict the HP of each Pokemon before releasing them into the game."}
{"completion": "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks."}
{"completion": "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K", "text": "query: We are running an animal adoption website and trying a machine learning approach to automatically classify images of pets as either cats or dogs."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "document: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}
{"completion": "modelscope-damo-text-to-video-synthesis", "text": "query: \"A dog running happily in the park\"."}
{"completion": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640", "text": "document: A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain."}
{"completion": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640", "text": "query: The environmental agency wants to predict the amount of carbon dioxide emissions for specific equipment based on the supplied dataset."}
{"completion": "ocariz/universe_1400", "text": "document: This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs."}
{"completion": "ocariz/universe_1400", "text": "query: Our company is planning an event about outer space. We need to generate images of the universe for promotional materials."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-mpnet-base-v2", "text": "query: A publisher uses articles from freelancers. We want to check similarity of articles to avoid publishing duplicate content."}
{"completion": "google/ddpm-celebahq-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46."}
{"completion": "google/ddpm-celebahq-256", "text": "query: I want to develop an AI art gallery website that generates unique images of people's faces."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "document: This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API."}
{"completion": "abhishek/autotrain-iris-logistic-regression", "text": "query: The botanist I am working with just sent me this table containing iris flower features. What do you say? Does it belong to setosa, versicolor, or virginica?"}
{"completion": "impira/layoutlm-invoices", "text": "document: This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head."}
{"completion": "impira/layoutlm-invoices", "text": "query: You have been hired as a software developer for an accounting firm that receives lots of invoices daily. Your task is to design a solution which can quickly take questions from accountants and provide relevant information from invoices."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-docvqa", "text": "query: Our company provides technical support. We have a lot of documentation on our products. We want to use AI Models to help us answer the questions quickly by providing sections of the document."}
{"completion": "deepset/roberta-large-squad2", "text": "document: A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context."}
{"completion": "deepset/roberta-large-squad2", "text": "query: In a short story about cats and dogs, we need to find out who the main character in the story is."}
{"completion": "vintedois-diffusion-v0-1", "text": "document: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps."}
{"completion": "vintedois-diffusion-v0-1", "text": "query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars."}
{"completion": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli", "text": "document: This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark."}
{"completion": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli", "text": "query: Analyze a news article and determine which categories it belongs to."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-depth", "text": "query: Our company wants to autonomously drive their vehicles in the city. We need a solution to estimate the depth of the surroundings using images taken by the vehicle's cameras."}
{"completion": "cointegrated/rut5-base-absum", "text": "document: This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets."}
{"completion": "cointegrated/rut5-base-absum", "text": "query: We have a large volume of Russian text and we need to summarize it to a smaller length."}
{"completion": "lakahaga/novel_reading_tts", "text": "document: This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks."}
{"completion": "lakahaga/novel_reading_tts", "text": "query: You are part of a team that is designing an audiobook app. The app must be able to convert text to speech for Korean content."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "document: This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "query: culture, society, economy, health, sports. The task requires me to categorize an article written in Spanish."}
{"completion": "glpn-nyu-finetuned-diode-221122-082237", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}
{"completion": "glpn-nyu-finetuned-diode-221122-082237", "text": "query: I need a depth estimation model for my computer vision project. I want detailed instructions on how to obtain this depth estimation model and how to use it."}
{"completion": "superb/hubert-base-superb-ks", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-base-superb-ks", "text": "query: We are an AI-driven company and we want to classify spoken commands from an audio file that includes keywords such as \"yes\", \"no\", \"stop\", \"go\", \"left\", and \"right\"."}
{"completion": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset."}
{"completion": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2", "text": "query: I would like to classify the actions in my videos. Can you integrate this model from Hugging Face to classify the videos?"}
{"completion": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition", "text": "document: The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']."}
{"completion": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition", "text": "query: Our therapist app wishes to identify the client's emotions within an audio recording during a session."}
{"completion": "tuner007/pegasus_paraphrase", "text": "document: PEGASUS fine-tuned for paraphrasing"}
{"completion": "tuner007/pegasus_paraphrase", "text": "query: I have some content for a blog article and need it to be paraphrased. Create a new content in such a way that it retains its original meaning but is not a direct copy."}
{"completion": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", "text": "document: A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k."}
{"completion": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", "text": "query: The product management team at a fashion company wants to quickly and accurately classify fashion product images for the website."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: We're designing an app where users can send in a picture and get a caption generated from the photo. User input a picture to get caption."}
{"completion": "opus-mt-en-ru", "text": "document: Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers."}
{"completion": "opus-mt-en-ru", "text": "query: Our American-Russian clients would like to communicate in their native languages. Could you please help me find a way to translate English text to Russian?"}
{"completion": "glpn-nyu-finetuned-diode-221116-054332", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-054332", "text": "query: Develop a system that will help self-driving cars to estimate the depth of objects on the road."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset."}
{"completion": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb", "text": "query: We have a gym and we want to detect the type of workout that a person is doing from a video."}
{"completion": "xm_transformer_unity_en-hk", "text": "document: Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain."}
{"completion": "xm_transformer_unity_en-hk", "text": "query: Translate a recorded TED talk from English to Hokkien using a speech-to-speech translation model and return the translated audio."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "document: A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization."}
{"completion": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "text": "query: We need a machine learning model that can identify dog breeds in photos in order to improve our dog breed identification app."}
{"completion": "duncan93/video", "text": "document: A text-to-video model trained on OpenAssistant/oasst1 dataset."}
{"completion": "duncan93/video", "text": "query: I want to create a video of a flower blooming from a text description using AI."}
{"completion": "google/mt5-base", "text": "document: mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks."}
{"completion": "google/mt5-base", "text": "query: I want to develop a language translation tool which can handle different languages like English, German, and Spanish."}
{"completion": "tiny-random-DPTForDepthEstimation", "text": "document: A tiny random DPT model for depth estimation using Hugging Face Transformers library."}
{"completion": "tiny-random-DPTForDepthEstimation", "text": "query: The company is building a depth estimation feature for its mobile application, and they are looking for a solution to generate depth maps from a single input image."}
{"completion": "gpt2", "text": "document: GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences."}
{"completion": "gpt2", "text": "query: We are producing a digital assistant, and we want it to generate text completion based on the user's question."}
{"completion": "opus-mt-sv-en", "text": "document: A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece."}
{"completion": "opus-mt-sv-en", "text": "query: We are a language learning platform. We want to build a small app to help our users translate Swedish texts to English."}
{"completion": "Salesforce/codegen-2B-multi", "text": "document: CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters."}
{"completion": "Salesforce/codegen-2B-multi", "text": "query: Develop a program that generates a Python function to calculate the area of a rectangle, given the length and width."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "document: A speech-to-speech translation model that can be loaded on the Inference API on-demand."}
{"completion": "facebook/xm_transformer_sm_all-en", "text": "query: We have a voice assistant project, and our team is Middle Eastern people that mostly speak Arabic. We need to convert our Arabic speech to English for further development."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "document: A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions."}
{"completion": "CQI_Visual_Question_Awnser_PT_v0", "text": "query: I need an AI model that can read invoices, receipts, and statements then answer questions about them."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_seg", "text": "query: We need to generate an image using the diffusion-based text-to-image method and save it."}
{"completion": "patrickjohncyh/fashion-clip", "text": "document: FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks."}
{"completion": "patrickjohncyh/fashion-clip", "text": "query: Our client is developing an ecommerce platform, and they need an AI solution that can automatically classify images of clothes in their inventory."}
{"completion": "google/flan-t5-xl", "text": "document: FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models."}
{"completion": "google/flan-t5-xl", "text": "query: Let's make a summary of the academic paper I just read."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "query: Can you create a model to predict carbon emissions based on the building age, area, number of electric devices?"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "document: Barthez model finetuned on orangeSum for abstract generation in French language"}
{"completion": "moussaKam/barthez-orangesum-abstract", "text": "query: As a writer for a French news agency, we need a system to generate abstracts from our news articles."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-scribble", "text": "query: The advertising team requires a unique design for a new product packaging based on an example that they provided. They have also given a description of the desired design."}
{"completion": "google/ddpm-church-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference."}
{"completion": "google/ddpm-church-256", "text": "query: Our client wants us to generate images of beautiful church architecture."}
{"completion": "cross-encoder/nli-deberta-v3-base", "text": "document: This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-deberta-v3-base", "text": "query: When the skater moves their arms closer to the body, the spin becomes faster. I want to know if this proves the conservation of angular momentum."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "Robertooo/autotrain-hmaet-2037366891", "text": "query: Help users to reduce their carbon footprint by providing a prediction tool that calculates the estimated carbon emissions of their daily habits."}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "document: TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository."}
{"completion": "facebook/timesformer-hr-finetuned-ssv2", "text": "query: We are working on a new health and safety application for a factory, we need a model that identifies a sequence of working actions using video inputs to validate whether the action conforms to the established safety regulations."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "document: A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "query: We received an email from a Dutch client. Help us translate it into English."}
{"completion": "google/ddpm-bedroom-256", "text": "document: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN."}
{"completion": "google/ddpm-bedroom-256", "text": "query: We would like to see an example of a dream bedroom interior design using computer-generated images."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "document: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes."}
{"completion": "keremberke/yolov8s-pothole-segmentation", "text": "query: We would like to develop a system for detecting potholes in an image. The output should be bounding box coordinates and masks for the detected potholes."}
{"completion": "mio/tokiwa_midori", "text": "document: This model was trained by mio using amadeus recipe in espnet."}
{"completion": "mio/tokiwa_midori", "text": "query: At our hotel, we are creating an English-language information video for our guests. We need a pleasant voice-over."}
{"completion": "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k", "text": "document: This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training."}
{"completion": "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k", "text": "query: We need to develop a real estate website and classify the images of properties based on their respective categories, like residential, commercial, or land."}
{"completion": "clip-vit-base-patch32-ko", "text": "document: Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data."}
{"completion": "clip-vit-base-patch32-ko", "text": "query: A Korean food brand wants to automatically categorize their Instagram photos into various product collections like snacks, drinks, and main dishes. Help them find suitable technology."}
{"completion": "tts_transformer-ar-cv7", "text": "document: Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset."}
{"completion": "tts_transformer-ar-cv7", "text": "query: Provide a method to convert a text in Arabic language into speech."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "query: We want to find the semantic similarity of a group of sentences."}
{"completion": "poca-SoccerTwosv2", "text": "document: A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}
{"completion": "poca-SoccerTwosv2", "text": "query: We want a reinforcement learning model that could play the SoccerTwos game with the player utilizing Unity ML-Agents Library."}
{"completion": "padmalcom/tts-tacotron2-german", "text": "document: Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently."}
{"completion": "padmalcom/tts-tacotron2-german", "text": "query: We need a quick method to create a German audio sample from a text input."}
{"completion": "guillaumekln/faster-whisper-large-v2", "text": "document: Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper."}
{"completion": "guillaumekln/faster-whisper-large-v2", "text": "query: We have a company transcript service that utilizes speech recognition software. We want to transcribe an audio file."}
{"completion": "hustvl/yolos-small", "text": "document: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN)."}
{"completion": "hustvl/yolos-small", "text": "query: I am an AI analyst working on computer vision for vehicle parking. I need to detect all the vehicles in the image, including their locations."}
{"completion": "microsoft/resnet-50", "text": "document: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback."}
{"completion": "microsoft/resnet-50", "text": "query: We are developing an App to classify photos. We need a suggestion on how a photo should be categorized."}
{"completion": "j-hartmann/emotion-english-distilroberta-base", "text": "document: This model classifies emotions in English text data. It predicts Ekman's 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base."}
{"completion": "j-hartmann/emotion-english-distilroberta-base", "text": "query: We are building a large scale chatbot, which needs an understanding of the emotional content of a user's input."}
{"completion": "microsoft/xclip-base-patch32", "text": "document: X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."}
{"completion": "microsoft/xclip-base-patch32", "text": "query: I want to create a system that can analyze a video and provide a short textual description. Utilize a model that is trained on both video and text data."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "document: This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."}
{"completion": "deepset/deberta-v3-base-squad2", "text": "query: The company is building an educational website for students. We need an assistant to answer questions from students."}
{"completion": "indobenchmark/indobert-base-p1", "text": "document: IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective."}
{"completion": "indobenchmark/indobert-base-p1", "text": "query: An Indonesian company wants to repurpose a BERT model to assess their social media marketing efforts to make it in their language."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on Portuguese using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese", "text": "query: Determine the speech-to-text of conference call recordings with international business partners from Portugal."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "query: I am developing an app to analyze movie reviews. I need to know whether the review is positive or negative."}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "document: This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China."}
{"completion": "sheldonxxxx/OFA_model_weights", "text": "query: Your toy company wants to create an interactive toy that answers questions about objects shown in images. We need to find a suitable pre-trained model for this task. "}
{"completion": "deepset/minilm-uncased-squad2", "text": "document: MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library."}
{"completion": "deepset/minilm-uncased-squad2", "text": "query: While I'm reading a scientific paper, I want to extract an answer to my question from a given paragraph."}
{"completion": "distilbert-base-uncased", "text": "document: DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. It was pretrained with three objectives: Distillation loss, Masked language modeling (MLM), and Cosine embedding loss. This model is uncased and can be used for masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task."}
{"completion": "distilbert-base-uncased", "text": "query: Assist me in completing my email draft by predicting the appropriate words to fill in the blanks in the sentences."}
{"completion": "geolocal/StreetCLIP", "text": "document: StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images."}
{"completion": "geolocal/StreetCLIP", "text": "query: I am planning a trip to San Francisco and need to double-check if this image I found online is really from this city."}
{"completion": "neulab/omnitab-large-1024shot", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting)."}
{"completion": "neulab/omnitab-large-1024shot", "text": "query: We have a table with information on various flowers' bloom periods, appearance, and care requirements. Help us answer a question about when a Cherry Blossom blooms."}
{"completion": "kobart-base-v2", "text": "document: KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more."}
{"completion": "kobart-base-v2", "text": "query: Our company intends to deal with reviews from our Korean clients, and we need to extract important features from the texts to perform further analysis."}
{"completion": "impira/layoutlm-document-qa", "text": "document: A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}
{"completion": "impira/layoutlm-document-qa", "text": "query: I have an invoice template with an invoice number. Tell me the invoice number."}
{"completion": "blip2-flan-t5-xl", "text": "document: BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xl", "text": "query: Our marketing team has an image feed and wants to trigger content related to their conversations about images. We require a model to embed both images and text into the embedding to generate text in response to those images."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "document: Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "query: Develop a conversational AI to talk about AI chatbots' history and their current capabilities based on input text from users."}
{"completion": "wavymulder/Analog-Diffusion", "text": "document: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}
{"completion": "wavymulder/Analog-Diffusion", "text": "query: We are launching a vintage-themed clothing line and would like to generate promotional images featuring an analog-style cityscape."}
{"completion": "xlm-roberta-large", "text": "document: XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering."}
{"completion": "xlm-roberta-large", "text": "query: \"The book is in the ____.\" Help us complete this phrase."}
{"completion": "financial-summarization-pegasus", "text": "document: This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization."}
{"completion": "financial-summarization-pegasus", "text": "query: We are working on a financial analysis project in our company. Our analysts need a quick summary of financial news covering stock, markets, currencies, rates, and cryptocurrencies. Summarize a given article in a short paragraph."}
{"completion": "Linaqruf/anything-v3.0", "text": "document: A text-to-image model that generates images from text descriptions."}
{"completion": "Linaqruf/anything-v3.0", "text": "query: We are marketing a product with the text \"Discover the wilderness\". We are tying to make a poster for it."}
{"completion": "ivelin/donut-refexp-combined-v1", "text": "document: A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question."}
{"completion": "ivelin/donut-refexp-combined-v1", "text": "query: We received an assignment where the instructor sent a mixed image between cat and dog, and the instructor was asked for the number of cats and dogs in the image."}
{"completion": "ocariz/butterfly_200", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs."}
{"completion": "ocariz/butterfly_200", "text": "query: We want to create an interactive website showcasing different types of butterflies. Visitors should be able to view generated images within each category."}
{"completion": "facebook/tts_transformer-zh-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Simplified Chinese, Single-speaker female voice, Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-zh-cv7_css10", "text": "query: We are tasked with creating an audio message in Simplified Chinese for our clientele."}
{"completion": "lllyasviel/sd-controlnet-normal", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-normal", "text": "query: We want to create a 3D representation of an object given a 2D image. To do this, we need to estimate the normal maps of the object."}
{"completion": "camenduru/text2-video-zero", "text": "document: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more."}
{"completion": "camenduru/text2-video-zero", "text": "query: I need to produce a short video summary based on a text description of an event."}
{"completion": "facebook/blenderbot-3B", "text": "document: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library."}
{"completion": "facebook/blenderbot-3B", "text": "query: Generate an engaging and coherent conversation with a chatbot that can discuss various topics and display knowledge, empathy, and personality."}
{"completion": "microsoft/unixcoder-base", "text": "document: UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks."}
{"completion": "microsoft/unixcoder-base", "text": "query: I want to extract features from unstructured multimodal data like code comments and abstract syntax trees to better understand the code structure."}
{"completion": "tiny-wav2vec2-stable-ln", "text": "document: A tiny wav2vec2 model for Automatic Speech Recognition"}
{"completion": "tiny-wav2vec2-stable-ln", "text": "query: I want to make an app that transcribes voice notes to text using Automatic Speech Recognition. Can you help me?"}
{"completion": "facebook/blenderbot-1B-distill", "text": "document: BlenderBot-1B is a large-scale open-domain chatbot model that can engage in conversations, ask and answer questions, and display knowledge, empathy, and personality. This distilled version is smaller and faster than the original 9.4B parameter model, making it more accessible for use."}
{"completion": "facebook/blenderbot-1B-distill", "text": "query: We're developing a chatbot for a big tech company. It should be able to provide general information, as well as answer questions based on user input."}
{"completion": "pyannote/voice-activity-detection", "text": "document: A pretrained voice activity detection pipeline that detects active speech in audio files."}
{"completion": "pyannote/voice-activity-detection", "text": "query: Our team wants to develop a software to detect the active speech from a recorded conference call."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "document: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes."}
{"completion": "stabilityai/stable-diffusion-2-1", "text": "query: Our design department is looking for a tool to generate creative and unique illustrations from text descriptions. We want to incorporate this feature into our design workflow."}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "document: This is an image captioning model training by Zayn"}
{"completion": "AICVTG_What_if_a_machine_could_create_captions_automatically", "text": "query: We want to create a robotics marketing tool that can describe images through text form."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset."}
{"completion": "JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k", "text": "query: I have a recorded audio file, which was noisy. I plan to separate the noise from the audio using a model trained on noisy audios."}
{"completion": "abhishek/autotrain-iris-knn", "text": "document: A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9."}
{"completion": "abhishek/autotrain-iris-knn", "text": "query: I am an ecology researcher, I don't know what this flower is, can you help me tell based on its features?"}
{"completion": "prompthero/openjourney-v4", "text": "document: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs."}
{"completion": "prompthero/openjourney-v4", "text": "query: We have a storytelling platform, and we want to create illustrations for our stories based on the text."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "query: A user is searching for specific information in a document. Help them find the answers to their questions."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset."}
{"completion": "microsoft/tapex-large-finetuned-wtq", "text": "query: I am an investment portfolio manager and I want to compare the annual returns of different stocks. Can you help me classify their performance based on a table?"}
{"completion": "text2vec-large-chinese", "text": "document: A Chinese sentence similarity model based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replacing MacBERT with LERT, and keeping other training conditions unchanged."}
{"completion": "text2vec-large-chinese", "text": "query: As part of our dating app matching algorithm, find out if two user descriptions are similar enough to recommend as potential partners."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "document: This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "query: We are a software agency and we just built a desktop application for our client. The client is a financial institution that needs to recognize the handwritten digits on checks."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: As a startup, we want to create an assistive tool for people with hearing issues. This tool should help to transcribe recorded spoken words into text."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "query: We have a customer support agency. We need to convert recorded conversations in Russian to text."}
{"completion": "blip2-flan-t5-xxl", "text": "document: BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}
{"completion": "blip2-flan-t5-xxl", "text": "query: We are now designing an AI assistant. We need the assistant to help users by returning some useful information based on the image and question."}
{"completion": "openai/whisper-large-v2", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-large-v2", "text": "query: Our company is planning to develop an application to transcribe conference calls. We need a model that can offer accurate speech recognition."}
{"completion": "superb/wav2vec2-base-superb-sid", "text": "document: This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Identification task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/wav2vec2-base-superb-sid", "text": "query: Our customer wants a project that classifies audio files as speech, music or noise."}
{"completion": "lllyasviel/sd-controlnet-hed", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-hed", "text": "query: We are looking to create a digital art repository by converting images to oil paintings."}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "document: A binary classification model for predicting carbon emissions"}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "query: I want to predict the carbon emissions for a series of buildings using this information."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "query: An astrophysicist needs an image of a galaxy-like shape for a research project."}
{"completion": "distilbart-cnn-12-6-samsum", "text": "document: This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text."}
{"completion": "distilbart-cnn-12-6-samsum", "text": "query: We are working on a chatbot that can condense long conversations into shorter ones for easy access to important information."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-ade-640-640", "text": "query: I am an architect, and I need a tool that analyzes an aerial photo of a neighborhood and recognizes different objects like buildings, roads, and trees."}
{"completion": "sentence-transformers/all-MiniLM-L12-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-MiniLM-L12-v2", "text": "query: Help me understand the relationship between movie characters based on lines from a script."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "document: TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "query: I am a data scientist, and I have been given an important task of analyzing data from different companies for a report. I need to make the required comparisons based on the details from a csv file."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "document: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "query: I want to create a virtual exhibition of animals depicted in a certain traditional art style. Make sure to generate images of high quality."}
{"completion": "Fish-Weight", "text": "document: This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes."}
{"completion": "Fish-Weight", "text": "query: A marine research company wants to predict the weight of fish based on different features such as length, width, and height. Let's provide them with a model that can perform this task."}
{"completion": "srg/outhimar_64-Close-regression", "text": "document: Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt."}
{"completion": "srg/outhimar_64-Close-regression", "text": "query: I need help predicting the closing price of a stock given its opening price, high price, low price, and trading volume."}
{"completion": "xhyi/layoutlmv3_docvqa_t11c5000", "text": "document: LayoutLMv3 model trained for document question answering task."}
{"completion": "xhyi/layoutlmv3_docvqa_t11c5000", "text": "query: We are running a library service. A user needs to know the date the document was published. "}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "document: CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER"}
{"completion": "GanjinZero/UMLSBert_ENG", "text": "query: We are a medical support system developing chatbots for doctors. We require keywords and intensity of each sentence related to medical terminology."}
{"completion": "shi-labs/oneformer_ade20k_swin_tiny", "text": "document: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model."}
{"completion": "shi-labs/oneformer_ade20k_swin_tiny", "text": "query: Can you offer a solution for a real estate platform? The platform requires a model that can segment images provided by users to identify interior spaces, objects, and elements in the rooms like furniture, walls, windows, etc."}
{"completion": "facebook/regnet-y-008", "text": "document: RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository."}
{"completion": "facebook/regnet-y-008", "text": "query: A scientist wants to know if the newly discovered animal is a species of dog or cat in a picture."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "document: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b0-finetuned-ade-512-512", "text": "query: We want to improve our parking lot management system by detecting the available free spots. Can we use image segmentation to achieve this?"}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "document: This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data."}
{"completion": "joeddav/distilbert-base-uncased-go-emotions-student", "text": "query: I want an app that would analyze and classify my moods based on my social media text inputs."}
{"completion": "git-large-textvqa", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}
{"completion": "git-large-textvqa", "text": "query: You found an ancient artifact, you are trying to describe it to an old historian who is blind. The historian can only read braille."}
{"completion": "google/flan-t5-base", "text": "document: FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering."}
{"completion": "google/flan-t5-base", "text": "query: As a language teacher, I want to translate a sentence from English to German in the LIA project between languages."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "document: A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "query: A company needs assistance in translating Italian user reviews into English for further analysis."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "query: Can you help us segment our customer reviews into positive, negative, and neutral categories?"}
{"completion": "flair/ner-english", "text": "document: This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english", "text": "query: I'm working on an article and I want to identify the names of all the organizations mentioned in the text."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "document: This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset."}
{"completion": "ConvTasNet_Libri3Mix_sepclean_8k", "text": "query: We are building a product to process historical recordings with background noise. We need to separate the speakers and reduce noise from these recordings."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "document: GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "query: Create a system that automatically generates creative and informative content about new technology products."}
{"completion": "chavinlo/TempoFunk", "text": "document: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}
{"completion": "chavinlo/TempoFunk", "text": "query: I want to implement a future classroom system where I can input a sentence, and a short video is generated relevant to that sentence, like a visual aid for my students."}
{"completion": "google/owlvit-large-patch14", "text": "document: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages."}
{"completion": "google/owlvit-large-patch14", "text": "query: An interior design company needs a system for their store to identify which furniture items are in a given room image."}
{"completion": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech."}
{"completion": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD", "text": "query: The customer support department wants to analyze the sentiment of audio recordings in incoming calls. The audio recordings are in Spanish. They need to know the sentiment of each call."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "text": "query: Help me to find similar sentences to the given sentence from a list of sentences."}
{"completion": "gpt2", "text": "document: GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences."}
{"completion": "gpt2", "text": "query: Our school wants a chatbot that can generate sentences for writing exercises for students. Please describe the pipeline required to create such a model and how to use it."}
{"completion": "araffin/ppo-LunarLander-v2", "text": "document: This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library."}
{"completion": "araffin/ppo-LunarLander-v2", "text": "query: Implement a reinforcement learning system using the trained Proximal Policy Optimization (PPO) model to play the LunarLander game and determine its performance."}
{"completion": "sb3/ppo-CartPole-v1", "text": "document: This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/ppo-CartPole-v1", "text": "query: I've just downloaded the pre-trained model for CartPole-v1. Please help me to load this model and test it on a given environment."}
{"completion": "t5_sentence_paraphraser", "text": "document: A T5 model for paraphrasing sentences"}
{"completion": "t5_sentence_paraphraser", "text": "query: \"The quick brown fox jumps over the lazy dog.\""}
{"completion": "vicgalle/xlm-roberta-large-xnli-anli", "text": "document: XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification."}
{"completion": "vicgalle/xlm-roberta-large-xnli-anli", "text": "query: very good, good, average, bad, very bad."}
{"completion": "deepset/bert-base-cased-squad2", "text": "document: This is a BERT base cased model trained on SQuAD v2"}
{"completion": "deepset/bert-base-cased-squad2", "text": "query: I heard of a new NLP technology that can answer questions using given context/input. Can you please tell me how I can use it?"}
{"completion": "clipseg-rd64-refined", "text": "document: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation."}
{"completion": "clipseg-rd64-refined", "text": "query: I am a biologist studying cells under a microscope. I have an image of a cell and want to segment it to identify different organelles by leveraging zero-shot image segmentation."}
{"completion": "cl-tohoku/bert-base-japanese-char", "text": "document: This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization."}
{"completion": "cl-tohoku/bert-base-japanese-char", "text": "query: A colleague is writing a Japanese chatbot. Assist them in completing sentences by predicting the perfect completion."}
{"completion": "keremberke/yolov8m-nlf-head-detection", "text": "document: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets."}
{"completion": "keremberke/yolov8m-nlf-head-detection", "text": "query: We need to ensure safety on our construction site. Detect workers who aren't wearing helmets using a computer vision model."}
{"completion": "SYSPIN/Marathi_Male_TTS", "text": "document: A Marathi Male Text-to-Speech model using ESPnet framework."}
{"completion": "SYSPIN/Marathi_Male_TTS", "text": "query: I am looking for a text-to-speech system that can convert my Marathi sentences into audio. How do I proceed?"}
{"completion": "fastspeech2-en-male1", "text": "document: FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4."}
{"completion": "fastspeech2-en-male1", "text": "query: We are looking to add an audio accompaniment to our company's website. The audio will greet and instruct visitors on how to navigate the page. Please generate a text-to-speech example."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "document: BrandonBot4Epochs is a conversational model trained on the GPT-2 architecture for text generation. It can be used to generate responses in a chatbot-like interface."}
{"completion": "Zixtrauce/BDBot4Epoch", "text": "query: Our goal is to create a chatbot that can carry a conversation between various users on different topics such as movies, music, and technology."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "document: TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}
{"completion": "google/tapas-large-finetuned-sqa", "text": "query: A multinational corporation needs an assistant to provide answers quickly and accurately. We need to provide answers from the given financial data."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/all-distilroberta-v1", "text": "query: I am looking for recommendations for books to read, based on my favorite book title and description. I have an API that can provide books based on semantic similarity."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data."}
{"completion": "microsoft/deberta-v2-xlarge", "text": "query: We are trying to find the most suitable job profiles for people from different countries by filling some missing country names."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "document: GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation."}
{"completion": "EleutherAI/gpt-neo-2.7B", "text": "query: Write a short article about the benefits of using artificial intelligence in the education sector."}
{"completion": "EleutherAI/gpt-j-6B", "text": "document: GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI."}
{"completion": "EleutherAI/gpt-j-6B", "text": "query: I want the story content of a sad man meeting a girl who can make everyone happy."}
{"completion": "caidas/swin2SR-classical-sr-x2-64", "text": "document: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository."}
{"completion": "caidas/swin2SR-classical-sr-x2-64", "text": "query: We are a media company working on enhancing the quality of old films. We want to improve the resolution of movie frames."}
{"completion": "keremberke/yolov8m-table-extraction", "text": "document: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset."}
{"completion": "keremberke/yolov8m-table-extraction", "text": "query: Make use of Computer Vision techniques to extract tables from a provided image of a document."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "document: A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing."}
{"completion": "Helsinki-NLP/opus-mt-nl-en", "text": "query: I got a message in Dutch from a business partner. Translate the contents of the message into English."}
{"completion": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')", "text": "query: We need to sort the images captured by the company's security camera into categories like human, animal, vehicle, and others."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "ntrant7/sd-class-butterflies-32", "text": "query: The science and biology classes are creating learning materials for a lesson on butterflies. Generate a picture of a butterfly for them."}
{"completion": "rajistics/california_housing", "text": "document: A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices."}
{"completion": "rajistics/california_housing", "text": "query: We are building a real estate website. Implement a model to predict housing prices based on input features such as location, number of rooms, and age of the property."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "document: distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks."}
{"completion": "valhalla/distilbart-mnli-12-1", "text": "query: Create a machine learning model capable of classifying movie subtitles as action, horror or comedy."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Arabic. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Arabic using the train and validation splits of Common Voice 6.1 and Arabic Speech Corpus."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic", "text": "query: We would like to transcribe an Arabic podcast so that our non-Arabic speaking audience can access the content."}
{"completion": "facebook/bart-base", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-base", "text": "query: I want to summarize an article written by a journalist in English. I need a fine-tuned model that generates a summarized text from the input."}
{"completion": "facebook/detr-resnet-50", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50", "text": "query: I want to analyze an image in our production line and automatically detect and classify objects."}
{"completion": "cl-tohoku/bert-base-japanese-char", "text": "document: This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization."}
{"completion": "cl-tohoku/bert-base-japanese-char", "text": "query: We have a translation app that translates texts from other languages into Japanese. We need to fill in the blanks by predicting the appropriate Japanese character."}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. Russian single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-ru-cv7_css10", "text": "query: A Russian science fiction writer wants to turn one of his books into an audiobook using text-to-speech technology that generates natural sounding speech."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "document: A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "query: I have a conversation from a movie script in English that I want to translate into Chinese."}
{"completion": "rasa/LaBSE", "text": "document: LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages."}
{"completion": "rasa/LaBSE", "text": "query: We are building a social network for language learners that supports multiple languages. When a user enters a new phrase or expression, they want to learn in their target language, we want to provide the nearest matching expressions in our database that have similar meanings."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "document: Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment."}
{"completion": "edbeeching/decision-transformer-gym-walker2d-expert", "text": "query: I want to build a model that will control a bipedal robot efficiently. Can you help me with an API?"}
{"completion": "Apocalypse-19/shoe-generator", "text": "document: This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution."}
{"completion": "Apocalypse-19/shoe-generator", "text": "query: Our client wants to use a shoe-generator model for generating potential sneaker designs for their new product line."}
{"completion": "superb/wav2vec2-base-superb-sid", "text": "document: This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Identification task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/wav2vec2-base-superb-sid", "text": "query: Create an AI-based system to identify if a clip belongs to a specific celebrity or an unknown speaker from an audio dataset."}
{"completion": "Intel/dpt-large", "text": "document: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation."}
{"completion": "Intel/dpt-large", "text": "query: We are a real estate company and we need depth information of the houses that we are going to sell."}
{"completion": "joeddav/xlm-roberta-large-xnli", "text": "document: This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline."}
{"completion": "joeddav/xlm-roberta-large-xnli", "text": "query: We are a company that works with products based on the internet, health, and politics. Analyze the sentences from the available news articles, and determine which category they fall into."}
{"completion": "google/byt5-small", "text": "document: ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5. ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is usable on a downstream task. ByT5 works especially well on noisy text data, e.g., google/byt5-small significantly outperforms mt5-small on TweetQA."}
{"completion": "google/byt5-small", "text": "query: A group of language learners requested a tool that translates English texts to French. We need to develop this tool."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "document: A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."}
{"completion": "JosephusCheung/GuanacoVQA", "text": "query: Detect whether the image of a mountain in front of me represents Mount Everest or not by asking a question about the mountain."}
{"completion": "naver-clova-ix/donut-base-finetuned-docvqa", "text": "document: Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base-finetuned-docvqa", "text": "query: As a financial consultant, I need to extract answers from pictures of financial documents."}
{"completion": "kredor/punctuate-all", "text": "document: A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian."}
{"completion": "kredor/punctuate-all", "text": "query: There is a machine-transcribed text of a patient's medical record. I would like to improve its readability by adding punctuation."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-russian", "text": "query: A research team in studying urban noise pollution is recording different audios from Russian city streets, and they intend to transcribe those audios into text."}
{"completion": "openai/whisper-small", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages."}
{"completion": "openai/whisper-small", "text": "query: As a teacher, I would like to transcribe and understand the content of a lecture given in English in an audio file."}
{"completion": "Intel/dpt-hybrid-midas", "text": "document: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone."}
{"completion": "Intel/dpt-hybrid-midas", "text": "query: We have recently started a project that requires depth estimation of drone pictures. Help us achieve this by predicting depth information. "}
{"completion": "temp_vilt_vqa", "text": "document: A visual question answering model for answering questions related to images using the Hugging Face Transformers library."}
{"completion": "temp_vilt_vqa", "text": "query: Develop a system to help visually impaired individuals by answering questions related to images."}
{"completion": "Realistic_Vision_V1.4", "text": "document: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}
{"completion": "Realistic_Vision_V1.4", "text": "query: As a game developer, I want to generate a specific concept art for the game. I need an image of a dark, haunted forest with a mysterious male figure in tattered clothing, holding a lantern, in the center. No other characters should appear in the image."}
{"completion": "malteos/scincl", "text": "document: SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph."}
{"completion": "malteos/scincl", "text": "query: We are a technology company trying to cluster research papers based on their content for better organization in the database."}
{"completion": "datadmg/autotrain-test-news-44534112235", "text": "document: This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality."}
{"completion": "datadmg/autotrain-test-news-44534112235", "text": "query: low, medium, and high."}
{"completion": "redshift-man-skiing", "text": "document: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'."}
{"completion": "redshift-man-skiing", "text": "query: We would like to generate a short video using the text \"A cat playing piano\" as a prompt."}
{"completion": "dpt-large-redesign", "text": "document: A depth estimation model based on the DPT architecture."}
{"completion": "dpt-large-redesign", "text": "query: I am building a project for an autonomous vehicle. I need a solution to estimate the depth in an image."}
{"completion": "textless_sm_sl_es", "text": "document: A Fairseq model for audio-to-audio speech-to-speech translation."}
{"completion": "textless_sm_sl_es", "text": "query: Develop a system to convert English audio to Spanish audio for a company that makes video tutorials."}
{"completion": "keras-io/tab_transformer", "text": "document: This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model."}
{"completion": "keras-io/tab_transformer", "text": "query: Our company processes structured data to predict customer preferences. Please create a model to help us classify the structured data effectively."}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "document: Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack"}
{"completion": "shahrukhx01/question-vs-statement-classifier", "text": "query: Help me understand if the provided text is a question or a statement, while processing user messages in a chatbot-like application."}
{"completion": "facebook/bart-base", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-base", "text": "query: Create a model to generate captions for social media posts based on the post's textual content and hashtag."}
{"completion": "td3-Ant-v3", "text": "document: This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "td3-Ant-v3", "text": "query: My AI application wants to play Ant-v3 game, provide what I need to run the game and use the Reinforcement Learning model."}
{"completion": "dslim/bert-base-NER", "text": "document: bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset."}
{"completion": "dslim/bert-base-NER", "text": "query: We have a new database containing company information. We need a tool to automatically locate people's name, organization, and location mentioned in the text."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "document: A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English."}
{"completion": "Helsinki-NLP/opus-mt-it-en", "text": "query: Our company wants to translate a user manual from Italian to English. The manual is stored in a file. Translate that for us."}
{"completion": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640", "text": "document: A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain."}
{"completion": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640", "text": "query: The city wants to monitor and predict carbon emissions to reduce pollution. Based on the given data, predict CO2 emission levels."}
{"completion": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition", "text": "document: The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']."}
{"completion": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition", "text": "query: Our company is building a voice assistant that can detect emotion. We want to use a pre-trained model to recognize emotions in speech and give us the probability of each emotion."}
{"completion": "ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "ddpm-cifar10-32", "text": "query: A pets magazine requested our team to provide pictures of animals. They need both realistic and imaginary creatures for their new edition."}
{"completion": "neulab/omnitab-large-1024shot-finetuned-wtq-1024shot", "text": "document: OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab. neulab/omnitab-large-1024shot-finetuned-wtq-1024shot (based on BART architecture) is initialized with neulab/omnitab-large-1024shot and fine-tuned on WikiTableQuestions in the 1024-shot setting."}
{"completion": "neulab/omnitab-large-1024shot-finetuned-wtq-1024shot", "text": "query: Identify the employees with their respective years of experience, role, and location using a given table and provide the total years of experience of employees in a specific location."}
{"completion": "andite/anything-v4.0", "text": "document: Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model."}
{"completion": "andite/anything-v4.0", "text": "query: Our company sells anime-style canvas prints. We want to generate new pictures based on specific prompts to expand our catalog. For example, generate an image of a sunset with a cherry blossom tree."}
{"completion": "glpn-kitti-finetuned-diode-221214-123047", "text": "document: This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications."}
{"completion": "glpn-kitti-finetuned-diode-221214-123047", "text": "query: Create a tool to determine the distance between multiple objects captured in an image."}
{"completion": "ocariz/universe_1400", "text": "document: This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs."}
{"completion": "ocariz/universe_1400", "text": "query: An astrophysics student wants to generate creative images of universes for her final project. How can she use your model?"}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "document: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages."}
{"completion": "stabilityai/stable-diffusion-x4-upscaler", "text": "query: Create an artistic image of a red rose with a soft golden glow from a low resolution image."}
{"completion": "ast-finetuned-audioset-10-10-0.4593", "text": "document: Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."}
{"completion": "ast-finetuned-audioset-10-10-0.4593", "text": "query: Create a program to help me identify bird sounds among ambient noises in nature recordings."}
{"completion": "j-hartmann/emotion-english-distilroberta-base", "text": "document: This model classifies emotions in English text data. It predicts Ekman's 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base."}
{"completion": "j-hartmann/emotion-english-distilroberta-base", "text": "query: I want to see the emotions of my customers' reviews to have a quick overview of how they feel about our product."}
{"completion": "pygmalion-1.3b", "text": "document: Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message."}
{"completion": "pygmalion-1.3b", "text": "query: My team is making a movie, and we want to generate some dialogue for a character named Alex, who is a treasure hunter with a mysterious past. Please provide a short dialogue for a scene where Alex is talking about a long-lost treasure with their best friend."}
{"completion": "danupurnomo/dummy-titanic", "text": "document: This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more."}
{"completion": "danupurnomo/dummy-titanic", "text": "query: We are building an app that informs travelers about the probability of surviving a disaster like the Titanic. Implement the model."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761513", "text": "document: A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework."}
{"completion": "jwan2021/autotrain-us-housing-prices-1771761513", "text": "query: We are a real estate company looking to predict housing prices. We want to utilize this model in our operations to make smart decisions."}
{"completion": "keremberke/yolov8m-building-segmentation", "text": "document: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images."}
{"completion": "keremberke/yolov8m-building-segmentation", "text": "query: Our client needs to analyze satellite images for their urban planning project. They require a solution that helps in identifying building structures in those images."}
{"completion": "lllyasviel/control_v11p_sd15_openpose", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images."}
{"completion": "lllyasviel/control_v11p_sd15_openpose", "text": "query: We are building an image transformation service that takes a text input and turns it into a stylized image. For example, if the input is \"chef in the kitchen\", it should generate a stylized image of a chef in the kitchen."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "document: This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}
{"completion": "GTA5_PROCESS_LEARNING_AI", "text": "query: We are a gaming company and we need an AI that will perform in-game tasks in Grand Theft Auto 5 (GTA5). It should be able to learn by playing the game with a capped data usage."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "document: This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :."}
{"completion": "oliverguhr/fullstop-punctuation-multilang-large", "text": "query: \"Hello what a nice day to go to the park I really love the sunshine and fresh air how about you\""}
{"completion": "redshift-man-skiing", "text": "document: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'."}
{"completion": "redshift-man-skiing", "text": "query: Imagine you work for an animation company. They want you to create a short animation of a giraffe playing basketball using a text-to-video tool."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "document: A tabular classification model for predicting carbon emissions in grams, trained using AutoTrain."}
{"completion": "omarques/autotrain-in-class-test-demo-1659958764", "text": "query: We are a power plant analyzing carbon emissions data to find ways to reduce our carbon footprint. Classify and predict the carbon emission level in grams from a given set of data."}
{"completion": "microsoft/git-base-textvqa", "text": "document: GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}
{"completion": "microsoft/git-base-textvqa", "text": "query: I am an advertising agency. I want to know what the target audience is thinking when they see an image. Can you provide me information that can help me ask user-based questions related to an image?"}
{"completion": "d4data/Indian-voice-cloning", "text": "document: A model for detecting voice activity in Indian languages."}
{"completion": "d4data/Indian-voice-cloning", "text": "query: I have an audio clip of conversation between two people in the Indian language. I want to segregate individual voices of the speakers."}
{"completion": "microsoft/GODEL-v1_1-base-seq2seq", "text": "document: GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs."}
{"completion": "microsoft/GODEL-v1_1-base-seq2seq", "text": "query: Create a chatbot that can answer my questions using given knowledge."}
{"completion": "sepformer-wham-enhancement", "text": "document: This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k."}
{"completion": "sepformer-wham-enhancement", "text": "query: We are a startup working on building audio communication devices. Our goal is to provide users with high quality, noise-reduced speech audio with enhanced clarity."}
{"completion": "facebook/blenderbot_small-90M", "text": "document: Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation."}
{"completion": "facebook/blenderbot_small-90M", "text": "query: We have a website that provides recipes. The chatbox will be enhanced to answer recipe-related questions."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "document: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification."}
{"completion": "distilbert-base-uncased-finetuned-sst-2-english", "text": "query: I want to classify movie reviews as positive or negative. Provide me with code and instructions."}
{"completion": "t5-small", "text": "document: T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks."}
{"completion": "t5-small", "text": "query: We are a clothing company, we need someone to translate descriptions of our products from English to French."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "document: This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "query: A software agency wants to build some artistic background images for their upcoming project. We need to provide them with sample art images."}
{"completion": "deepset/tinyroberta-squad2", "text": "document: This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model."}
{"completion": "deepset/tinyroberta-squad2", "text": "query: We need a tool that can answer simple questions based on a given text."}
{"completion": "tiny-random-LayoutLMForQuestionAnswering", "text": "document: A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture."}
{"completion": "tiny-random-LayoutLMForQuestionAnswering", "text": "query: We are a real estate company and we want to use a model to extract information from scanned documents of lease agreements. Explain how to use the given model to extract information."}
{"completion": "GroNLP/bert-base-dutch-cased", "text": "document: BERTje is a Dutch pre-trained BERT model developed at the University of Groningen."}
{"completion": "GroNLP/bert-base-dutch-cased", "text": "query: Our research team in Groningen is shifting to work with Dutch text analytics. Help us build a NLP model for processing and understanding the language."}
{"completion": "clipseg-rd64-refined", "text": "document: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation."}
{"completion": "clipseg-rd64-refined", "text": "query: We are working with surveillance cameras and planning to identify and segment the people in the captured images."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "document: roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case."}
{"completion": "Jean-Baptiste/roberta-large-ner-english", "text": "query: Discover the names of people, organizations, and locations in the given text."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "document: A model that classifies images as hotdog or not hotdog."}
{"completion": "julien-c/hotdog-not-hotdog", "text": "query: We are holding a hotdog-themed contest at the company picnic. Determine if people are wearing costumes that look like a hotdog or not."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset."}
{"completion": "google/ddpm-ema-bedroom-256", "text": "query: Generate a high-quality image of a 3D indoor scene to be used in our website's background."}
{"completion": "superb/hubert-large-superb-sid", "text": "document: Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification."}
{"completion": "superb/hubert-large-superb-sid", "text": "query: I just recorded a podcast with multiple guests. I need to identify each speaker with a classifier."}
{"completion": "t5-base", "text": "document: T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library."}
{"completion": "t5-base", "text": "query: I have a text written in English and I would like to translate it to French. How can I achieve this using transformers and T5-Base?"}
{"completion": "videomae-base-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-base-ssv2", "text": "query: Create a video based intrusion detection system that analyzes video frames and identifies any unauthorized activity in a restricted area."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "document: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "query: We have been asked to create an anime image based on the description we are working with."}
{"completion": "cpierse/wav2vec2-large-xlsr-53-esperanto", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "cpierse/wav2vec2-large-xlsr-53-esperanto", "text": "query: I want to build a tool that can transcribe Esperanto audio files. How can I achieve that?"}
{"completion": "sshleifer/tiny-marian-en-de", "text": "document: A tiny English to German translation model using the Marian framework in Hugging Face Transformers."}
{"completion": "sshleifer/tiny-marian-en-de", "text": "query: I work in a company that builds global shopping products. I need to translate English product descriptions to German."}
{"completion": "openai-gpt", "text": "document: openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long-range dependencies."}
{"completion": "openai-gpt", "text": "query: Can you provide a creative beginning for a sci-fi novel?"}
{"completion": "facebook/bart-large-mnli", "text": "document: This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities."}
{"completion": "facebook/bart-large-mnli", "text": "query: We are developing a blog that categorizes different articles automatically. Detect the category for a given article called \"one day I will see the world\"."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-english", "text": "document: Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-english", "text": "query: We are a transcription service company. To save time, we need to convert an audio file to text."}
{"completion": "microsoft/tapex-base-finetuned-wikisql", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wikisql", "text": "query: I am looking for a table about Olympic Games and I want to find out the year when Beijing hosted the Olympic Games."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "document: BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model."}
{"completion": "bert-large-uncased-whole-word-masking-finetuned-squad", "text": "query: A student is working on a project and they need to know the difference between convex and concave polygons. I need a reliable source to get information."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "document: BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository."}
{"completion": "microsoft/beit-base-patch16-224-pt22k-ft22k", "text": "query: I have an online store, and I want to automatically classify and sort the products' pictures with their respective categories."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "query: I am a graphic designer. I would like a tool to determine if an image I am working on is a cat, dog, or fish."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "document: Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository."}
{"completion": "dandelin/vilt-b32-finetuned-vqa", "text": "query: An advertising agency requested an AI solution to answer questions about the content of an image."}
{"completion": "facebook/convnext-large-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration."}
{"completion": "facebook/convnext-large-224", "text": "query: I want to analyze images of cats and automatically determine their breed."}
{"completion": "roberta-base", "text": "document: RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task."}
{"completion": "roberta-base", "text": "query: Please assist me in completing a sentence with a missing word. The sentence is \"The rabbit quickly jumped over the __ log.\""}
{"completion": "audio-spectrogram-transformer", "text": "document: One custom ast model for testing of HF repos"}
{"completion": "audio-spectrogram-transformer", "text": "query: We are developing a music discovery app that identifies its genre by analyzing its audio spectrogram. Integrate a feature extraction method to obtain spectrograms from audio files."}
{"completion": "prithivida/parrot_fluency_model", "text": "document: Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model."}
{"completion": "prithivida/parrot_fluency_model", "text": "query: I have a game in the arcade space genre, and I need to generate alternative sentences to describe it for marketing purposes."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "document: This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart."}
{"completion": "johnowhitaker/sd-class-wikiart-from-bedrooms", "text": "query: I want to generate a piece of art inspired by WikiArt. How can I use this API to achieve my goal?"}
{"completion": "keremberke/yolov8m-pcb-defect-segmentation", "text": "document: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit."}
{"completion": "keremberke/yolov8m-pcb-defect-segmentation", "text": "query: We need to detect defects in printed circuit board images in order to evaluate the quality of our production."}
{"completion": "convnext_base.fb_in1k", "text": "document: A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings."}
{"completion": "convnext_base.fb_in1k", "text": "query: I am building an application to classify images of various dishes. I would like to identify the type of dish like pizza, burger or salad."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "document: This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification."}
{"completion": "typeform/mobilebert-uncased-mnli", "text": "query: Pop, Rock, Hip-hop, Country, Jazz. Please provide a suggestion on how to use the mobilebert model to do so."}
{"completion": "sentence-transformers/distiluse-base-multilingual-cased-v1", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/distiluse-base-multilingual-cased-v1", "text": "query: Explain how to find similar hotel reviews within a large number of reviews."}
{"completion": "lllyasviel/sd-controlnet-seg", "text": "document: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion."}
{"completion": "lllyasviel/sd-controlnet-seg", "text": "query: I have a set of images of houses. Use ControlNet Model to predict a segmentation map for each of them."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "document: A LayoutLM model for document question answering."}
{"completion": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "text": "query: While analyzing a legal document, I need to quickly find the answers to specific questions about the document."}
{"completion": "pyannote/overlapped-speech-detection", "text": "document: Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file."}
{"completion": "pyannote/overlapped-speech-detection", "text": "query: As analysts in an autority analyzing political debates on national TV, we need to detect when multiple people are speaking at the same time."}
{"completion": "microsoft/trocr-large-printed", "text": "document: TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-large-printed", "text": "query: We are building an app that converts images of printed text to regular text. We need to generate text from a given image."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "document: This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "query: I want to replicate the results of the Acrobot-v1 trained model, and I need to know how to load and use the model."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "document: Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context."}
{"completion": "mrm8488/t5-base-finetuned-question-generation-ap", "text": "query: We need to generate a question from the context and answer provided by the user."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "document: A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year."}
{"completion": "abhishek/autotrain-adult-census-xgboost", "text": "query: The financial consulting firm that we work for needs to analyze income levels of clients based on the provided dataset."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "document: A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets."}
{"completion": "svalabs/gbert-large-zeroshot-nli", "text": "query: Detect the theme of a German text to know if it is urgent or not."}
{"completion": "hustvl/yolos-tiny", "text": "document: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model."}
{"completion": "hustvl/yolos-tiny", "text": "query: We are setting up a robotics framework for self-navigating and following different object types. For now, we need a software to detect objects in images."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "document: RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1]."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "query: You've been hired by a Russian literary magazine. They need an AI that can generate summaries for Russian articles based on their features."}
{"completion": "navteca/tapas-large-finetuned-wtq", "text": "document: TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table."}
{"completion": "navteca/tapas-large-finetuned-wtq", "text": "query: Temperature in San Francisco at 12PM yesterday?"}
{"completion": "keremberke/yolov8m-plane-detection", "text": "document: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy."}
{"completion": "keremberke/yolov8m-plane-detection", "text": "query: I want my smart camera system to detect planes in the images it captures."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "document: This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", "text": "query: I am a journalist, and I want to know the category of my article which is in German language."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "document: A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID)."}
{"completion": "Helsinki-NLP/opus-mt-en-zh", "text": "query: Our website has users from multiple countries, we need a system to translate customer queries from English to simplified Chinese."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "document: Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images."}
{"completion": "ceyda/butterfly_cropped_uniq1K_512", "text": "query: A student who wants to study biology needs a collection of butterfly images to gain better insights. Can you create some quality butterfly images for them?"}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563588", "text": "query: Design a calculator for carbon emissions based on tabular data."}
{"completion": "temp_vilt_vqa", "text": "document: A visual question answering model for answering questions related to images using the Hugging Face Transformers library."}
{"completion": "temp_vilt_vqa", "text": "query: We are designing a mobile app for tourists, and we want to implement a feature that allows them to ask questions about an image they took while traveling."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-dot-v1", "text": "document: This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-dot-v1", "text": "query: A manager at a tourism agency is responsible for curating travel suggestions for customers. The manager wants to quickly find relevant information in a document to answer specific customer questions."}
{"completion": "videomae-base-ssv2", "text": "document: VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."}
{"completion": "videomae-base-ssv2", "text": "query: We are a computer vision startup focusing on classifying sports activities. We want to classify videos, given a video input."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "document: An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach."}
{"completion": "nlpconnect/vit-gpt2-image-captioning", "text": "query: We need a program that can caption images, so we can create captivating descriptions for the images we post on social media."}
{"completion": "anilbs/segmentation", "text": "document: Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Herv\u00e9 Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space."}
{"completion": "anilbs/segmentation", "text": "query: I want to process a call center conversation and detect the voice activity in it."}
{"completion": "deepset/roberta-base-squad2", "text": "document: This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions."}
{"completion": "deepset/roberta-base-squad2", "text": "query: I am looking for an AI solution to find answers in a given text based on the questions asked."}
{"completion": "facebook/bart-base", "text": "document: BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}
{"completion": "facebook/bart-base", "text": "query: Please generate a simple summary of a given paragraph"}
{"completion": "Intel/dpt-large", "text": "document: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation."}
{"completion": "Intel/dpt-large", "text": "query: We need a tool to analyze the depth of the objects in the provided images to classify them by their distance to the camera."}
{"completion": "glpn-nyu-finetuned-diode-221121-113853", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221121-113853", "text": "query: Our team wants to apply vision-based depth perception for an autonomous vehicle application. Estimate the depth map of an input image."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "document: LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"completion": "nlpaueb/legal-bert-small-uncased", "text": "query: \"The defendant is found not ___ and is released immediately.\""}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "document: A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks."}
{"completion": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind", "text": "query: We are a startup company looking for a way to classify the type of bird from an image."}
{"completion": "deepset/minilm-uncased-squad2", "text": "document: MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library."}
{"completion": "deepset/minilm-uncased-squad2", "text": "query: We are trying to develop a product that can answer questions about any given text. We want the product to be a language model."}
{"completion": "facebook/convnext-tiny-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification."}
{"completion": "facebook/convnext-tiny-224", "text": "query: I have an image of food here, and I want to know what type of cuisine it is. Can you tell me the model to do so?"}
{"completion": "facebook/hubert-large-ls960-ft", "text": "document: Facebook's Hubert-Large-Finetuned is an Automatic Speech Recognition model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. It is based on the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. The model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech and Libri-light benchmarks with various fine-tuning subsets."}
{"completion": "facebook/hubert-large-ls960-ft", "text": "query: I am building a virtual assistant and need to convert a short sample of a voice message to text."}
{"completion": "tuner007/pegasus_summarizer", "text": "document: PEGASUS fine-tuned for summarization"}
{"completion": "tuner007/pegasus_summarizer", "text": "query: I need a text summarization model to generate summaries of my long articles."}
{"completion": "opus-mt-ROMANCE-en", "text": "document: A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing."}
{"completion": "opus-mt-ROMANCE-en", "text": "query: A language education platform needs to translate a text from a Romance language like French to English."}
{"completion": "deepset/roberta-base-squad2", "text": "document: This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions."}
{"completion": "deepset/roberta-base-squad2", "text": "query: I would like to learn the history of Google. Generate a question that might be asked by someone learning about Google."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "document: This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages."}
{"completion": "csebuetnlp/mT5_multilingual_XLSum", "text": "query: Summarize the following news article about YouTube's policy change regarding anti-vaccine content."}
{"completion": "uclanlp/visualbert-vqa", "text": "document: A VisualBERT model for Visual Question Answering."}
{"completion": "uclanlp/visualbert-vqa", "text": "query: A blind person wants to have their questions answered about a picture they cannot see. Help them by generating an answer to their question about an image."}
{"completion": "sentiment_analysis_generic_dataset", "text": "document: This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification."}
{"completion": "sentiment_analysis_generic_dataset", "text": "query: I will need to analyze customer reviews to determine how they feel about our company's services or products."}
{"completion": "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg", "text": "document: A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP."}
{"completion": "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg", "text": "query: We are producing robots for home purposes. The robots should be able to recognize various objects in the home and classify them according to their function and usage. What is the most appropriate model to accomplish this task?"}
{"completion": "openai/whisper-base", "text": "document: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning."}
{"completion": "openai/whisper-base", "text": "query: We want to help people in listening comprehension by transcribing the audio into text."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries."}
{"completion": "microsoft/tapex-base-finetuned-wtq", "text": "query: I put lifestyle advices on a weekly table, and need assistance to answer related question about it."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "document: The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "query: For our image categorization project, we need to extract features from images in high resolution."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "document: RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1]."}
{"completion": "DeepPavlov/rubert-base-cased", "text": "query: We have multiple audio files of people speaking, our team wants to analyze some extracted features to find patterns for a research project."}
{"completion": "sberbank-ai/sbert_large_mt_nlu_ru", "text": "document: BERT large model multitask (cased) for Sentence Embeddings in Russian language."}
{"completion": "sberbank-ai/sbert_large_mt_nlu_ru", "text": "query: Create a book recommendation system that generates a list of similar books based on the input book's description."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "document: A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-wtq", "text": "query: We have a small database in the form of a table that contains information about employees. How can we leverage the TAPAS model to retrieve the salary of the highest-earning employee in the company?"}
{"completion": "Pi3141/DialoGPT-medium-elon-3", "text": "document: DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think."}
{"completion": "Pi3141/DialoGPT-medium-elon-3", "text": "query: We are designing an AI-powered chatbot for our website. We want it to talk like Elon Musk. How can we set this up?"}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "query: I want to decorate my astronomy room with some high-quality computer-generated images of galaxies. How can I achieve this?"}
{"completion": "facebook/mask2former-swin-small-coco-instance", "text": "document: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency."}
{"completion": "facebook/mask2former-swin-small-coco-instance", "text": "query: Help a user create a tool to recommend if market a product for this person based on a photo of the user."}
{"completion": "glpn-nyu-finetuned-diode-221122-044810", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221122-044810", "text": "query: A real estate company requested an estimation of the depth of an indoor scene based on a given image."}
{"completion": "data2vec-audio-base-960h", "text": "document: Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision."}
{"completion": "data2vec-audio-base-960h", "text": "query: Recently our company got some audio recordings from online meetings. Our responsibility now is to transcribe those recordings into text."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "document: This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks."}
{"completion": "dbmdz/bert-large-cased-finetuned-conll03-english", "text": "query: I have a document with names, addresses, and other information. I'd like to extract the names and locations from the document."}
{"completion": "mio/Artoria", "text": "document: This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output."}
{"completion": "mio/Artoria", "text": "query: We are working on an audiobook application and we need a text-to-speech model to read the books."}
{"completion": "setu4993/LaBSE", "text": "document: Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval."}
{"completion": "setu4993/LaBSE", "text": "query: I need my AI to perform similarity comparison between given sentences in multiple languages like English, Italian, and Japanese."}
{"completion": "superb/hubert-large-superb-er", "text": "document: This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark."}
{"completion": "superb/hubert-large-superb-er", "text": "query: I want to build a movie recommendation app that suggests film suggestions based on the emotions contained in the audio samples of the movies."}
{"completion": "stabilityai/stable-diffusion-2", "text": "document: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes."}
{"completion": "stabilityai/stable-diffusion-2", "text": "query: A video game developer wants to come up with new game characters. Can you create a character image based on the description \"A fierce warrior who has an affinity for nature and animals\"?"}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "document: This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets."}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "query: Have a new startup focusing on cloud services. I want to create marketing materials with images based on key phrases like \"cloud computing,\" \"secure data storage,\" \"high-speed internet,\" and \"scalable solutions.\" Can you generate images based on these phrases?"}
{"completion": "flax-community/clip-rsicd-v2", "text": "document: This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images."}
{"completion": "flax-community/clip-rsicd-v2", "text": "query: I am an environmental scientist, and I need a tool to automatically classify satellite images into categories like residential areas, playgrounds, stadiums, forests, and airports."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "document: This is a trained model of a DQN agent playing MountainCar-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-MountainCar-v0", "text": "query: As the chief of a mountain rescue team, I need my helicopter to be able to travel autonomously. Use a reinforcement learning model to have our helicopter learn the required movements."}
{"completion": "naver-clova-ix/donut-base", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base", "text": "query: Design a system to generate descriptions of images for a news agency's website. "}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "document: Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "jinhybr/OCR-DocVQA-Donut", "text": "query: \"What is the title of this document?\""}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "document: Transformer text-to-speech model from fairseq S^2. French, single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10."}
{"completion": "facebook/tts_transformer-fr-cv7_css10", "text": "query: The company's CEO needs to make a public announcement to the French-speaking employees in their preferred language. We need to generate an audio message from the given text."}
{"completion": "flair/ner-english-fast", "text": "document: This is the fast 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-fast", "text": "query: I found an old book from a flea market. I wonder if there is any significant events or persons in it. Can you help me?"}
{"completion": "cross-encoder/nli-deberta-v3-xsmall", "text": "document: This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks."}
{"completion": "cross-encoder/nli-deberta-v3-xsmall", "text": "query: I am working on a project to find the relevance of news articles to different categories. I need to be able to quickly classify news articles into categories such as politics, sports, and technology."}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "document: Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class"}
{"completion": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "text": "query: We want to enrich the digital artwork for the virtual gallery by generating a vintage-themed image."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "document: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5."}
{"completion": "lllyasviel/control_v11p_sd15_normalbae", "text": "query: Our client wants a creative visualization of the phrase \"A head full of roses\" for an upcoming campaign. Prepare a script to generate an image of this phrase using AI."}
{"completion": "michellejieli/NSFW_text_classifier", "text": "document: DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API."}
{"completion": "michellejieli/NSFW_text_classifier", "text": "query: Our client is responsible for tracking offensive content in user messages. We need to determine if a message is safe for work or not safe for work."}
{"completion": "naver-clova-ix/donut-base", "text": "document: Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}
{"completion": "naver-clova-ix/donut-base", "text": "query: We're building an application that needs to generate a caption based on an image as input."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "document: A tabular regression model trained with AutoTrain to predict carbon emissions."}
{"completion": "pcoloc/autotrain-mikrotik-7-7-1860563597", "text": "query: I want to build a software tool that estimates the carbon emissions of different activities by using machine learning. Help me to use your API."}
{"completion": "tts-hifigan-ljspeech", "text": "document: This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz."}
{"completion": "tts-hifigan-ljspeech", "text": "query: The AI needs to read the python code. Every python code needs to be read by AI."}
{"completion": "Narsil/deberta-large-mnli-zero-cls", "text": "document: DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task."}
{"completion": "Narsil/deberta-large-mnli-zero-cls", "text": "query: In a research project, we need to classify news articles as positive, negative, or neutral based on their content."}
{"completion": "microsoft/trocr-base-handwritten", "text": "document: TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}
{"completion": "microsoft/trocr-base-handwritten", "text": "query: I would like to extract text from images of handwritten documents."}
{"completion": "glpn-nyu-finetuned-diode-221116-054332", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-054332", "text": "query: We have a robot moving in a room; we want to measure the depth of the objects in that room using a pre-trained model."}
{"completion": "EleutherAI/gpt-j-6B", "text": "document: GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI."}
{"completion": "EleutherAI/gpt-j-6B", "text": "query: Our blog needs a captivating introduction paragraph about the latest advancements in technology."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "document: This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training."}
{"completion": "Recognai/bert-base-spanish-wwm-cased-xnli", "text": "query: Our news editor team is publishing articles in Spanish. Classify the articles based on their theme."}
{"completion": "ingen51/DialoGPT-medium-GPT4", "text": "document: A GPT-4 model for generating conversational responses in a dialogue setting."}
{"completion": "ingen51/DialoGPT-medium-GPT4", "text": "query: We need an AI system for handling customer support inquiries. This system should generate responses for specific customer questions."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode-221116-062619", "text": "query: I want to estimate the distance from my camera to each object in a given image."}
{"completion": "flair/ner-english-ontonotes-fast", "text": "document: This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF."}
{"completion": "flair/ner-english-ontonotes-fast", "text": "query: I'd like a chatbot that can help me identify named entities such as dates, names, and organizations within a given text."}
{"completion": "hkunlp/instructor-base", "text": "document: Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves state-of-the-art performance on 70 diverse embedding tasks."}
{"completion": "hkunlp/instructor-base", "text": "query: I work in a robotics lab, and I'm trying to represent a scientific article's title with a specific embedding."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "document: This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."}
{"completion": "cross-encoder/nli-distilroberta-base", "text": "query: Create a system to rank insights extracted from news articles in different categories such as technology, sports, and politics."}
{"completion": "facebook/detr-resnet-50", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50", "text": "query: An autonomous vehicle company is looking for object detection capabilities using object detection systems. They need a solution to understand and process various objects on roads."}
{"completion": "rajistics/california_housing", "text": "document: A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices."}
{"completion": "rajistics/california_housing", "text": "query: The real estate company we are collaborating with wants to have an estimate of houses' prices in California. Provide a way for them to predict house prices based on given features."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "document: The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him."}
{"completion": "google/vit-base-patch16-224-in21k", "text": "query: Build me a custom software to manage an image dataset from satellite, we need to detect changes or anomalies in the images."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "document: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library."}
{"completion": "dreamlike-art/dreamlike-anime-1.0", "text": "query: Create an anime image of a girl with a soft pastel background and surrounded by flowers."}
{"completion": "tiny-random-CLIPSegModel", "text": "document: A tiny random CLIPSegModel for zero-shot image classification."}
{"completion": "tiny-random-CLIPSegModel", "text": "query: We are building a product that uses machine learning to guess the character's emotion in an image from a comic."}
{"completion": "google/flan-t5-xxl", "text": "document: FLAN-T5 XXL is a fine-tuned version of the T5 language model, achieving state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. It has been fine-tuned on more than 1000 additional tasks covering multiple languages, including English, German, and French. It can be used for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning and question answering."}
{"completion": "google/flan-t5-xxl", "text": "query: We're an AI language translation application wanting to offer new languages. We need to translate answers to most common travel phrases into German."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "document: A tiny TAPAS model for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "query: Provide a simplified API to extract details from a table based on given questions."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "document: This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness."}
{"completion": "harshit345/xlsr-wav2vec-speech-emotion-recognition", "text": "query: We need an application to recognize different emotions for a call center."}
{"completion": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024", "text": "document: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}
{"completion": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024", "text": "query: The urban planning department of our city needs to identify different objects on a satellite map to improve public services."}
{"completion": "xlm-roberta-base", "text": "document: XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task."}
{"completion": "xlm-roberta-base", "text": "query: Construct a conversational AI bot that can help finish sentences by replacing masked words with suitable words."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "document: This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources."}
{"completion": "sentence-transformers/multi-qa-mpnet-base-cos-v1", "text": "query: I have a list of search documents and a question. Help me find the most relevant document that contains the answer to my question."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "document: Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment."}
{"completion": "decision-transformer-gym-hopper-medium", "text": "query: We are building a game, How can we predict the possible approaches for a robot to cross a environment."}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "document: This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets."}
{"completion": "stabilityai/sd-vae-ft-mse", "text": "query: We need to develop a text-to-image application for generating custom images of animals based on descriptions for pet adoption campaigns."}
{"completion": "textless_sm_cs_en", "text": "document: A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation."}
{"completion": "textless_sm_cs_en", "text": "query: We need a speech-to-speech translation model for our international calls that can convert between languages without relying on text."}
{"completion": "ProsusAI/finbert", "text": "document: FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning."}
{"completion": "ProsusAI/finbert", "text": "query: We are a financial technology company investing in different companies. We want to classify news articles to determine whether there is a positive, negative, or neutral sentiment."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "document: BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts."}
{"completion": "google/bigbird-pegasus-large-arxiv", "text": "query: Create a summary for this scientific paper I am about to present at a conference."}
{"completion": "tiny-random-LayoutLMForQuestionAnswering", "text": "document: A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture."}
{"completion": "tiny-random-LayoutLMForQuestionAnswering", "text": "query: In a legal firm, they need assistance in quickly extracting relevant information from legal documents. Implement a question-answering system for getting specific information from legal documentation."}
{"completion": "microsoft/tapex-large", "text": "document: TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."}
{"completion": "microsoft/tapex-large", "text": "query: Our project requires an AI that can extract information from tables and answer questions based on them."}
{"completion": "speechbrain/sepformer-wham", "text": "document: This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise."}
{"completion": "speechbrain/sepformer-wham", "text": "query: We're creating an application that can separate speakers from an audio file of a conversation. How do I perform this task?"}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "document: A tiny TAPAS model for table question answering tasks."}
{"completion": "lysandre/tiny-tapas-random-sqa", "text": "query: A finance company is developing a chatbot for users to find investment opportunities. Recognize the companies' performance from tables."}
{"completion": "nguyenvulebinh/wav2vec2-base-vietnamese-250h", "text": "document: Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio."}
{"completion": "nguyenvulebinh/wav2vec2-base-vietnamese-250h", "text": "query: I am creating a Vietnamese voice recognition system and I need to transcribe speech for my app."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "document: Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."}
{"completion": "hyunwoongko/blenderbot-9B", "text": "query: Can you provide a tool that would allow me to chat with an AI in a human-like manner?"}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "document: TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up."}
{"completion": "google/tapas-medium-finetuned-sqa", "text": "query: A list of costs and revenues for different departments in a company are provided in a tabular format. Identify which department generated the most revenue."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}
{"completion": "layoutlmv2-base-uncased-finetuned-infovqa", "text": "query: A robot wants to answer questions related to product manuals using the model."}
{"completion": "facebook/textless_sm_en_fr", "text": "document: This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech."}
{"completion": "facebook/textless_sm_en_fr", "text": "query: How can I use a model that translates English spoken language into French spoken language?"}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L6-v2", "text": "query: Create a system that can group similar sentences together."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L3-v2", "text": "document: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."}
{"completion": "sentence-transformers/paraphrase-MiniLM-L3-v2", "text": "query: \"The quick brown fox jumped over the lazy dog.\" and \"A fast brown fox leaped over a lazy canine.\""}
{"completion": "facebook/convnext-tiny-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification."}
{"completion": "facebook/convnext-tiny-224", "text": "query: The team is building an app for animal recognition. The goal is to determine the type of animal in a photo."}
{"completion": "fcakyon/yolov5s-v7.0", "text": "document: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories."}
{"completion": "fcakyon/yolov5s-v7.0", "text": "query: I am building a security product for detecting unauthorized objects in a restricted area. Can you detect objects in images and return their coordinates, scores, and categories?"}
{"completion": "google/vit-base-patch16-224", "text": "document: Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al."}
{"completion": "google/vit-base-patch16-224", "text": "query: A software company is building a mobile app that uses computer vision technology to recognize images of landmarks. We need to classify the images."}
{"completion": "speechbrain/sepformer-wham16k-enhancement", "text": "document: This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k."}
{"completion": "speechbrain/sepformer-wham16k-enhancement", "text": "query: Our company develops a new app for noise-canceling headphones. We need to improve the quality of the speech input by enhancing the signal from noisy recordings."}
{"completion": "facebook/convnext-large-224", "text": "document: ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration."}
{"completion": "facebook/convnext-large-224", "text": "query: I have a plant and I want to find out what plant it is. Can you help me identify it?"}
{"completion": "git-large-coco", "text": "document: GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}
{"completion": "git-large-coco", "text": "query: Develop a tool for a blind person that can generate captions based on what they see around them."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: I have an audio recording of someone speaking in a foreign language. I need to transcribe it so that I can understand what they are saying."}
{"completion": "ddpm-cifar10-32", "text": "document: Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm."}
{"completion": "ddpm-cifar10-32", "text": "query: Our marketing team is looking for diverse images for a campaign, and they want to use AI-generated images. Generate an image for them."}
{"completion": "facebook/opt-350m", "text": "document: OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, developed by Meta AI. It is designed to enable reproducible and responsible research at scale and bring more voices to the table in studying the impact of large language models. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation. It can also be fine-tuned on a downstream task using the CLM example."}
{"completion": "facebook/opt-350m", "text": "query: I am a language teacher. I wanted to automatically generate 5 interesting English sentences start with \"The man worked as a\"."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "document: BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA."}
{"completion": "Salesforce/blip-vqa-capfilt-large", "text": "query: I have a robotic dog that monitors my yard for security. I want him to identify intruders with good reliability. Could you come up with a method using this API to identify the intruders?"}
{"completion": "decapoda-research/llama-7b-hf", "text": "document: LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English."}
{"completion": "decapoda-research/llama-7b-hf", "text": "query: Generate a fictional story about a young scientist discovering a new element."}
{"completion": "tiny-wav2vec2-stable-ln", "text": "document: A tiny wav2vec2 model for Automatic Speech Recognition"}
{"completion": "tiny-wav2vec2-stable-ln", "text": "query: I have an audio recording from a meeting and I want a transcription of the conversation."}
{"completion": "flax-community/clip-rsicd-v2", "text": "document: This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images."}
{"completion": "flax-community/clip-rsicd-v2", "text": "query: Our company is analyzing aerial images to understand infrastructure development in different locations. We need an AI to classify images of areas such as residential neighborhoods, playgrounds, stadiums, forests, and airports."}
{"completion": "deepset/roberta-large-squad2", "text": "document: A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context."}
{"completion": "deepset/roberta-large-squad2", "text": "query: We are a book club. We recently finished discussing Brave New World by Aldous Huxley? What's the name of the main protagonist?"}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "document: Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS."}
{"completion": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn", "text": "query: We are a Chinese voice assistant company, and we need to know how to transcribe conversations during a conference or meetings."}
{"completion": "layoutlmv3-base-mpdocvqa", "text": "document: This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA."}
{"completion": "layoutlmv3-base-mpdocvqa", "text": "query: A user sent a picture of a document and has a question about it. Help them find their answer from the document."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "document: A Korean Table Question Answering model finetuned on the korwikitq dataset."}
{"completion": "dsba-lab/koreapas-finetuned-korwikitq", "text": "query: Our team is working on a product that answers questions about information in Korean tables. Implement an API to gather information from tables."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "document: This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973."}
{"completion": "mazkooleg/0-9up-wavlm-base-plus-ft", "text": "query: I want you to use the latest Hugging Face Transformers library to create an audio classifier that can help me detect voice commands amongst the numbers 0-9 for a smart home application."}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "document: A binary classification model for predicting carbon emissions"}
{"completion": "kem000123/autotrain-model1-binary-class-1843363194", "text": "query: We run a sustainable living network that helps people estimate their carbon emissions based on the data they provide us with."}
{"completion": "mywateriswet/ShuanBot", "text": "document: ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context."}
{"completion": "mywateriswet/ShuanBot", "text": "query: Help me build an AI assistant for an online customer support system."}
{"completion": "facebook/blenderbot-3B", "text": "document: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library."}
{"completion": "facebook/blenderbot-3B", "text": "query: We need a natural language processing model to have a multi-turn conversation with a user to help them solve their problems."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli", "text": "document: This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying model was pre-trained by Microsoft on the CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset."}
{"completion": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli", "text": "query: Given this germam text \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\", implement a text classification model to classify whether it's related to politics, economy, entertainment or environment."}
{"completion": "runwayml/stable-diffusion-inpainting", "text": "document: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask."}
{"completion": "runwayml/stable-diffusion-inpainting", "text": "query: I am creating a picture book for children. We need to generate an image of a yellow cat sitting on a park bench, surrounded by colorful balloons."}
{"completion": "opus-mt-tc-big-en-pt", "text": "document: Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world."}
{"completion": "opus-mt-tc-big-en-pt", "text": "query: Design a language translator software between English and Portuguese which could detect the format of the translation source text automatically."}
{"completion": "TF_Decision_Trees", "text": "document: Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year."}
{"completion": "TF_Decision_Trees", "text": "query: Our finance team needs a model to determine if a person makes over 50k a year based on provided data."}
{"completion": "lysandre/tapas-temporary-repo", "text": "document: TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up."}
{"completion": "lysandre/tapas-temporary-repo", "text": "query: We are building a productivity app where users are allowed to add simple tables to track their tasks. We need to answer questions related to user's tasks."}
{"completion": "textless_sm_sl_es", "text": "document: A Fairseq model for audio-to-audio speech-to-speech translation."}
{"completion": "textless_sm_sl_es", "text": "query: We're developing a tool that needs to translate Spanish spoken language into English spoken language without processing the written text. Implement a solution to do so."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "document: This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b."}
{"completion": "myunus1/diffmodels_galaxies_scratchbook", "text": "query: Build a system that generates images of galaxies using the diffusion model 'myunus1/diffmodels_galaxies_scratchbook'."}
{"completion": "sd-class-butterflies-32", "text": "document: This model is a diffusion model for unconditional image generation of cute butterflies."}
{"completion": "sd-class-butterflies-32", "text": "query: Develop an application for a museum where visitors can upload pictures of butterflies, and the application will generate new, similar images."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "document: This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks."}
{"completion": "videomae-base-finetuned-ucf101-subset", "text": "query: I want to develop an AI tool to categorize videos into different sports categories like soccer, basketball, or swimming."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "document: This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}
{"completion": "layoutlmv2-base-uncased_finetuned_docvqa", "text": "query: As a financial advisor, I need to extract the total amount from a client's invoice to analyze their expenses."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "document: Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words."}
{"completion": "facebook/wav2vec2-xlsr-53-espeak-cv-ft", "text": "query: A call center is interested in transcribing calls between agents and customers. We should process the audio to obtain the transcription."}
{"completion": "albert-base-v2", "text": "document: ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English."}
{"completion": "albert-base-v2", "text": "query: Classify novels into their genres based on their descriptions. Provide the model with a list of genres to choose from."}
{"completion": "glpn-nyu-finetuned-diode-221215-093747", "text": "document: A depth estimation model fine-tuned on the DIODE dataset."}
{"completion": "glpn-nyu-finetuned-diode-221215-093747", "text": "query: We need to analyze the depth of an image shared by the user on our application to analyze indoor spaces."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "document: This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks."}
{"completion": "vitouphy/wav2vec2-xls-r-300m-phoneme", "text": "query: I want to build a speech-to-text algorithm for the Indonesian language."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "document: Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0."}
{"completion": "superb/wav2vec2-base-superb-ks", "text": "query: We are developing a voice-controlled device. It needs to detect certain keywords."}
{"completion": "Xinhhd/autotrain-zhongxin-contest-49402119333", "text": "document: A multi-class classification model trained with AutoTrain to predict carbon emissions based on input features."}
{"completion": "Xinhhd/autotrain-zhongxin-contest-49402119333", "text": "query: Let's build a reporting tool for a sustainability department to predict and monitor the carbon emissions of various activities within the company."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "document: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}
{"completion": "facebook/detr-resnet-50-panoptic", "text": "query: Our company is looking to conduct market research for new products and would like to extract information from images."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "document: This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included."}
{"completion": "sb3/dqn-Acrobot-v1", "text": "query: Develop an AI bot for Acrobot game. Load the pre-trained model for Acrobot game and let the AI play it."}
{"completion": "Realistic_Vision_V1.4", "text": "document: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}
{"completion": "Realistic_Vision_V1.4", "text": "query: \"A bright and colorful banner showcasing a variety of beautiful flowers in a lush garden, with a clear blue sky above and the sun shining brightly. The text on the banner says 'Welcome to our Gardening Extravaganza' in an elegant font.\""}
{"completion": "ProsusAI/finbert", "text": "document: FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning."}
{"completion": "ProsusAI/finbert", "text": "query: We need to analyse the sentiment of a company's latest earnings report."}
{"completion": "glpn-nyu-finetuned-diode", "text": "document: This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}
{"completion": "glpn-nyu-finetuned-diode", "text": "query: My startup is working on an AR application that measures the distance between objects in an image. We are testing out different depth estimation models."}
{"completion": "flair/ner-english-ontonotes-large", "text": "document: English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT."}
{"completion": "flair/ner-english-ontonotes-large", "text": "query: I need a virtual agent for my company's support chat. The agent needs to be able to extract relevant information like names and dates from user messages."}
